{"source-free": {"Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation": "|**2025-9-24**|**Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation**|Silvio Mazzucco et.al|[paper](https://arxiv.org/abs/2509.15225)|[code](https://thegoodailab.org/blog/vocalign)|<details><summary>detail</summary>BMVC 2025 - Project Page: https://thegoodailab</details>|\n", "Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment": "|**2025-9-22**|**Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment**|Wenjie Liu et.al|[paper](https://arxiv.org/abs/2509.18502)|-|-|\n", "Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation": "|**2025-9-22**|**Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation**|Amirhossein Dadashzadeh et.al|[paper](https://arxiv.org/abs/2504.11669)|[code](https://github.com/Plrbear/Co-Star)|-|\n", "Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation": "|**2025-9-21**|**Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation**|Bin Wang et.al|[paper](https://arxiv.org/abs/2509.16942)|-|-|\n", "Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments": "|**2025-9-18**|**Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2501.02184)|-|-|\n", "Step-wise Distribution Alignment Guided Style Prompt Tuning for Source-free Cross-domain Few-shot Learning": "|**2025-9-13**|**Step-wise Distribution Alignment Guided Style Prompt Tuning for Source-free Cross-domain Few-shot Learning**|Huali Xu et.al|[paper](https://arxiv.org/abs/2411.10070)|[code](https://github.com/xuhuali-mxj/StepSPT.)|<details><summary>detail</summary>IEEE TPAMI</details>|\n", "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment": "|**2025-9-12**|**Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment**|Rini Smita Thakur et.al|[paper](https://arxiv.org/abs/2509.10134)|[code](https://visdomlab.github.io/GCL/.)|<details><summary>detail</summary>Accepted in BMVC 2025</details>|\n", "Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models": "|**2025-9-10**|**Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models**|Kosuke Kihara et.al|[paper](https://arxiv.org/abs/2509.08372)|-|<details><summary>detail</summary>Accepted by the IEEE ICIP 2025 Satellite Workshop 1: Edge Intelligence: Smart</details>|\n", "StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails": "|**2025-9-2**|**StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails**|Hritik Arasu et.al|[paper](https://arxiv.org/abs/2509.02982)|-|<details><summary>detail</summary>5 page paper</details>|\n", "Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration": "|**2025-8-28**|**Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2508.20836)|-|-|\n", "VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection": "|**2025-8-26**|**VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection**|Jianhong Han et.al|[paper](https://arxiv.org/abs/2508.11167)|-|<details><summary>detail</summary>Manuscript submitted to IEEE TCSVT</details>|\n", "Towards Source-Free Machine Unlearning": "|**2025-8-20**|**Towards Source-Free Machine Unlearning**|Sk Miraj Ahmed et.al|[paper](https://arxiv.org/abs/2508.15127)|-|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method": "|**2025-8-14**|**Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2508.09202)|-|-|\n", "Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation": "|**2025-8-7**|**Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation**|Jianming Liu et.al|[paper](https://arxiv.org/abs/2508.05213)|[code](https://github.com/ljm198134/TVGTANet.)|-|\n", "Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning": "|**2025-8-4**|**Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning**|Sijia Wang et.al|[paper](https://arxiv.org/abs/2508.02039)|-|-|\n"}, "object detection": {"SpaRC: Sparse Radar-Camera Fusion for 3D Object Detection": "|**2025-9-24**|**SpaRC: Sparse Radar-Camera Fusion for 3D Object Detection**|Philipp Wolters et.al|[paper](https://arxiv.org/abs/2411.19860)|[code](https://github.com/phi-wol/sparc.)|-|\n", "Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection": "|**2025-9-24**|**Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection**|Yunqing Hu et.al|[paper](https://arxiv.org/abs/2509.19875)|-|-|\n", "BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting": "|**2025-9-24**|**BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting**|Yixun Zhang et.al|[paper](https://arxiv.org/abs/2509.19793)|-|<details><summary>detail</summary>Intend to submit to RA-L</details>|\n", "HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection": "|**2025-9-23**|**HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection**|Ruichao Hou et.al|[paper](https://arxiv.org/abs/2509.18738)|[code](https://github.com/milotic233/HyPSAM.)|-|\n", "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection": "|**2025-9-23**|**LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection**|Lanhu Wu et.al|[paper](https://arxiv.org/abs/2509.18683)|-|<details><summary>detail</summary>ACM MM 2025</details>|\n", "MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving": "|**2025-9-23**|**MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving**|Yuzhi Wu et.al|[paper](https://arxiv.org/abs/2509.18613)|-|-|\n", "MVP: Motion Vector Propagation for Zero-Shot Video Object Detection": "|**2025-9-22**|**MVP: Motion Vector Propagation for Zero-Shot Video Object Detection**|Binhua Huang et.al|[paper](https://arxiv.org/abs/2509.18388)|[code](https://github.com/microa/MVP.)|-|\n", "RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion": "|**2025-9-22**|**RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion**|Geonho Bang et.al|[paper](https://arxiv.org/abs/2509.17712)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "Domain Adaptive Object Detection for Space Applications with Real-Time Constraints": "|**2025-9-22**|**Domain Adaptive Object Detection for Space Applications with Real-Time Constraints**|Samet Hicsonmez et.al|[paper](https://arxiv.org/abs/2509.17593)|-|<details><summary>detail</summary>Advanced Space Technologies in Robotics and Automation (ASTRA) 2025</details>|\n", "An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection": "|**2025-9-22**|**An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection**|Edwine Nabahirwa et.al|[paper](https://arxiv.org/abs/2509.17561)|-|<details><summary>detail</summary>28 Pages</details>|\n", "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity": "|**2025-9-21**|**GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity**|Seongheon Park et.al|[paper](https://arxiv.org/abs/2508.19972)|-|<details><summary>detail</summary>NeurIPS 2025</details>|\n", "Enhanced Detection of Tiny Objects in Aerial Images": "|**2025-9-21**|**Enhanced Detection of Tiny Objects in Aerial Images**|Kihyun Kim et.al|[paper](https://arxiv.org/abs/2509.17078)|[code](https://github.com/Kihyun11/MoonNet)|-|\n", "LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection": "|**2025-9-21**|**LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection**|Wei Liao et.al|[paper](https://arxiv.org/abs/2509.16970)|-|-|\n", "MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image": "|**2025-9-21**|**MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image**|Leiyu Wang et.al|[paper](https://arxiv.org/abs/2509.16957)|[code](https://github.com/Iwill-github/MORCNN.)|-|\n", "Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection": "|**2025-9-20**|**Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection**|Wenhuan Lu et.al|[paper](https://arxiv.org/abs/2509.16670)|-|-|\n"}, "domain adaptation": {"DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data": "|**2025-9-24**|**DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data**|Yuhang Zhou et.al|[paper](https://arxiv.org/abs/2505.15074)|[code](https://github.com/Tonyzhou98/disco_grpo.)|<details><summary>detail</summary>Accepted by EMNLP 2025 Findings</details>|\n", "Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation": "|**2025-9-24**|**Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation**|Matteo Cardoni et.al|[paper](https://arxiv.org/abs/2509.20269)|-|-|\n", "Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation": "|**2025-9-24**|**Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation**|Silvio Mazzucco et.al|[paper](https://arxiv.org/abs/2509.15225)|[code](https://thegoodailab.org/blog/vocalign)|<details><summary>detail</summary>BMVC 2025 - Project Page: https://thegoodailab</details>|\n", "SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition": "|**2025-9-24**|**SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition**|Jiahao Tang et.al|[paper](https://arxiv.org/abs/2507.17524)|[code](https://github.com/XuanSuTrum/SDC-Net.)|-|\n", "Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains": "|**2025-9-23**|**Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains**|Dongzhe Zheng et.al|[paper](https://arxiv.org/abs/2509.19672)|-|<details><summary>detail</summary>Accepted by NeurIPS 2025</details>|\n", "EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data": "|**2025-9-23**|**EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data**|Ryan Punamiya et.al|[paper](https://arxiv.org/abs/2509.19626)|[code](https://ego-bridge.github.io)|<details><summary>detail</summary>39th Conference on Neural Information Processing Systems (NeurIPS 2025) and Oral at Conference on Robot Learning (CoRL 2025)</details>|\n", "Code Driven Planning with Domain-Adaptive Critic": "|**2025-9-23**|**Code Driven Planning with Domain-Adaptive Critic**|Zikang Tian et.al|[paper](https://arxiv.org/abs/2509.19077)|-|-|\n", "Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images": "|**2025-9-23**|**Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images**|Jiabao Chen et.al|[paper](https://arxiv.org/abs/2509.18973)|-|<details><summary>detail</summary>MICCAI2025</details>|\n", "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training": "|**2025-9-23**|**Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training**|Shuo Cheng et.al|[paper](https://arxiv.org/abs/2509.18631)|-|-|\n", "Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment": "|**2025-9-22**|**Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment**|Wenjie Liu et.al|[paper](https://arxiv.org/abs/2509.18502)|-|-|\n", "Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation": "|**2025-9-22**|**Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation**|Amirhossein Dadashzadeh et.al|[paper](https://arxiv.org/abs/2504.11669)|[code](https://github.com/Plrbear/Co-Star)|-|\n", "SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI": "|**2025-9-22**|**SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI**|Yuanhan Wang et.al|[paper](https://arxiv.org/abs/2509.17925)|[code](https://github.com/baiyou1234/SmaRT.)|-|\n", "Domain Adaptive Object Detection for Space Applications with Real-Time Constraints": "|**2025-9-22**|**Domain Adaptive Object Detection for Space Applications with Real-Time Constraints**|Samet Hicsonmez et.al|[paper](https://arxiv.org/abs/2509.17593)|-|<details><summary>detail</summary>Advanced Space Technologies in Robotics and Automation (ASTRA) 2025</details>|\n", "Training-Free Label Space Alignment for Universal Domain Adaptation": "|**2025-9-22**|**Training-Free Label Space Alignment for Universal Domain Adaptation**|Dujin Lee et.al|[paper](https://arxiv.org/abs/2509.17452)|-|-|\n", "Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation": "|**2025-9-21**|**Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation**|Bin Wang et.al|[paper](https://arxiv.org/abs/2509.16942)|-|-|\n"}, "domain generalization": {"Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation": "|**2025-9-24**|**Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation**|Chaojun Nie et.al|[paper](https://arxiv.org/abs/2509.20162)|[code](https://github.com/ChaojunNie/RLAG.)|-|\n", "Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization": "|**2025-9-24**|**Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization**|Tan Pan et.al|[paper](https://arxiv.org/abs/2509.15791)|-|<details><summary>detail</summary>Accepted by NeurIPS 2025</details>|\n", "Diffusion-Based Action Recognition Generalizes to Untrained Domains": "|**2025-9-22**|**Diffusion-Based Action Recognition Generalizes to Untrained Domains**|Rogerio Guimaraes et.al|[paper](https://arxiv.org/abs/2509.08908)|[code](https://www.vision.caltech.edu/actiondiff.)|<details><summary>detail</summary>Project page: https://www</details>|\n", "Unsupervised Structural-Counterfactual Generation under Domain Shift": "|**2025-9-22**|**Unsupervised Structural-Counterfactual Generation under Domain Shift**|Krishn Vishwas Kher et.al|[paper](https://arxiv.org/abs/2502.12013)|-|<details><summary>detail</summary>Updated author ordering</details>|\n", "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning": "|**2025-9-21**|**Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning**|Simon Ouellette et.al|[paper](https://arxiv.org/abs/2507.15877)|-|<details><summary>detail</summary>this version fixes errors in AlphaEvolve total % calculation</details>|\n", "From domain-landmark graph learning to problem-landmark graph generation": "|**2025-9-21**|**From domain-landmark graph learning to problem-landmark graph generation**|Cristian P\u00e9rez-Corral et.al|[paper](https://arxiv.org/abs/2509.17062)|-|-|\n", "Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains": "|**2025-9-20**|**Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains**|Junghwan Kim et.al|[paper](https://arxiv.org/abs/2509.16531)|-|<details><summary>detail</summary>EMNLP 2025</details>|\n", "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR": "|**2025-9-19**|**From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR**|Juan Castorena et.al|[paper](https://arxiv.org/abs/2509.16346)|-|-|\n", "CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization": "|**2025-9-18**|**CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization**|Min Zhang et.al|[paper](https://arxiv.org/abs/2509.15330)|-|-|\n", "Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications": "|**2025-9-18**|**Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications**|Tahar Chettaoui et.al|[paper](https://arxiv.org/abs/2509.14921)|-|<details><summary>detail</summary>the IEEE International Joint Conference on Biometrics 2025 (IJCB 2025)</details>|\n", "Domain Generalization for In-Orbit 6D Pose Estimation": "|**2025-9-18**|**Domain Generalization for In-Orbit 6D Pose Estimation**|Antoine Legrand et.al|[paper](https://arxiv.org/abs/2406.11743)|-|-|\n", "SNaRe: Domain-aware Data Generation for Low-Resource Event Detection": "|**2025-9-17**|**SNaRe: Domain-aware Data Generation for Low-Resource Event Detection**|Tanmay Parekh et.al|[paper](https://arxiv.org/abs/2502.17394)|-|<details><summary>detail</summary>EMNLP 2025 Main</details>|\n", "Class-invariant Test-Time Augmentation for Domain Generalization": "|**2025-9-17**|**Class-invariant Test-Time Augmentation for Domain Generalization**|Zhicheng Lin et.al|[paper](https://arxiv.org/abs/2509.14420)|-|-|\n", "CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning": "|**2025-9-17**|**CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning**|Huy Le et.al|[paper](https://arxiv.org/abs/2509.14373)|-|-|\n", "SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics": "|**2025-9-17**|**SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics**|Songhao Huang et.al|[paper](https://arxiv.org/abs/2509.13691)|-|-|\n"}, "vision language": {"Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector": "|**2025-9-24**|**Enhancing Targeted Adversarial Attacks on Large Vision-Language Models via Intermediate Projector**|Yiming Cao et.al|[paper](https://arxiv.org/abs/2508.13739)|-|-|\n", "Deciphering Functions of Neurons in Vision-Language Models": "|**2025-9-24**|**Deciphering Functions of Neurons in Vision-Language Models**|Jiaqi Xu et.al|[paper](https://arxiv.org/abs/2502.18485)|-|<details><summary>detail</summary>Accepted by the 31st ACM International Conference on Multimedia (ACM MM 2025)</details>|\n", "Universal Camouflage Attack on Vision-Language Models for Autonomous Driving": "|**2025-9-24**|**Universal Camouflage Attack on Vision-Language Models for Autonomous Driving**|Dehong Kong et.al|[paper](https://arxiv.org/abs/2509.20196)|-|-|\n", "EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models": "|**2025-9-24**|**EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models**|Botai Yuan et.al|[paper](https://arxiv.org/abs/2509.20146)|-|-|\n", "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving": "|**2025-9-24**|**Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving**|Pengxiang Li et.al|[paper](https://arxiv.org/abs/2509.20109)|-|-|\n", "To Trust Or Not To Trust Your Vision-Language Model's Prediction": "|**2025-9-24**|**To Trust Or Not To Trust Your Vision-Language Model's Prediction**|Hao Dong et.al|[paper](https://arxiv.org/abs/2505.23745)|[code](https://github.com/EPFL-IMOS/TrustVLM.)|-|\n", "FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models": "|**2025-9-24**|**FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models**|Xin Wang et.al|[paper](https://arxiv.org/abs/2509.19870)|-|-|\n", "CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition": "|**2025-9-24**|**CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition**|Sina J. Semnani et.al|[paper](https://arxiv.org/abs/2509.19768)|-|<details><summary>detail</summary>EMNLP 2025</details>|\n", "VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model": "|**2025-9-23**|**VLM See, Robot Do: Human Demo Video to Robot Action Plan via Vision Language Model**|Beichen Wang et.al|[paper](https://arxiv.org/abs/2410.08792)|-|-|\n", "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models": "|**2025-9-23**|**OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models**|Mengdi Jia et.al|[paper](https://arxiv.org/abs/2506.03135)|[code](https://qizekun.github.io/omnispatial/)|<details><summary>detail</summary>Project Page: https://qizekun</details>|\n", "Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models": "|**2025-9-23**|**Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models**|Mohammad Saim et.al|[paper](https://arxiv.org/abs/2509.19595)|-|-|\n", "CNS-Obsidian: A Neurosurgical Vision-Language Model Built From Scientific Publications": "|**2025-9-23**|**CNS-Obsidian: A Neurosurgical Vision-Language Model Built From Scientific Publications**|Anton Alyakin et.al|[paper](https://arxiv.org/abs/2502.19546)|-|-|\n", "Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models": "|**2025-9-23**|**Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models**|Tan-Hanh Pham et.al|[paper](https://arxiv.org/abs/2508.12587)|[code](https://github.com/Hanhpt23/OmniMod.)|-|\n", "OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation": "|**2025-9-23**|**OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation**|Noriaki Hirose et.al|[paper](https://arxiv.org/abs/2509.19480)|-|-|\n", "Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability": "|**2025-9-23**|**Large Vision-Language Model Alignment and Misalignment: A Survey Through the Lens of Explainability**|Dong Shu et.al|[paper](https://arxiv.org/abs/2501.01346)|-|<details><summary>detail</summary>EMNLP 2025 Findings</details>|\n"}}