{"source-free": {"Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-6-26**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "SFDLA: Source-Free Document Layout Analysis": "|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|\n", "Unified Source-Free Domain Adaptation": "|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|\n", "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|\n", "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization": "|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|\n", "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data": "|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|\n", "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation": "|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|\n", "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation": "|**2025-5-30**|**Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation**|Prasanna Reddy Pulakurthi et.al|[paper](https://arxiv.org/abs/2505.24216)|[code](https://github.com/PrasannaPulakurthi/SPM)|-|\n", "Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation": "|**2025-5-27**|**Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation**|Peihua Deng et.al|[paper](https://arxiv.org/abs/2411.16064)|[code](https://github.com/dengpeihua/GROTO.)|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Training-Free Multi-Step Audio Source Separation": "|**2025-5-26**|**Training-Free Multi-Step Audio Source Separation**|Yongyi Zang et.al|[paper](https://arxiv.org/abs/2505.19534)|-|-|\n", "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation": "|**2025-5-23**|**Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation**|Peiliang Gong et.al|[paper](https://arxiv.org/abs/2505.21525)|-|-|\n", "Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing": "|**2025-5-20**|**Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing**|Yang Xiao et.al|[paper](https://arxiv.org/abs/2505.14601)|-|<details><summary>detail</summary>Accepted by Interspeech 2025</details>|\n", "DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation": "|**2025-5-14**|**DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation**|Siqi Yin et.al|[paper](https://arxiv.org/abs/2505.09927)|-|-|\n"}, "object detection": {"Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection": "|**2025-6-26**|**Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection**|Tobias J. Riedlinger et.al|[paper](https://arxiv.org/abs/2506.21486)|-|-|\n", "Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes": "|**2025-6-26**|**Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes**|Zhangjun Zhou et.al|[paper](https://arxiv.org/abs/2412.10943)|[code](https://github.com/ssecv/USCNet.)|-|\n", "DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic": "|**2025-6-26**|**DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic**|Munish Monga et.al|[paper](https://arxiv.org/abs/2506.21260)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "ROA-BEV: 2D Region-Oriented Attention for BEV-based 3D Object Detection": "|**2025-6-26**|**ROA-BEV: 2D Region-Oriented Attention for BEV-based 3D Object Detection**|Jiwei Chen et.al|[paper](https://arxiv.org/abs/2410.10298)|[code](https://github.com/DFLyan/ROA-BEV.)|<details><summary>detail</summary>accepted by IROS 2025</details>|\n", "LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection": "|**2025-6-26**|**LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection**|Lei Hao et.al|[paper](https://arxiv.org/abs/2506.21018)|[code](https://github.com/leileilei2000/LASFNet.)|-|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos": "|**2025-6-25**|**Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos**|Yitong Quan et.al|[paper](https://arxiv.org/abs/2506.20550)|-|<details><summary>detail</summary>Submitted to ECMR 2025</details>|\n", "Align and Distill: Unifying and Improving Domain Adaptive Object Detection": "|**2025-6-23**|**Align and Distill: Unifying and Improving Domain Adaptive Object Detection**|Justin Kay et.al|[paper](https://arxiv.org/abs/2403.12029)|[code](https://github.com/justinkay/aldi)|<details><summary>detail</summary>TMLR camera ready (Featured Certification)</details>|\n", "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation": "|**2025-6-23**|**LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation**|Yang Zhou et.al|[paper](https://arxiv.org/abs/2503.13794)|-|-|\n", "On the Robustness of Human-Object Interaction Detection against Distribution Shift": "|**2025-6-22**|**On the Robustness of Human-Object Interaction Detection against Distribution Shift**|Chi Xie et.al|[paper](https://arxiv.org/abs/2506.18021)|-|<details><summary>detail</summary>This work has been submitted to the IEEE for possible publication</details>|\n", "Towards Reflected Object Detection: A Benchmark": "|**2025-6-22**|**Towards Reflected Object Detection: A Benchmark**|Yiquan Wu et.al|[paper](https://arxiv.org/abs/2407.05575)|[code](https://github.com/jirouvan/ROD.)|-|\n", "DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training": "|**2025-6-21**|**DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training**|Chen Xin et.al|[paper](https://arxiv.org/abs/2407.09174)|[code](https://github.com/chen-xin-94/DART.)|<details><summary>detail</summary>Corrected minor typos</details>|\n", "Efficient Feature Aggregation and Scale-Aware Regression for Monocular 3D Object Detection": "|**2025-6-21**|**Efficient Feature Aggregation and Scale-Aware Regression for Monocular 3D Object Detection**|Yifan Wang et.al|[paper](https://arxiv.org/abs/2411.02747)|-|-|\n", "YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception": "|**2025-6-21**|**YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception**|Mengqi Lei et.al|[paper](https://arxiv.org/abs/2506.17733)|[code](https://github.com/iMoonLab/yolov13.)|-|\n", "CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection": "|**2025-6-21**|**CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection**|Wei Haolin et.al|[paper](https://arxiv.org/abs/2506.17679)|-|-|\n"}, "domain adaptation": {"TITAN: Query-Token based Domain Adaptive Adversarial Learning": "|**2025-6-26**|**TITAN: Query-Token based Domain Adaptive Adversarial Learning**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2506.21484)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability": "|**2025-6-26**|**Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability**|Boyong He et.al|[paper](https://arxiv.org/abs/2506.21042)|[code](https://github.com/heboyong/Fitness-Generalization-Transferability)|<details><summary>detail</summary>Accepted by ICCV2025</details>|\n", "crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023": "|**2025-6-24**|**crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023**|Navodini Wijethilake et.al|[paper](https://arxiv.org/abs/2506.12006)|-|-|\n", "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning": "|**2025-6-24**|**Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning**|Harisankar Babu et.al|[paper](https://arxiv.org/abs/2506.19592)|-|-|\n", "M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition": "|**2025-6-24**|**M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition**|Ting Luo et.al|[paper](https://arxiv.org/abs/2404.15615)|-|-|\n", "Progressive Modality Cooperation for Multi-Modality Domain Adaptation": "|**2025-6-24**|**Progressive Modality Cooperation for Multi-Modality Domain Adaptation**|Weichen Zhang et.al|[paper](https://arxiv.org/abs/2506.19316)|-|-|\n", "Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation": "|**2025-6-23**|**Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation**|Weichen Zhang et.al|[paper](https://arxiv.org/abs/2506.19267)|-|-|\n", "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation": "|**2025-6-23**|**Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation**|Junjie Chen et.al|[paper](https://arxiv.org/abs/2504.01668)|-|<details><summary>detail</summary>This paper has been accepted to the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)</details>|\n", "Align and Distill: Unifying and Improving Domain Adaptive Object Detection": "|**2025-6-23**|**Align and Distill: Unifying and Improving Domain Adaptive Object Detection**|Justin Kay et.al|[paper](https://arxiv.org/abs/2403.12029)|[code](https://github.com/justinkay/aldi)|<details><summary>detail</summary>TMLR camera ready (Featured Certification)</details>|\n", "DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data": "|**2025-6-22**|**DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data**|Sabbir Ahmed et.al|[paper](https://arxiv.org/abs/2506.18173)|-|<details><summary>detail</summary>Submitted to ACPR Springer</details>|\n", "Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction": "|**2025-6-22**|**Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction**|Rui An et.al|[paper](https://arxiv.org/abs/2506.18939)|-|-|\n", "IDAL: Improved Domain Adaptive Learning for Natural Images Dataset": "|**2025-6-22**|**IDAL: Improved Domain Adaptive Learning for Natural Images Dataset**|Ravi Kant Gupta et.al|[paper](https://arxiv.org/abs/2506.17931)|-|<details><summary>detail</summary>Accepted in ICPR'24 (International Conference on Pattern Recognition)</details>|\n", "When can isotropy help adapt LLMs' next word prediction to numerical domains?": "|**2025-6-20**|**When can isotropy help adapt LLMs' next word prediction to numerical domains?**|Rashed Shelim et.al|[paper](https://arxiv.org/abs/2505.17135)|-|-|\n", "Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation": "|**2025-6-20**|**Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation**|Quoc Thinh Vo et.al|[paper](https://arxiv.org/abs/2506.17409)|-|<details><summary>detail</summary>This paper has been accepted for the 33rd European Signal Processing Conference (EUSIPCO) 2025 in Palermo</details>|\n"}, "domain generalization": {"Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability": "|**2025-6-26**|**Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability**|Boyong He et.al|[paper](https://arxiv.org/abs/2506.21042)|[code](https://github.com/heboyong/Fitness-Generalization-Transferability)|<details><summary>detail</summary>Accepted by ICCV2025</details>|\n", "FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization": "|**2025-6-25**|**FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization**|Ha Min Son et.al|[paper](https://arxiv.org/abs/2506.20841)|-|-|\n", "General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound": "|**2025-6-24**|**General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound**|Jakob Ambsdorf et.al|[paper](https://arxiv.org/abs/2506.19552)|-|<details><summary>detail</summary>Submitted version of paper accepted at MICCAI 2025</details>|\n", "Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey": "|**2025-6-23**|**Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey**|Xinyao Li et.al|[paper](https://arxiv.org/abs/2506.18504)|-|-|\n", "RLPR: Extrapolating RLVR to General Domains without Verifiers": "|**2025-6-22**|**RLPR: Extrapolating RLVR to General Domains without Verifiers**|Tianyu Yu et.al|[paper](https://arxiv.org/abs/2506.18254)|[code](https://github.com/openbmb/RLPR)|<details><summary>detail</summary>Project Website: https://github</details>|\n", "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation": "|**2025-6-22**|**RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation**|Tianxing Chen et.al|[paper](https://arxiv.org/abs/2506.18088)|[code](https://robotwin-platform.github.io/)|<details><summary>detail</summary>Project Page: https://robotwin-platform</details>|\n", "Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains": "|**2025-6-21**|**Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains**|Zhuo He et.al|[paper](https://arxiv.org/abs/2506.17718)|-|<details><summary>detail</summary>ICML 2025</details>|\n", "Domain Generalization using Action Sequences for Egocentric Action Recognition": "|**2025-6-21**|**Domain Generalization using Action Sequences for Egocentric Action Recognition**|Amirshayan Nasirimajd et.al|[paper](https://arxiv.org/abs/2506.17685)|[code](https://github.com/Ashayan97/SeqDG)|<details><summary>detail</summary>Pattern Recognition Letters</details>|\n", "When and How Does CLIP Enable Domain and Compositional Generalization?": "|**2025-6-20**|**When and How Does CLIP Enable Domain and Compositional Generalization?**|Elias Kempf et.al|[paper](https://arxiv.org/abs/2502.09507)|-|<details><summary>detail</summary>ICML 2025 (Spotlight)</details>|\n", "How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension": "|**2025-6-19**|**How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension**|Cynthia Dwork et.al|[paper](https://arxiv.org/abs/2506.16704)|-|-|\n", "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation": "|**2025-6-19**|**GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation**|Yilin Xiao et.al|[paper](https://arxiv.org/abs/2506.02404)|-|-|\n", "Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization": "|**2025-6-19**|**Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization**|Jiyao Wang et.al|[paper](https://arxiv.org/abs/2506.16160)|-|-|\n", "Video-Guided Text-to-Music Generation Using Public Domain Movie Collections": "|**2025-6-18**|**Video-Guided Text-to-Music Generation Using Public Domain Movie Collections**|Haven Kim et.al|[paper](https://arxiv.org/abs/2506.12573)|[code](https://havenpersona.github.io/ossl-v1.)|<details><summary>detail</summary>ISMIR 2025 regular paper</details>|\n", "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation": "|**2025-6-18**|**Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2506.09881)|[code](https://github.com/anonymouse-9c53tp182bvz/Vireo.)|-|\n", "Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains": "|**2025-6-17**|**Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains**|Emanuel Moss et.al|[paper](https://arxiv.org/abs/2506.14567)|-|-|\n"}, "vision language": {"Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration": "|**2025-6-26**|**Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration**|Jiahe Chen et.al|[paper](https://arxiv.org/abs/2506.21509)|-|-|\n", "ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models": "|**2025-6-26**|**ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models**|Hongbo Liu et.al|[paper](https://arxiv.org/abs/2506.21356)|-|-|\n", "World-aware Planning Narratives Enhance Large Vision-Language Model Planner": "|**2025-6-26**|**World-aware Planning Narratives Enhance Large Vision-Language Model Planner**|Junhao Shi et.al|[paper](https://arxiv.org/abs/2506.21230)|-|-|\n", "V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling": "|**2025-6-26**|**V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling**|Junwei You et.al|[paper](https://arxiv.org/abs/2506.21041)|-|-|\n", "SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes": "|**2025-6-26**|**SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes**|Yifan Yang et.al|[paper](https://arxiv.org/abs/2506.20990)|-|-|\n", "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference": "|**2025-6-25**|**AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference**|Kai Huang et.al|[paper](https://arxiv.org/abs/2503.23956)|-|<details><summary>detail</summary>We have withdrawn this manuscript due to a critical error in the methodology which affects the validity of the main results</details>|\n", "ClimateIQA: A New Dataset and Benchmark to Advance Vision-Language Models in Meteorology Anomalies Analysis": "|**2025-6-25**|**ClimateIQA: A New Dataset and Benchmark to Advance Vision-Language Models in Meteorology Anomalies Analysis**|Jian Chen et.al|[paper](https://arxiv.org/abs/2406.09838)|-|-|\n", "Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models": "|**2025-6-25**|**Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models**|Cansu Korkmaz et.al|[paper](https://arxiv.org/abs/2506.20832)|-|-|\n", "HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction": "|**2025-6-25**|**HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction**|Zhonghao Shi et.al|[paper](https://arxiv.org/abs/2506.20566)|[code](https://github.com/interaction-lab/HRIBench.)|<details><summary>detail</summary>the 19th International Symposium on Experimental Robotics (ISER 2025)</details>|\n", "CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition": "|**2025-6-25**|**CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition**|Joerg Deigmoeller et.al|[paper](https://arxiv.org/abs/2506.20373)|-|-|\n", "World-Consistent Data Generation for Vision-and-Language Navigation": "|**2025-6-25**|**World-Consistent Data Generation for Vision-and-Language Navigation**|Yu Zhong et.al|[paper](https://arxiv.org/abs/2412.06413)|-|-|\n", "LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models": "|**2025-6-25**|**LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models**|Hengyuan Zhao et.al|[paper](https://arxiv.org/abs/2503.21227)|-|<details><summary>detail</summary>Preprint</details>|\n", "VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning": "|**2025-6-25**|**VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning**|Zhangyang Qi et.al|[paper](https://arxiv.org/abs/2506.17221)|-|<details><summary>detail</summary>project page: vlnr1</details>|\n", "Shape and Texture Recognition in Large Vision-Language Models": "|**2025-6-24**|**Shape and Texture Recognition in Large Vision-Language Models**|Sagi Eppel et.al|[paper](https://arxiv.org/abs/2503.23062)|-|-|\n", "GlyphPattern: An Abstract Pattern Recognition Benchmark for Vision-Language Models": "|**2025-6-24**|**GlyphPattern: An Abstract Pattern Recognition Benchmark for Vision-Language Models**|Zixuan Wu et.al|[paper](https://arxiv.org/abs/2408.05894)|-|<details><summary>detail</summary>Journal ref:Findings of the ACL 2025</details>|\n"}}