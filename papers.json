{"source-free": {"Text-Driven Causal Representation Learning for Source-Free Domain Generalization": "|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|\n", "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting": "|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|\n", "Source-Free Domain Adaptation via Multi-view Contrastive Learning": "|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|\n", "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-6-26**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "SFDLA: Source-Free Document Layout Analysis": "|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|\n", "Unified Source-Free Domain Adaptation": "|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|\n", "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|\n", "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization": "|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|\n", "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data": "|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|\n", "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation": "|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|\n", "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation": "|**2025-5-30**|**Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation**|Prasanna Reddy Pulakurthi et.al|[paper](https://arxiv.org/abs/2505.24216)|[code](https://github.com/PrasannaPulakurthi/SPM)|-|\n", "Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation": "|**2025-5-27**|**Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation**|Peihua Deng et.al|[paper](https://arxiv.org/abs/2411.16064)|[code](https://github.com/dengpeihua/GROTO.)|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Training-Free Multi-Step Audio Source Separation": "|**2025-5-26**|**Training-Free Multi-Step Audio Source Separation**|Yongyi Zang et.al|[paper](https://arxiv.org/abs/2505.19534)|-|-|\n"}, "object detection": {"OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments": "|**2025-7-18**|**OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments**|Hayat Ullah et.al|[paper](https://arxiv.org/abs/2507.12396)|-|-|\n", "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection": "|**2025-7-18**|**Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection**|Yujian Mo et.al|[paper](https://arxiv.org/abs/2507.13899)|-|-|\n", "Entropy Loss: An Interpretability Amplifier of 3D Object Detection Network for Intelligent Driving": "|**2025-7-18**|**Entropy Loss: An Interpretability Amplifier of 3D Object Detection Network for Intelligent Driving**|Haobo Yang et.al|[paper](https://arxiv.org/abs/2409.00839)|[code](https://github.com/yhbcode000/Eloss-Interpretability.)|-|\n", "Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation": "|**2025-7-17**|**Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation**|Masahiro Ogawa et.al|[paper](https://arxiv.org/abs/2507.13628)|-|-|\n", "A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains": "|**2025-7-17**|**A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains**|Antonio Finocchiaro et.al|[paper](https://arxiv.org/abs/2507.13326)|-|-|\n", "RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images": "|**2025-7-17**|**RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images**|Xiaozheng Jiang et.al|[paper](https://arxiv.org/abs/2507.13120)|-|-|\n", "Decoupled PROB: Decoupled Query Initialization Tasks and Objectness-Class Learning for Open World Object Detection": "|**2025-7-17**|**Decoupled PROB: Decoupled Query Initialization Tasks and Objectness-Class Learning for Open World Object Detection**|Riku Inoue et.al|[paper](https://arxiv.org/abs/2507.13085)|-|<details><summary>detail</summary>This paper has been accepted to WACV 2025 (Tucson</details>|\n", "SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery": "|**2025-7-16**|**SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery**|Peijun Wang et.al|[paper](https://arxiv.org/abs/2507.12727)|[code](https://github.com/iamwangxiaobai/SOD-YOLO.)|<details><summary>detail</summary>ACM Class:I</details>|\n", "2.5D Object Detection for Intelligent Roadside Infrastructure": "|**2025-7-16**|**2.5D Object Detection for Intelligent Roadside Infrastructure**|Nikolai Polley et.al|[paper](https://arxiv.org/abs/2507.03564)|[code](https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection)|<details><summary>detail</summary>2025 IEEE 28th International Conference on Intelligent Transportation Systems (ITSC)</details>|\n", "SpikeDet: Better Firing Patterns for Accurate and Energy-Efficient Object Detection with Spiking Neuron Networks": "|**2025-7-16**|**SpikeDet: Better Firing Patterns for Accurate and Energy-Efficient Object Detection with Spiking Neuron Networks**|Yimeng Fan et.al|[paper](https://arxiv.org/abs/2501.15151)|-|-|\n", "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection": "|**2025-7-16**|**SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection**|Xiwei Zhang et.al|[paper](https://arxiv.org/abs/2507.12017)|-|-|\n", "Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery": "|**2025-7-15**|**Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery**|Nicolas Drapier et.al|[paper](https://arxiv.org/abs/2507.11040)|-|-|\n", "Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection": "|**2025-7-15**|**Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection**|Quan Bi Pay et.al|[paper](https://arxiv.org/abs/2507.10977)|[code](https://github.com/henry-pay/RayEncoder].)|<details><summary>detail</summary>International Joint Conference on Neural Networks (IJCNN 2025)</details>|\n", "LLM-Guided Agentic Object Detection for Open-World Understanding": "|**2025-7-14**|**LLM-Guided Agentic Object Detection for Open-World Understanding**|Furkan Mumcu et.al|[paper](https://arxiv.org/abs/2507.10844)|-|-|\n", "Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection": "|**2025-7-14**|**Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection**|Huiyi Wang et.al|[paper](https://arxiv.org/abs/2507.10814)|-|-|\n"}, "domain adaptation": {"SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering": "|**2025-7-18**|**SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering**|Durgesh Singh et.al|[paper](https://arxiv.org/abs/2507.13779)|-|<details><summary>detail</summary>Journal ref:Pattern Recognition 2025</details>|\n", "Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica": "|**2025-7-17**|**Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica**|Jaber Daneshamooz et.al|[paper](https://arxiv.org/abs/2507.13476)|-|-|\n", "SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with Equivariant Neural Networks": "|**2025-7-17**|**SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with Equivariant Neural Networks**|Sneh Pandya et.al|[paper](https://arxiv.org/abs/2501.14048)|[code](https://github.com/deepskies/SIDDA)|-|\n", "A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique": "|**2025-7-16**|**A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique**|Homare Sueyoshi et.al|[paper](https://arxiv.org/abs/2507.12730)|-|-|\n", "Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms": "|**2025-7-16**|**Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms**|Keru Wu et.al|[paper](https://arxiv.org/abs/2309.10301)|-|-|\n", "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection": "|**2025-7-16**|**SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection**|Xiwei Zhang et.al|[paper](https://arxiv.org/abs/2507.12017)|-|-|\n", "Dual form Complementary Masking for Domain-Adaptive Image Segmentation": "|**2025-7-16**|**Dual form Complementary Masking for Domain-Adaptive Image Segmentation**|Jiawen Wang et.al|[paper](https://arxiv.org/abs/2507.12008)|-|<details><summary>detail</summary>Accepted by ICML 2025</details>|\n", "Few-Shot Radar Signal Recognition through Self-Supervised Learning and Radio Frequency Domain Adaptation": "|**2025-7-15**|**Few-Shot Radar Signal Recognition through Self-Supervised Learning and Radio Frequency Domain Adaptation**|Zi Huang et.al|[paper](https://arxiv.org/abs/2501.03461)|-|-|\n", "Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving": "|**2025-7-15**|**Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving**|Chinmay Vilas Samak et.al|[paper](https://arxiv.org/abs/2507.00236)|-|-|\n", "SA-GDA: Spectral Augmentation for Graph Domain Adaptation": "|**2025-7-14**|**SA-GDA: Spectral Augmentation for Graph Domain Adaptation**|Jinhui Pang et.al|[paper](https://arxiv.org/abs/2408.09189)|-|-|\n", "Domain-Adaptive Small Language Models for Structured Tax Code Prediction": "|**2025-7-14**|**Domain-Adaptive Small Language Models for Structured Tax Code Prediction**|Souvik Nath et.al|[paper](https://arxiv.org/abs/2507.10880)|-|-|\n", "DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation": "|**2025-7-14**|**DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation**|Yiru Yang et.al|[paper](https://arxiv.org/abs/2507.10313)|-|-|\n", "Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation": "|**2025-7-14**|**Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation**|Manuel R\u00f6der et.al|[paper](https://arxiv.org/abs/2507.10160)|[code](http://dx.doi.org/10.5220/0012351900003654)|<details><summary>detail</summary>Extension of http://dx</details>|\n", "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance": "|**2025-7-13**|**NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance**|Hanwool Lee et.al|[paper](https://arxiv.org/abs/2507.09601)|-|<details><summary>detail</summary>Under Review</details>|\n", "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data": "|**2025-7-12**|**Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data**|Timothy Chase Jr et.al|[paper](https://arxiv.org/abs/2507.09420)|[code](https://tjchase34.github.io/assets/pdfs/rss_poster.pdf)|<details><summary>detail</summary>Presented at the RSS Space Robotics Workshop 2025</details>|\n", "Confidence sharing adaptation for out-of-domain human pose and shape estimation": "|**2025-7-19**|**Confidence sharing adaptation for out-of-domain human pose and shape estimation**|T Yue et.al|[paper](https://www.sciencedirect.com/science/article/pii/S1077314224001322)|-|<details><summary>detail</summary>Computer Vision and Image\u00a0\u2026, 2024 Elsevier</details>|\n", "\u2026\u00a0Carbon Content and Temperature in Bof Steelmaking Based on Adaptive Balanced Joint Distribution Alignment Domain Adaptation with Variational Autoencoder": "|**2025-7-18**|**\u2026\u00a0Carbon Content and Temperature in Bof Steelmaking Based on Adaptive Balanced Joint Distribution Alignment Domain Adaptation with Variational Autoencoder**|Z Liu et.al|[paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4863841)|-|<details><summary>detail</summary>Available at SSRN 4863841 papers.ssrn.com</details>|\n", "POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt Tuning": "|**2025-7-18**|**POND: Multi-Source Time Series Domain Adaptation with Information-Aware Prompt Tuning**|J Wang et.al|[paper](https://www.researchgate.net/profile/Junxiang-Wang-3/publication/381225385_POND_Multi-Source_Time_Series_Domain_Adaptation_with_Information-Aware_Prompt_Tuning/links/6663974e85a4ee7261ae011e/POND-Multi-Source-Time-Series-Domain-Adaptation-with-Information-Aware-Prompt-Tuning.pdf)|[code](https://paperswithcode.com/paper/prompt-based-domain-discrimination-for-multi)|<details><summary>detail</summary>2024 researchgate.net</details>|\n", "Continuous Test-time Domain Adaptation for Efficient Fault Detection under Evolving Operating Conditions": "|**2025-7-18**|**Continuous Test-time Domain Adaptation for Efficient Fault Detection under Evolving Operating Conditions**|H Sun et.al|[paper](https://ui.adsabs.harvard.edu/abs/2024arXiv240606607S/abstract)|[code](https://paperswithcode.com/paper/continuous-test-time-domain-adaptation-for)|-|\n", "Source-Free Domain Adaptation for Speaker Verification in Data-Scarce Languages and Noisy Channels": "|**2025-7-18**|**Source-Free Domain Adaptation for Speaker Verification in Data-Scarce Languages and Noisy Channels**|S Salo Elia et.al|[paper](https://ui.adsabs.harvard.edu/abs/2024arXiv240605863S/abstract)|[code](https://paperswithcode.com/paper/source-free-domain-adaptation-for-speaker)|-|\n", "Cross-Domain Classification Based on Frequency Component Adaptation for Remote Sensing Images": "|**2025-7-18**|**Cross-Domain Classification Based on Frequency Component Adaptation for Remote Sensing Images**|P Zhu et.al|[paper](https://www.mdpi.com/2072-4292/16/12/2134)|-|<details><summary>detail</summary>Remote Sensing, 2024 mdpi.com</details>|\n", "TSFAN: Tensorized spatial-frequency attention network with domain adaptation for cross-session EEG-based biometric recognition": "|**2025-7-18**|**TSFAN: Tensorized spatial-frequency attention network with domain adaptation for cross-session EEG-based biometric recognition**|X Jin et.al|[paper](https://automatedtest.iopscience.iop.org/article/10.1088/1741-2552/ad5761)|-|<details><summary>detail</summary>Journal of\u00a0\u2026, 2024 automatedtest.iopscience.iop.org</details>|\n", "SE/BN Adapter: Parametric Efficient Domain Adaptation for Speaker Recognition": "|**2025-7-18**|**SE/BN Adapter: Parametric Efficient Domain Adaptation for Speaker Recognition**|T Wang et.al|[paper](https://arxiv.org/abs/2406.07832)|-|-|\n", "Novel Deep Learning Domain Adaptation Approach for Object Detection Using Semi-Self Building Dataset and Modified YOLOv4": "|**2025-7-17**|**Novel Deep Learning Domain Adaptation Approach for Object Detection Using Semi-Self Building Dataset and Modified YOLOv4**|A Gomaa et.al|[paper](https://www.mdpi.com/2032-6653/15/6/255)|-|<details><summary>detail</summary>World Electric Vehicle Journal, 2024 mdpi.com</details>|\n", "Global self-sustaining and local inheritance for source-free unsupervised domain adaptation": "|**2025-7-17**|**Global self-sustaining and local inheritance for source-free unsupervised domain adaptation**|L Peng et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0031320324004308)|-|<details><summary>detail</summary>Pattern Recognition, 2024 Elsevier</details>|\n"}, "domain generalization": {"Generative Multi-Target Cross-Domain Recommendation": "|**2025-7-17**|**Generative Multi-Target Cross-Domain Recommendation**|Jinqiu Jin et.al|[paper](https://arxiv.org/abs/2507.12871)|-|<details><summary>detail</summary>fix author information</details>|\n", "Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica": "|**2025-7-17**|**Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica**|Jaber Daneshamooz et.al|[paper](https://arxiv.org/abs/2507.13476)|-|-|\n", "CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings": "|**2025-7-17**|**CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings**|Daniil Orel et.al|[paper](https://arxiv.org/abs/2503.13733)|-|-|\n", "A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints": "|**2025-7-17**|**A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints**|Youssef Tawfilis et.al|[paper](https://arxiv.org/abs/2507.12979)|[code](https://github.com/youssefga28/HuSCF-GAN.)|-|\n", "Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization": "|**2025-7-17**|**Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization**|Ziyi Wang et.al|[paper](https://arxiv.org/abs/2507.12851)|[code](https://github.com/bitPrincy/SRE-DG.)|<details><summary>detail</summary>\\c{opyright} 20XX IEEE</details>|\n", "Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation": "|**2025-7-15**|**Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2504.12753)|[code](https://github.com/anonymouse-xzrptkvyqc/DepthForge.)|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "Text-Driven Causal Representation Learning for Source-Free Domain Generalization": "|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|\n", "Fair Domain Generalization: An Information-Theoretic View": "|**2025-7-13**|**Fair Domain Generalization: An Information-Theoretic View**|Tangzheng Lian et.al|[paper](https://arxiv.org/abs/2507.05823)|-|-|\n", "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze": "|**2025-7-11**|**RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze**|Yunsoo Kim et.al|[paper](https://arxiv.org/abs/2507.09097)|-|-|\n", "Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement": "|**2025-7-11**|**Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement**|Jia-Xuan Jiang et.al|[paper](https://arxiv.org/abs/2507.08340)|[code](https://github.com/HopkinsKwong/MCCSDG)|<details><summary>detail</summary>Accepted by ACMMM 25</details>|\n", "From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry": "|**2025-7-10**|**From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry**|Chetan Arora et.al|[paper](https://arxiv.org/abs/2507.07689)|-|-|\n", "Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation": "|**2025-7-10**|**Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation**|Junyu Luo et.al|[paper](https://arxiv.org/abs/2507.07621)|-|<details><summary>detail</summary>ICML 2025</details>|\n", "SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains": "|**2025-7-9**|**SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains**|Krithika Ramesh et.al|[paper](https://arxiv.org/abs/2507.07229)|-|-|\n", "Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis": "|**2025-7-9**|**Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis**|Srihari K B et.al|[paper](https://arxiv.org/abs/2507.06571)|-|-|\n", "AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework": "|**2025-7-7**|**AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework**|Suoxiang Zhang et.al|[paper](https://arxiv.org/abs/2507.05621)|-|-|\n", "Entity-centric multi-domain transformer for improving generalization in fake news detection": "|**2025-7-19**|**Entity-centric multi-domain transformer for improving generalization in fake news detection**|P Bazmi et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0306457324001663)|-|<details><summary>detail</summary>Information Processing &\u00a0\u2026, 2024 Elsevier</details>|\n", "Fine-Grained Domain Generalization with Feature Structuralization": "|**2025-7-18**|**Fine-Grained Domain Generalization with Feature Structuralization**|W Yu et.al|[paper](https://arxiv.org/abs/2406.09166)|[code](https://github.com/yuwlong666/fsdg)|-|\n", "Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification": "|**2025-7-16**|**Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification**|S Addepalli et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Addepalli_Leveraging_Vision-Language_Models_for_Improving_Domain_Generalization_in_Image_Classification_CVPR_2024_paper.html)|[code](https://github.com/val-iisc/VL2V-ADiP)|<details><summary>detail</summary>Proceedings of the\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "Disentangled Prompt Representation for Domain Generalization": "|**2025-7-16**|**Disentangled Prompt Representation for Domain Generalization**|D Cheng et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_Disentangled_Prompt_Representation_for_Domain_Generalization_CVPR_2024_paper.html)|[code](https://github.com/henry123-boy/SpaTracker)|<details><summary>detail</summary>Proceedings of the\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "Supplementary Materials: Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential in Open Domain Generalization": "|**2025-7-16**|**Supplementary Materials: Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential in Open Domain Generalization**|M Singha et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Singha_Unknown_Prompt_the_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>openaccess.thecvf.com</details>|\n", "Supplementary Material for DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning": "|**2025-7-16**|**Supplementary Material for DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning**|B Dataset - openaccess.thecvf.com et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Bai_DiPrompT_Disentangled_Prompt_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>openaccess.thecvf.com</details>|\n", "Supplementary Material for Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization": "|**2025-7-16**|**Supplementary Material for Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization**|K Le et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Le_Efficiently_Assemble_Normalization_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>Phuoc, KS Wong openaccess.thecvf.com</details>|\n", "Domain Generalization for Crop Segmentation with Standardized Ensemble Knowledge Distillation": "|**2025-7-16**|**Domain Generalization for Crop Segmentation with Standardized Ensemble Knowledge Distillation**|S Angarano et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024W/Vision4Ag/html/Angarano_Domain_Generalization_for_Crop_Segmentation_with_Standardized_Ensemble_Knowledge_Distillation_CVPRW_2024_paper.html)|[code](https://github.com/pic4ser/agriseg)|<details><summary>detail</summary>Proceedings of the\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "MixStyle-Based Contrastive Test-Time Adaptation: Pathway to Domain Generalization": "|**2025-7-16**|**MixStyle-Based Contrastive Test-Time Adaptation: Pathway to Domain Generalization**|K Yamashita et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024W/MAT/html/Yamashita_MixStyle-Based_Contrastive_Test-Time_Adaptation_Pathway_to_Domain_Generalization_CVPRW_2024_paper.html)|-|<details><summary>detail</summary>\u2026\u00a0of the IEEE/CVF Conference on\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "Fault vibration model driven fault-aware domain generalization framework for bearing fault diagnosis": "|**2025-7-16**|**Fault vibration model driven fault-aware domain generalization framework for bearing fault diagnosis**|B Pang et.al|[paper](https://www.sciencedirect.com/science/article/pii/S1474034624002684)|-|<details><summary>detail</summary>Advanced Engineering\u00a0\u2026, 2024 Elsevier</details>|\n"}, "vision language": {"How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study": "|**2025-7-18**|**How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study**|Che Liu et.al|[paper](https://arxiv.org/abs/2507.11200)|-|<details><summary>detail</summary>Technical report</details>|\n", "VLA-Mark: A cross modal watermark for large vision-language alignment model": "|**2025-7-18**|**VLA-Mark: A cross modal watermark for large vision-language alignment model**|Shuliang Liu et.al|[paper](https://arxiv.org/abs/2507.14067)|-|-|\n", "EdgeVLA: Efficient Vision-Language-Action Models": "|**2025-7-18**|**EdgeVLA: Efficient Vision-Language-Action Models**|Pawe\u0142 Budzianowski et.al|[paper](https://arxiv.org/abs/2507.14049)|[code](https://github.com/kscalelabs/evla)|<details><summary>detail</summary>Journal ref:IROS-MoMA3 Workshop 2024</details>|\n", "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models": "|**2025-7-18**|**When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models**|Francesco Ortu et.al|[paper](https://arxiv.org/abs/2507.13868)|-|-|\n", "GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing": "|**2025-7-18**|**GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing**|Xianzhi Ma et.al|[paper](https://arxiv.org/abs/2507.05887)|-|-|\n", "Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment": "|**2025-7-18**|**Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment**|Angelos Zavras et.al|[paper](https://arxiv.org/abs/2402.09816)|[code](https://github.com/Orion-AI-Lab/MindTheModalityGap.)|<details><summary>detail</summary>the ISPRS Journal of Photogrammetry and Remote Sensing</details>|\n", "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions": "|**2025-7-18**|**Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions**|Pu Jian et.al|[paper](https://arxiv.org/abs/2507.13773)|-|<details><summary>detail</summary>ACL2025 Main</details>|\n", "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos": "|**2025-7-18**|**EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**|Ruihan Yang et.al|[paper](https://arxiv.org/abs/2507.12440)|[code](https://rchalyang.github.io/EgoVLA)|<details><summary>detail</summary>More videos can be found on our website: https://rchalyang</details>|\n", "ZonUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding": "|**2025-7-18**|**ZonUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding**|ZongHan Hsieh et.al|[paper](https://arxiv.org/abs/2506.23491)|[code](https://github.com/Han1018/ZonUI-3B)|-|\n", "EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation": "|**2025-7-17**|**EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation**|Bingqian Lin et.al|[paper](https://arxiv.org/abs/2506.01551)|[code](https://github.com/expectorlin/EvolveNav.)|-|\n", "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning": "|**2025-7-17**|**VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning**|Senqiao Yang et.al|[paper](https://arxiv.org/abs/2507.13348)|[code](https://github.com/dvlab-research/VisionThink.)|<details><summary>detail</summary>Code and models are available at https://github</details>|\n", "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It": "|**2025-7-17**|**Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It**|Yulu Qin et.al|[paper](https://arxiv.org/abs/2507.13328)|-|-|\n", "SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models": "|**2025-7-17**|**SE-VLN: A Self-Evolving Vision-Language Navigation Framework Based on Multimodal Large Language Models**|Xiangyu Dong et.al|[paper](https://arxiv.org/abs/2507.13152)|-|-|\n", "Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching": "|**2025-7-17**|**Aligning Information Capacity Between Vision and Language via Dense-to-Sparse Feature Distillation for Image-Text Matching**|Yang Liu et.al|[paper](https://arxiv.org/abs/2503.14953)|-|-|\n", "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models": "|**2025-7-17**|**Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models**|Junjie Wu et.al|[paper](https://arxiv.org/abs/2410.23114)|[code](https://github.com/wujunjie1998/Tri-HE.)|<details><summary>detail</summary>Accepted by TMLR 2025</details>|\n"}}