{"source-free": {"Text-Driven Causal Representation Learning for Source-Free Domain Generalization": "|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|\n", "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting": "|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|\n", "Source-Free Domain Adaptation via Multi-view Contrastive Learning": "|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|\n", "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-6-26**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "SFDLA: Source-Free Document Layout Analysis": "|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|\n", "Unified Source-Free Domain Adaptation": "|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|\n", "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|\n", "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization": "|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|\n", "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data": "|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|\n", "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation": "|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|\n", "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation": "|**2025-5-30**|**Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation**|Prasanna Reddy Pulakurthi et.al|[paper](https://arxiv.org/abs/2505.24216)|[code](https://github.com/PrasannaPulakurthi/SPM)|-|\n", "Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation": "|**2025-5-27**|**Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation**|Peihua Deng et.al|[paper](https://arxiv.org/abs/2411.16064)|[code](https://github.com/dengpeihua/GROTO.)|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Training-Free Multi-Step Audio Source Separation": "|**2025-5-26**|**Training-Free Multi-Step Audio Source Separation**|Yongyi Zang et.al|[paper](https://arxiv.org/abs/2505.19534)|-|-|\n"}, "object detection": {"OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments": "|**2025-7-16**|**OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments**|Hayat Ullah et.al|[paper](https://arxiv.org/abs/2507.12396)|-|-|\n", "2.5D Object Detection for Intelligent Roadside Infrastructure": "|**2025-7-16**|**2.5D Object Detection for Intelligent Roadside Infrastructure**|Nikolai Polley et.al|[paper](https://arxiv.org/abs/2507.03564)|[code](https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection)|<details><summary>detail</summary>2025 IEEE 28th International Conference on Intelligent Transportation Systems (ITSC)</details>|\n", "SpikeDet: Better Firing Patterns for Accurate and Energy-Efficient Object Detection with Spiking Neuron Networks": "|**2025-7-16**|**SpikeDet: Better Firing Patterns for Accurate and Energy-Efficient Object Detection with Spiking Neuron Networks**|Yimeng Fan et.al|[paper](https://arxiv.org/abs/2501.15151)|-|-|\n", "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection": "|**2025-7-16**|**SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection**|Xiwei Zhang et.al|[paper](https://arxiv.org/abs/2507.12017)|-|-|\n", "Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery": "|**2025-7-15**|**Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery**|Nicolas Drapier et.al|[paper](https://arxiv.org/abs/2507.11040)|-|-|\n", "Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection": "|**2025-7-15**|**Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection**|Quan Bi Pay et.al|[paper](https://arxiv.org/abs/2507.10977)|[code](https://github.com/henry-pay/RayEncoder].)|<details><summary>detail</summary>International Joint Conference on Neural Networks (IJCNN 2025)</details>|\n", "LLM-Guided Agentic Object Detection for Open-World Understanding": "|**2025-7-14**|**LLM-Guided Agentic Object Detection for Open-World Understanding**|Furkan Mumcu et.al|[paper](https://arxiv.org/abs/2507.10844)|-|-|\n", "Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection": "|**2025-7-14**|**Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection**|Huiyi Wang et.al|[paper](https://arxiv.org/abs/2507.10814)|-|-|\n", "Fine-Grained Zero-Shot Object Detection": "|**2025-7-14**|**Fine-Grained Zero-Shot Object Detection**|Hongxu Ma et.al|[paper](https://arxiv.org/abs/2507.10358)|-|<details><summary>detail</summary>Accepted by ACM MM'25</details>|\n", "Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?": "|**2025-7-14**|**Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?**|Miao Zhang et.al|[paper](https://arxiv.org/abs/2503.02687)|-|-|\n", "Measuring the Impact of Rotation Equivariance on Aerial Object Detection": "|**2025-7-14**|**Measuring the Impact of Rotation Equivariance on Aerial Object Detection**|Xiuyu Wu et.al|[paper](https://arxiv.org/abs/2507.09896)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline": "|**2025-7-12**|**Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline**|Shiyi Mu et.al|[paper](https://arxiv.org/abs/2507.09214)|[code](https://github.com/xxxx/xxx).)|<details><summary>detail</summary>under review</details>|\n", "HA-RDet: Hybrid Anchor Rotation Detector for Oriented Object Detection": "|**2025-7-12**|**HA-RDet: Hybrid Anchor Rotation Detector for Oriented Object Detection**|Phuc D. A. Nguyen et.al|[paper](https://arxiv.org/abs/2412.14379)|-|<details><summary>detail</summary>Bachelor thesis</details>|\n", "RoHOI: Robustness Benchmark for Human-Object Interaction Detection": "|**2025-7-11**|**RoHOI: Robustness Benchmark for Human-Object Interaction Detection**|Di Wen et.al|[paper](https://arxiv.org/abs/2507.09111)|[code](https://github.com/Kratos-Wen/RoHOI.)|<details><summary>detail</summary>Benchmarks</details>|\n", "Quantifying Context Bias in Domain Adaptation for Object Detection": "|**2025-7-11**|**Quantifying Context Bias in Domain Adaptation for Object Detection**|Hojun Son et.al|[paper](https://arxiv.org/abs/2409.14679)|-|<details><summary>detail</summary>Under review</details>|\n"}, "domain adaptation": {"Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms": "|**2025-7-16**|**Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms**|Keru Wu et.al|[paper](https://arxiv.org/abs/2309.10301)|-|-|\n", "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection": "|**2025-7-16**|**SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection**|Xiwei Zhang et.al|[paper](https://arxiv.org/abs/2507.12017)|-|-|\n", "Dual form Complementary Masking for Domain-Adaptive Image Segmentation": "|**2025-7-16**|**Dual form Complementary Masking for Domain-Adaptive Image Segmentation**|Jiawen Wang et.al|[paper](https://arxiv.org/abs/2507.12008)|-|<details><summary>detail</summary>Accepted by ICML 2025</details>|\n", "Few-Shot Radar Signal Recognition through Self-Supervised Learning and Radio Frequency Domain Adaptation": "|**2025-7-15**|**Few-Shot Radar Signal Recognition through Self-Supervised Learning and Radio Frequency Domain Adaptation**|Zi Huang et.al|[paper](https://arxiv.org/abs/2501.03461)|-|-|\n", "Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving": "|**2025-7-15**|**Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving**|Chinmay Vilas Samak et.al|[paper](https://arxiv.org/abs/2507.00236)|-|-|\n", "SA-GDA: Spectral Augmentation for Graph Domain Adaptation": "|**2025-7-14**|**SA-GDA: Spectral Augmentation for Graph Domain Adaptation**|Jinhui Pang et.al|[paper](https://arxiv.org/abs/2408.09189)|-|-|\n", "Domain-Adaptive Small Language Models for Structured Tax Code Prediction": "|**2025-7-14**|**Domain-Adaptive Small Language Models for Structured Tax Code Prediction**|Souvik Nath et.al|[paper](https://arxiv.org/abs/2507.10880)|-|-|\n", "DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation": "|**2025-7-14**|**DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation**|Yiru Yang et.al|[paper](https://arxiv.org/abs/2507.10313)|-|-|\n", "Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation": "|**2025-7-14**|**Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation**|Manuel R\u00f6der et.al|[paper](https://arxiv.org/abs/2507.10160)|[code](http://dx.doi.org/10.5220/0012351900003654)|<details><summary>detail</summary>Extension of http://dx</details>|\n", "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance": "|**2025-7-13**|**NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance**|Hanwool Lee et.al|[paper](https://arxiv.org/abs/2507.09601)|-|<details><summary>detail</summary>Under Review</details>|\n", "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data": "|**2025-7-12**|**Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data**|Timothy Chase Jr et.al|[paper](https://arxiv.org/abs/2507.09420)|[code](https://tjchase34.github.io/assets/pdfs/rss_poster.pdf)|<details><summary>detail</summary>Presented at the RSS Space Robotics Workshop 2025</details>|\n", "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting": "|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|\n", "CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification": "|**2025-7-12**|**CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification**|Nan Yin et.al|[paper](https://arxiv.org/abs/2306.04979)|-|-|\n", "Quantifying Context Bias in Domain Adaptation for Object Detection": "|**2025-7-11**|**Quantifying Context Bias in Domain Adaptation for Object Detection**|Hojun Son et.al|[paper](https://arxiv.org/abs/2409.14679)|-|<details><summary>detail</summary>Under review</details>|\n", "Adaptive Graph Integration for Cross-Domain Recommendation via Heterogeneous Graph Coordinators": "|**2025-7-10**|**Adaptive Graph Integration for Cross-Domain Recommendation via Heterogeneous Graph Coordinators**|Hengyu Zhang et.al|[paper](https://arxiv.org/abs/2410.11719)|[code](https://github.com/zhy99426/HAGO.)|<details><summary>detail</summary>Accept by SIGIR 2025</details>|\n"}, "domain generalization": {"Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation": "|**2025-7-15**|**Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2504.12753)|[code](https://github.com/anonymouse-xzrptkvyqc/DepthForge.)|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "Text-Driven Causal Representation Learning for Source-Free Domain Generalization": "|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|\n", "Fair Domain Generalization: An Information-Theoretic View": "|**2025-7-13**|**Fair Domain Generalization: An Information-Theoretic View**|Tangzheng Lian et.al|[paper](https://arxiv.org/abs/2507.05823)|-|-|\n", "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze": "|**2025-7-11**|**RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze**|Yunsoo Kim et.al|[paper](https://arxiv.org/abs/2507.09097)|-|-|\n", "Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement": "|**2025-7-11**|**Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement**|Jia-Xuan Jiang et.al|[paper](https://arxiv.org/abs/2507.08340)|[code](https://github.com/HopkinsKwong/MCCSDG)|<details><summary>detail</summary>Accepted by ACMMM 25</details>|\n", "From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry": "|**2025-7-10**|**From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry**|Chetan Arora et.al|[paper](https://arxiv.org/abs/2507.07689)|-|-|\n", "Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation": "|**2025-7-10**|**Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation**|Junyu Luo et.al|[paper](https://arxiv.org/abs/2507.07621)|-|<details><summary>detail</summary>ICML 2025</details>|\n", "SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains": "|**2025-7-9**|**SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains**|Krithika Ramesh et.al|[paper](https://arxiv.org/abs/2507.07229)|-|-|\n", "Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis": "|**2025-7-9**|**Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis**|Srihari K B et.al|[paper](https://arxiv.org/abs/2507.06571)|-|-|\n", "AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework": "|**2025-7-7**|**AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework**|Suoxiang Zhang et.al|[paper](https://arxiv.org/abs/2507.05621)|-|-|\n", "LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains": "|**2025-7-7**|**LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains**|Nicholas Chivaran et.al|[paper](https://arxiv.org/abs/2507.05162)|[code](https://github.com/nchivar/LAID.)|<details><summary>detail</summary>To appear in the proceedings of PST2025</details>|\n", "Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization": "|**2025-7-6**|**Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization**|Zuyu Zhang et.al|[paper](https://arxiv.org/abs/2507.04302)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning": "|**2025-7-5**|**CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning**|Jiacheng Shi et.al|[paper](https://arxiv.org/abs/2507.04048)|-|<details><summary>detail</summary>Interspeech2025</details>|\n", "Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations": "|**2025-7-4**|**Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations**|Hai Huang et.al|[paper](https://arxiv.org/abs/2507.03304)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "Set Valued Predictions For Robust Domain Generalization": "|**2025-7-3**|**Set Valued Predictions For Robust Domain Generalization**|Ron Tsibulsky et.al|[paper](https://arxiv.org/abs/2507.03146)|-|<details><summary>detail</summary>ICML 2025</details>|\n", "Entity-centric multi-domain transformer for improving generalization in fake news detection": "|**2025-7-15**|**Entity-centric multi-domain transformer for improving generalization in fake news detection**|P Bazmi et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0306457324001663)|-|<details><summary>detail</summary>Information Processing &\u00a0\u2026, 2024 Elsevier</details>|\n", "Fine-Grained Domain Generalization with Feature Structuralization": "|**2025-7-14**|**Fine-Grained Domain Generalization with Feature Structuralization**|W Yu et.al|[paper](https://arxiv.org/abs/2406.09166)|[code](https://github.com/yuwlong666/fsdg)|-|\n", "Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification": "|**2025-7-12**|**Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification**|S Addepalli et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Addepalli_Leveraging_Vision-Language_Models_for_Improving_Domain_Generalization_in_Image_Classification_CVPR_2024_paper.html)|[code](https://github.com/val-iisc/VL2V-ADiP)|<details><summary>detail</summary>Proceedings of the\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "Disentangled Prompt Representation for Domain Generalization": "|**2025-7-12**|**Disentangled Prompt Representation for Domain Generalization**|D Cheng et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_Disentangled_Prompt_Representation_for_Domain_Generalization_CVPR_2024_paper.html)|[code](https://github.com/henry123-boy/SpaTracker)|<details><summary>detail</summary>Proceedings of the\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "Supplementary Materials: Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential in Open Domain Generalization": "|**2025-7-12**|**Supplementary Materials: Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential in Open Domain Generalization**|M Singha et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Singha_Unknown_Prompt_the_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>openaccess.thecvf.com</details>|\n", "Supplementary Material for DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning": "|**2025-7-12**|**Supplementary Material for DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning**|B Dataset - openaccess.thecvf.com et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Bai_DiPrompT_Disentangled_Prompt_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>openaccess.thecvf.com</details>|\n", "Supplementary Material for Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization": "|**2025-7-12**|**Supplementary Material for Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization**|K Le et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Le_Efficiently_Assemble_Normalization_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>Phuoc, KS Wong openaccess.thecvf.com</details>|\n", "Domain Generalization for Crop Segmentation with Standardized Ensemble Knowledge Distillation": "|**2025-7-12**|**Domain Generalization for Crop Segmentation with Standardized Ensemble Knowledge Distillation**|S Angarano et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024W/Vision4Ag/html/Angarano_Domain_Generalization_for_Crop_Segmentation_with_Standardized_Ensemble_Knowledge_Distillation_CVPRW_2024_paper.html)|[code](https://github.com/pic4ser/agriseg)|<details><summary>detail</summary>Proceedings of the\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "MixStyle-Based Contrastive Test-Time Adaptation: Pathway to Domain Generalization": "|**2025-7-12**|**MixStyle-Based Contrastive Test-Time Adaptation: Pathway to Domain Generalization**|K Yamashita et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024W/MAT/html/Yamashita_MixStyle-Based_Contrastive_Test-Time_Adaptation_Pathway_to_Domain_Generalization_CVPRW_2024_paper.html)|-|<details><summary>detail</summary>\u2026\u00a0of the IEEE/CVF Conference on\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "Fault vibration model driven fault-aware domain generalization framework for bearing fault diagnosis": "|**2025-7-12**|**Fault vibration model driven fault-aware domain generalization framework for bearing fault diagnosis**|B Pang et.al|[paper](https://www.sciencedirect.com/science/article/pii/S1474034624002684)|-|<details><summary>detail</summary>Advanced Engineering\u00a0\u2026, 2024 Elsevier</details>|\n"}, "vision language": {"EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos": "|**2025-7-16**|**EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**|Ruihan Yang et.al|[paper](https://arxiv.org/abs/2507.12440)|[code](https://rchalyang.github.io/EgoVLA)|<details><summary>detail</summary>More videos can be found on our website: https://rchalyang</details>|\n", "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models": "|**2025-7-16**|**AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models**|Santosh Vasa et.al|[paper](https://arxiv.org/abs/2507.12414)|-|-|\n", "Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models": "|**2025-7-16**|**Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models**|Felix N\u00fctzel et.al|[paper](https://arxiv.org/abs/2507.12236)|[code](https://github.com/Felix-012/generate_to_ground.)|-|\n", "InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing": "|**2025-7-16**|**InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing**|Kun-Hsiang Lin et.al|[paper](https://arxiv.org/abs/2507.12060)|[code](https://kunkunlin1221.github.io/InstructFLIP.)|<details><summary>detail</summary>Accepted by MM'25</details>|\n", "GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models": "|**2025-7-16**|**GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models**|Zhaohong Huang et.al|[paper](https://arxiv.org/abs/2507.11969)|-|-|\n", "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering": "|**2025-7-16**|**POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering**|Yichen Xu et.al|[paper](https://arxiv.org/abs/2507.11939)|-|<details><summary>detail</summary>Work in Progress</details>|\n", "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision": "|**2025-7-15**|**Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision**|Xiaofeng Han et.al|[paper](https://arxiv.org/abs/2504.02477)|[code](https://github.com/Xiaofeng-Han-Res/MF-RV.)|-|\n", "Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models": "|**2025-7-15**|**Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models**|Lucy Xiaoyang Shi et.al|[paper](https://arxiv.org/abs/2502.19417)|[code](https://www.pi.website/research/hirobot)|<details><summary>detail</summary>ICML 2025</details>|\n", "How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study": "|**2025-7-15**|**How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study**|Che Liu et.al|[paper](https://arxiv.org/abs/2507.11200)|-|<details><summary>detail</summary>Accepted by the International Conference on AI in Healthcare 2025</details>|\n", "Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities": "|**2025-7-15**|**Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**|Yiting Qu et.al|[paper](https://arxiv.org/abs/2507.11155)|-|<details><summary>detail</summary>To Appear in the 34th USENIX Security Symposium</details>|\n", "Assessing Color Vision Test in Large Vision-language Models": "|**2025-7-15**|**Assessing Color Vision Test in Large Vision-language Models**|Hongfei Ye et.al|[paper](https://arxiv.org/abs/2507.11153)|[code](https://anonymous.4open.science/r/color-vision-test-dataset-3BCD)|-|\n", "MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models": "|**2025-7-15**|**MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models**|Seif Ahmed et.al|[paper](https://arxiv.org/abs/2507.11114)|-|-|\n", "Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander": "|**2025-7-15**|**Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander**|Li Wang et.al|[paper](https://arxiv.org/abs/2507.11079)|-|-|\n", "View Invariant Learning for Vision-Language Navigation in Continuous Environments": "|**2025-7-14**|**View Invariant Learning for Vision-Language Navigation in Continuous Environments**|Josh Qixuan Sun et.al|[paper](https://arxiv.org/abs/2507.08831)|-|<details><summary>detail</summary>Under review</details>|\n", "TAB: Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models": "|**2025-7-14**|**TAB: Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models**|Pooyan Rahmanzadehgervi et.al|[paper](https://arxiv.org/abs/2412.18675)|-|-|\n", "Towards Vision-Language Geo-Foundation Model: A Survey": "|**2025-7-15**|**Towards Vision-Language Geo-Foundation Model: A Survey**|Y Zhou et.al|[paper](https://www.researchgate.net/profile/Yue-Zhou-139/publication/381403816_Towards_Vision-Language_Geo-Foundation_Model_A_Survey/links/666ba71ea54c5f0b9464c544/Towards-Vision-Language-Geo-Foundation-Model-A-Survey.pdf)|[code](https://github.com/zytx121/awesome-vlgfm)|<details><summary>detail</summary>researchgate.net</details>|\n", "VLind-Bench: Measuring Language Priors in Large Vision-Language Models": "|**2025-7-14**|**VLind-Bench: Measuring Language Priors in Large Vision-Language Models**|K Lee et.al|[paper](https://arxiv.org/abs/2406.08702)|[code](https://github.com/klee972/vlind-bench)|-|\n", "How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models": "|**2025-7-14**|**How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models**|T Khajuria et.al|[paper](https://arxiv.org/abs/2406.09067)|-|-|\n", "AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models": "|**2025-7-14**|**AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models**|Y Wu et.al|[paper](https://arxiv.org/abs/2406.09295)|[code](https://paperswithcode.com/paper/alignmmbench-evaluating-chinese-multimodal)|-|\n", "MirrorCheck: Efficient Adversarial Defense for Vision-Language Models": "|**2025-7-14**|**MirrorCheck: Efficient Adversarial Defense for Vision-Language Models**|S Fares et.al|[paper](https://arxiv.org/abs/2406.09250)|[code](https://paperswithcode.com/paper/mirrorcheck-efficient-adversarial-defense-for)|-|\n", "LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living": "|**2025-7-14**|**LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living**|R Chakraborty et.al|[paper](https://arxiv.org/abs/2406.09390)|-|-|\n", "OpenVLA: An Open-Source Vision-Language-Action Model": "|**2025-7-14**|**OpenVLA: An Open-Source Vision-Language-Action Model**|MJ Kim et.al|[paper](https://arxiv.org/abs/2406.09246)|[code](https://github.com/openvla/openvla)|-|\n", "Generative AI-based Prompt Evolution Engineering Design Optimization With Vision-Language Model": "|**2025-7-14**|**Generative AI-based Prompt Evolution Engineering Design Optimization With Vision-Language Model**|M Wong et.al|[paper](https://arxiv.org/abs/2406.09143)|[code](https://paperswithcode.com/paper/generative-ai-based-prompt-evolution)|-|\n", "VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks": "|**2025-7-13**|**VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks**|J Wu et.al|[paper](https://arxiv.org/abs/2406.08394)|[code](https://github.com/opengvlab/visionllm)|-|\n", "RWKV-CLIP: A Robust Vision-Language Representation Learner": "|**2025-7-13**|**RWKV-CLIP: A Robust Vision-Language Representation Learner**|T Gu et.al|[paper](https://arxiv.org/abs/2406.06973)|[code](https://github.com/deepglint/rwkv-clip)|-|\n"}}