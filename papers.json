{"source-free": {"Black-box Source-free Domain Adaptation via Two-stage Knowledge Distillation": "|**2023-5-13**|**Black-box Source-free Domain Adaptation via Two-stage Knowledge Distillation**|Shuai Wanget.al|[paper](https://arxiv.org/abs/2305.07881)|-|-|\n", "FVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation": "|**2023-4-26**|**FVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation**|Yan Wanget.al|[paper](https://arxiv.org/abs/2304.13672)|-|-|\n", "Inference in Linear Observations with Multiple Signal Sources: Analysis of Approximate Message Passing and Applications to Unsourced Random Access in Cell-Free Systems": "|**2023-4-24**|**Inference in Linear Observations with Multiple Signal Sources: Analysis of Approximate Message Passing and Applications to Unsourced Random Access in Cell-Free Systems**|Burak \u00c7akmaket.al|[paper](https://arxiv.org/abs/2304.12290)|-|-|\n", "Few-shot Fine-tuning is All You Need for Source-free Domain Adaptation": "|**2023-4-24**|**Few-shot Fine-tuning is All You Need for Source-free Domain Adaptation**|Suho Leeet.al|[paper](https://arxiv.org/abs/2304.00792)|[code](https://github.com/daintlab/fewshot-SFDA)|<details><summary>detail</summary>The first two authors contributed equally</details>|\n", "Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation": "|**2023-4-19**|**Chaos to Order: A Label Propagation Perspective on Source-Free Domain Adaptation**|Chunwei Wuet.al|[paper](https://arxiv.org/abs/2301.08413)|-|<details><summary>detail</summary>This work has been submitted for peer review</details>|\n", "Continual Source-Free Unsupervised Domain Adaptation": "|**2023-4-14**|**Continual Source-Free Unsupervised Domain Adaptation**|Waqar Ahmedet.al|[paper](https://arxiv.org/abs/2304.07374)|-|<details><summary>detail</summary>International Conference on Image Analysis and Processing</details>|\n", "CoSDA: Continual Source-Free Domain Adaptation": "|**2023-4-13**|**CoSDA: Continual Source-Free Domain Adaptation**|Haozhe Fenget.al|[paper](https://arxiv.org/abs/2304.06627)|-|-|\n", "Source-free Domain Adaptation Requires Penalized Diversity": "|**2023-4-12**|**Source-free Domain Adaptation Requires Penalized Diversity**|Laya Rafiee Sevyeriet.al|[paper](https://arxiv.org/abs/2304.02798)|-|-|\n", "SOSR: Source-Free Image Super-Resolution with Wavelet Augmentation Transformer": "|**2023-3-30**|**SOSR: Source-Free Image Super-Resolution with Wavelet Augmentation Transformer**|Yuang Aiet.al|[paper](https://arxiv.org/abs/2303.17783)|-|-|\n", "C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient Source Free Domain Adaptation": "|**2023-3-29**|**C-SFDA: A Curriculum Learning Aided Self-Training Framework for Efficient Source Free Domain Adaptation**|Nazmul Karimet.al|[paper](https://arxiv.org/abs/2303.17132)|-|<details><summary>detail</summary>CVPR 2023</details>|\n", "SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis": "|**2023-3-28**|**SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis**|Nicola K Dinsdaleet.al|[paper](https://arxiv.org/abs/2303.15965)|[code](https://github.com/nkdinsdale/SFHarmony)|-|\n", "Spatio-Temporal Pixel-Level Contrastive Learning-based Source-Free Domain Adaptation for Video Semantic Segmentation": "|**2023-3-25**|**Spatio-Temporal Pixel-Level Contrastive Learning-based Source-Free Domain Adaptation for Video Semantic Segmentation**|Shao-Yuan Loet.al|[paper](https://arxiv.org/abs/2303.14361)|[code](https://github.com/shaoyuanlo/STPL)|<details><summary>detail</summary>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</details>|\n", "Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection": "|**2023-3-21**|**Instance Relation Graph Guided Source-Free Domain Adaptive Object Detection**|Vibashan VSet.al|[paper](https://arxiv.org/abs/2203.15793)|[code](https://viudomain.github.io/irg-sfda-web/)|<details><summary>detail</summary>CVPR 2023</details>|\n", "Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation": "|**2023-3-17**|**Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation**|Yuqi Chenet.al|[paper](https://arxiv.org/abs/2301.13428)|-|<details><summary>detail</summary>Journal articles</details>|\n", "Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation": "|**2023-3-17**|**Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation**|Mattia Litricoet.al|[paper](https://arxiv.org/abs/2303.03770)|-|<details><summary>detail</summary>To be published in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2023</details>|\n"}, "object detection": {"PillarAcc: Sparse PointPillars Accelerator for Real-Time Point Cloud 3D Object Detection on Edge Devices": "|**2023-5-15**|**PillarAcc: Sparse PointPillars Accelerator for Real-Time Point Cloud 3D Object Detection on Edge Devices**|Minjae Leeet.al|[paper](https://arxiv.org/abs/2305.07522)|-|-|\n", "RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection": "|**2023-5-15**|**RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for Oriented Object Detection**|Hakjin Leeet.al|[paper](https://arxiv.org/abs/2305.07598)|-|<details><summary>detail</summary>State-of-the-art Rotated Object Detector in DOTA v1</details>|\n", "MUS-CDB: Mixed Uncertainty Sampling with Class Distribution Balancing for Active Annotation in Aerial Object Detection": "|**2023-5-14**|**MUS-CDB: Mixed Uncertainty Sampling with Class Distribution Balancing for Active Annotation in Aerial Object Detection**|Dong Lianget.al|[paper](https://arxiv.org/abs/2212.02804)|[code](https://github.com/ZJW700/MUS-CDB)|-|\n", "Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection": "|**2023-5-14**|**Instance-Aware Repeat Factor Sampling for Long-Tailed Object Detection**|Burhaneddin Yamanet.al|[paper](https://arxiv.org/abs/2305.08069)|-|-|\n", "A Secure and Efficient Multi-Object Grasping Detection Approach for Robotic Arms": "|**2023-5-13**|**A Secure and Efficient Multi-Object Grasping Detection Approach for Robotic Arms**|Hui Wanget.al|[paper](https://arxiv.org/abs/2209.03511)|-|-|\n", "Multi-Modal 3D Object Detection by Box Matching": "|**2023-5-12**|**Multi-Modal 3D Object Detection by Box Matching**|Zhe Liuet.al|[paper](https://arxiv.org/abs/2305.07713)|[code](https://github.com/happinesslz/FBMNet.)|-|\n", "SSD-MonoDTR: Supervised Scale-constrained Deformable Transformer for Monocular 3D Object Detection": "|**2023-5-12**|**SSD-MonoDTR: Supervised Scale-constrained Deformable Transformer for Monocular 3D Object Detection**|Xuan Heet.al|[paper](https://arxiv.org/abs/2305.07270)|[code](https://github.com/mikasa3lili/SSD-MonoDETR.)|<details><summary>detail</summary>Code will be publicly available at https://github</details>|\n", "An Effective Crop-Paste Pipeline for Few-shot Object Detection": "|**2023-5-12**|**An Effective Crop-Paste Pipeline for Few-shot Object Detection**|Shaobo Linet.al|[paper](https://arxiv.org/abs/2302.14452)|-|<details><summary>detail</summary>Journal ref:CVPR2023 Workshop on Learning with Limited Labelled Data</details>|\n", "Explore the Power of Synthetic Data on Few-shot Object Detection": "|**2023-5-12**|**Explore the Power of Synthetic Data on Few-shot Object Detection**|Shaobo Linet.al|[paper](https://arxiv.org/abs/2303.13221)|-|<details><summary>detail</summary>Journal ref:CVPR2023 Workshop on Generative Models for Computer Vision</details>|\n", "Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers": "|**2023-5-11**|**Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers**|Dahun Kimet.al|[paper](https://arxiv.org/abs/2305.07011)|-|<details><summary>detail</summary>CVPR 2023</details>|\n", "SalienDet: A Saliency-based Feature Enhancement Algorithm for Object Detection for Autonomous Driving": "|**2023-5-11**|**SalienDet: A Saliency-based Feature Enhancement Algorithm for Object Detection for Autonomous Driving**|Ning Dinget.al|[paper](https://arxiv.org/abs/2305.06940)|-|<details><summary>detail</summary>Paper submitted to IEEE Transactions on Intelligent Vehicles</details>|\n", "Combating noisy labels in object detection datasets": "|**2023-5-11**|**Combating noisy labels in object detection datasets**|Krystian Chachu\u0142aet.al|[paper](https://arxiv.org/abs/2211.13993)|-|-|\n", "Extracting Complex Named Entities in Legal Documents via Weakly Supervised Object Detection": "|**2023-5-9**|**Extracting Complex Named Entities in Legal Documents via Weakly Supervised Object Detection**|Hsiu-Wei Yanget.al|[paper](https://arxiv.org/abs/2305.05836)|-|-|\n", "Guided Focal Stack Refinement Network for Light Field Salient Object Detection": "|**2023-5-9**|**Guided Focal Stack Refinement Network for Light Field Salient Object Detection**|Bo Yuanet.al|[paper](https://arxiv.org/abs/2305.05260)|-|<details><summary>detail</summary>Accepted by ICME 2023</details>|\n", "Texture-guided Saliency Distilling for Unsupervised Salient Object Detection": "|**2023-5-9**|**Texture-guided Saliency Distilling for Unsupervised Salient Object Detection**|Huajun Zhouet.al|[paper](https://arxiv.org/abs/2207.05921)|-|-|\n"}, "domain adaptation": {"A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation": "|**2023-5-15**|**A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation**|Hui Tanget.al|[paper](https://arxiv.org/abs/2303.09165)|[code](https://github.com/huitangtang/On_the_Utility_of_Synthetic_Data.)|-|\n", "FeatFSDA: Towards Few-shot Domain Adaptation for Video-based Activity Recognition": "|**2023-5-15**|**FeatFSDA: Towards Few-shot Domain Adaptation for Video-based Activity Recognition**|Kunyu Penget.al|[paper](https://arxiv.org/abs/2305.08420)|[code](https://github.com/KPeng9510/FeatFSDA.)|<details><summary>detail</summary>Benchmarks and code will be released at https://github</details>|\n", "ITportrait: Image-Text Coupled 3D Portrait Domain Adaptation": "|**2023-5-15**|**ITportrait: Image-Text Coupled 3D Portrait Domain Adaptation**|Xiangwen Denget.al|[paper](https://arxiv.org/abs/2304.04364)|-|-|\n", "PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations": "|**2023-5-13**|**PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations**|Lodagala V S V Durga Prasadet.al|[paper](https://arxiv.org/abs/2203.16965)|-|<details><summary>detail</summary>IEEE SLT 2022</details>|\n", "DNN-Compressed Domain Visual Recognition with Feature Adaptation": "|**2023-5-13**|**DNN-Compressed Domain Visual Recognition with Feature Adaptation**|Yingpeng Denget.al|[paper](https://arxiv.org/abs/2305.08000)|-|-|\n", "Black-box Source-free Domain Adaptation via Two-stage Knowledge Distillation": "|**2023-5-13**|**Black-box Source-free Domain Adaptation via Two-stage Knowledge Distillation**|Shuai Wanget.al|[paper](https://arxiv.org/abs/2305.07881)|-|-|\n", "Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation": "|**2023-5-12**|**Beyond invariant representation learning: linearly alignable latent spaces for efficient closed-form domain adaptation**|Oliver Struckmeieret.al|[paper](https://arxiv.org/abs/2305.07500)|-|-|\n", "Color Deconvolution applied to Domain Adaptation in HER2 histopathological images": "|**2023-5-12**|**Color Deconvolution applied to Domain Adaptation in HER2 histopathological images**|David Anglada-Rotgeret.al|[paper](https://arxiv.org/abs/2305.07404)|-|-|\n", "Cross-Domain Few-Shot Relation Extraction via Representation Learning and Domain Adaptation": "|**2023-5-10**|**Cross-Domain Few-Shot Relation Extraction via Representation Learning and Domain Adaptation**|Zhongju Yuanet.al|[paper](https://arxiv.org/abs/2212.02560)|-|-|\n", "Inclusive FinTech Lending via Contrastive Learning and Domain Adaptation": "|**2023-5-9**|**Inclusive FinTech Lending via Contrastive Learning and Domain Adaptation**|Xiyang Huet.al|[paper](https://arxiv.org/abs/2305.05827)|-|-|\n", "Unsupervised Domain Adaptation for Semantic Segmentation via Feature-space Density Matching": "|**2023-5-9**|**Unsupervised Domain Adaptation for Semantic Segmentation via Feature-space Density Matching**|Tushar Katariaet.al|[paper](https://arxiv.org/abs/2305.05789)|-|-|\n", "Interpretations of Domain Adaptations via Layer Variational Analysis": "|**2023-5-9**|**Interpretations of Domain Adaptations via Layer Variational Analysis**|Huan-Hsin Tsenget.al|[paper](https://arxiv.org/abs/2302.01798)|-|<details><summary>detail</summary>Published at ICLR 2023</details>|\n", "Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels": "|**2023-5-9**|**Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels**|Enric Moreuet.al|[paper](https://arxiv.org/abs/2305.05580)|-|-|\n", "Tailoring Domain Adaptation for Machine Translation Quality Estimation": "|**2023-5-9**|**Tailoring Domain Adaptation for Machine Translation Quality Estimation**|Javad Pourmostafa Roshan Sharamiet.al|[paper](https://arxiv.org/abs/2304.08891)|-|<details><summary>detail</summary>EAMT 2023 (main)</details>|\n", "MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection": "|**2023-5-8**|**MS3D: Leveraging Multiple Detectors for Unsupervised Domain Adaptation in 3D Object Detection**|Darren Tsaiet.al|[paper](https://arxiv.org/abs/2304.02431)|[code](https://github.com/darrenjkt/MS3D)|<details><summary>detail</summary>Our code is available at https://github</details>|\n"}, "domain generalization": {"ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human": "|**2023-5-15**|**ChatPLUG: Open-Domain Generative Dialogue System with Internet-Augmented Instruction Tuning for Digital Human**|Junfeng Tianet.al|[paper](https://arxiv.org/abs/2304.07849)|[code](https://modelscope.cn/models/damo/ChatPLUG-3.7B)|-|\n", "Learning to Generalize for Cross-domain QA": "|**2023-5-14**|**Learning to Generalize for Cross-domain QA**|Yingjie Niuet.al|[paper](https://arxiv.org/abs/2305.08208)|-|-|\n", "Contrastive Domain Generalization via Logit Attribution Matching": "|**2023-5-13**|**Contrastive Domain Generalization via Logit Attribution Matching**|Han Gaoet.al|[paper](https://arxiv.org/abs/2305.07888)|-|-|\n", "Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation": "|**2023-5-12**|**Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation**|Zhen Guoet.al|[paper](https://arxiv.org/abs/2305.07804)|-|-|\n", "Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation": "|**2023-5-12**|**Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation**|Lei Liuet.al|[paper](https://arxiv.org/abs/2305.07393)|[code](https://github.com/JeremyLeiLiu/XLinguDial.)|<details><summary>detail</summary>Accepted for presentation at SIGIR 2023</details>|\n", "The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain": "|**2023-5-11**|**The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain**|Arseny Moskvichevet.al|[paper](https://arxiv.org/abs/2305.07141)|-|-|\n", "Back-to-Bones: Rediscovering the Role of Backbones in Domain Generalization": "|**2023-5-9**|**Back-to-Bones: Rediscovering the Role of Backbones in Domain Generalization**|Simone Angaranoet.al|[paper](https://arxiv.org/abs/2209.01121)|[code](https://github.com/PIC4SeR/Back-to-Bones.)|-|\n", "Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks": "|**2023-5-9**|**Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks**|Aristotelis Ballaset.al|[paper](https://arxiv.org/abs/2303.11338)|-|<details><summary>detail</summary>Preprint of under review manuscript at IEEE Transactions on Emerging Topics in Computational Intelligence</details>|\n", "Adaptive Domain Generalization for Digital Pathology Images": "|**2023-5-8**|**Adaptive Domain Generalization for Digital Pathology Images**|Andrew Walkeret.al|[paper](https://arxiv.org/abs/2305.05100)|-|-|\n", "FS-BAN: Born-Again Networks for Domain Generalization Few-Shot Classification": "|**2023-5-8**|**FS-BAN: Born-Again Networks for Domain Generalization Few-Shot Classification**|Yunqing Zhaoet.al|[paper](https://arxiv.org/abs/2208.10930)|[code](https://yunqing-me.github.io/Born-Again-FS/.)|-|\n", "Generalized Universal Domain Adaptation with Generative Flow Networks": "|**2023-5-8**|**Generalized Universal Domain Adaptation with Generative Flow Networks**|Didi Zhuet.al|[paper](https://arxiv.org/abs/2305.04466)|-|-|\n", "General-to-Specific Transfer Labeling for Domain Adaptable Keyphrase Generation": "|**2023-5-7**|**General-to-Specific Transfer Labeling for Domain Adaptable Keyphrase Generation**|Rui Menget.al|[paper](https://arxiv.org/abs/2208.09606)|[code](https://github.com/memray/OpenNMT-kpg-release.)|<details><summary>detail</summary>The submission has been accepted to the Findings of ACL 2023</details>|\n", "Domain Generalization for Mammographic Image Analysis via Contrastive Learning": "|**2023-5-6**|**Domain Generalization for Mammographic Image Analysis via Contrastive Learning**|Zheren Liet.al|[paper](https://arxiv.org/abs/2304.10226)|-|<details><summary>detail</summary>arXiv admin note: text overlap with arXiv:2111</details>|\n", "Mask The Bias: Improving Domain-Adaptive Generalization of CTC-based ASR with Internal Language Model Estimation": "|**2023-5-5**|**Mask The Bias: Improving Domain-Adaptive Generalization of CTC-based ASR with Internal Language Model Estimation**|Nilaksh Daset.al|[paper](https://arxiv.org/abs/2305.03837)|-|<details><summary>detail</summary>ICASSP 2023</details>|\n", "Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain": "|**2023-5-4**|**Stylized Data-to-Text Generation: A Case Study in the E-Commerce Domain**|Liqiang Jinget.al|[paper](https://arxiv.org/abs/2305.03256)|-|-|\n"}, "vision language": {"Improved baselines for vision-language pre-training": "|**2023-5-15**|**Improved baselines for vision-language pre-training**|Enrico Finiet.al|[paper](https://arxiv.org/abs/2305.08675)|-|-|\n", "Debiasing Vision-Language Models via Biased Prompts": "|**2023-5-15**|**Debiasing Vision-Language Models via Biased Prompts**|Ching-Yao Chuanget.al|[paper](https://arxiv.org/abs/2302.00070)|-|-|\n", "Mode Approximation Makes Good Vision-Language Prompts": "|**2023-5-15**|**Mode Approximation Makes Good Vision-Language Prompts**|Haixin Wanget.al|[paper](https://arxiv.org/abs/2305.08381)|[code](https://github.com/WillDreamer/Aurora.)|-|\n", "Continual Vision-Language Representation Learning with Off-Diagonal Information": "|**2023-5-14**|**Continual Vision-Language Representation Learning with Off-Diagonal Information**|Zixuan Niet.al|[paper](https://arxiv.org/abs/2305.07437)|-|<details><summary>detail</summary>Journal ref:ICML 2023</details>|\n", "Multi-task Paired Masking with Alignment Modeling for Medical Vision-Language Pre-training": "|**2023-5-13**|**Multi-task Paired Masking with Alignment Modeling for Medical Vision-Language Pre-training**|Ke Zhanget.al|[paper](https://arxiv.org/abs/2305.07920)|-|-|\n", "Measuring Progress in Fine-grained Vision-and-Language Understanding": "|**2023-5-12**|**Measuring Progress in Fine-grained Vision-and-Language Understanding**|Emanuele Bugliarelloet.al|[paper](https://arxiv.org/abs/2305.07558)|-|<details><summary>detail</summary>ACL 2023</details>|\n", "WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models": "|**2023-5-12**|**WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models**|Aboli Maratheet.al|[paper](https://arxiv.org/abs/2305.07528)|[code](https://infernolia.github.io/WEDGE.)|<details><summary>detail</summary>Accepted in Vision Datasets Understanding at CVPR 2023</details>|\n", "ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4": "|**2023-5-12**|**ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4**|Zhengqing Yuanet.al|[paper](https://arxiv.org/abs/2305.07490)|[code](https://huggingface.co/Tyrannosaurus/ArtGPT-4)|-|\n", "Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts": "|**2023-5-11**|**Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts**|Zhaoyang Zhanget.al|[paper](https://arxiv.org/abs/2305.07019)|-|-|\n", "VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification": "|**2023-5-11**|**VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification**|Souhail Bakkaliet.al|[paper](https://arxiv.org/abs/2205.12029)|-|<details><summary>detail</summary>PR</details>|\n", "InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language": "|**2023-5-11**|**InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language**|Zhaoyang Liuet.al|[paper](https://arxiv.org/abs/2305.05662)|[code](https://github.com/OpenGVLab/InternGPT.)|<details><summary>detail</summary>Technical Report</details>|\n", "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning": "|**2023-5-10**|**InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning**|Wenliang Daiet.al|[paper](https://arxiv.org/abs/2305.06500)|[code](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip.)|<details><summary>detail</summary>preprint</details>|\n", "Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs": "|**2023-5-10**|**Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs**|Roei Herziget.al|[paper](https://arxiv.org/abs/2305.06343)|-|<details><summary>detail</summary>Tech Report</details>|\n", "Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization": "|**2023-5-10**|**Modeling Paragraph-Level Vision-Language Semantic Alignment for Multi-Modal Summarization**|Chenhao Cuiet.al|[paper](https://arxiv.org/abs/2208.11303)|-|-|\n", "Vision-Language Models in Remote Sensing: Current Progress and Future Trends": "|**2023-5-9**|**Vision-Language Models in Remote Sensing: Current Progress and Future Trends**|Congcong Wenet.al|[paper](https://arxiv.org/abs/2305.05726)|-|-|\n"}}