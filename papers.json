{"source-free": {"Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-6-26**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "SFDLA: Source-Free Document Layout Analysis": "|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|\n", "Unified Source-Free Domain Adaptation": "|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|\n", "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|\n", "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization": "|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|\n", "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data": "|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|\n", "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation": "|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|\n", "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation": "|**2025-5-30**|**Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation**|Prasanna Reddy Pulakurthi et.al|[paper](https://arxiv.org/abs/2505.24216)|[code](https://github.com/PrasannaPulakurthi/SPM)|-|\n", "Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation": "|**2025-5-27**|**Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation**|Peihua Deng et.al|[paper](https://arxiv.org/abs/2411.16064)|[code](https://github.com/dengpeihua/GROTO.)|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Training-Free Multi-Step Audio Source Separation": "|**2025-5-26**|**Training-Free Multi-Step Audio Source Separation**|Yongyi Zang et.al|[paper](https://arxiv.org/abs/2505.19534)|-|-|\n", "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation": "|**2025-5-23**|**Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation**|Peiliang Gong et.al|[paper](https://arxiv.org/abs/2505.21525)|-|-|\n", "Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing": "|**2025-5-20**|**Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing**|Yang Xiao et.al|[paper](https://arxiv.org/abs/2505.14601)|-|<details><summary>detail</summary>Accepted by Interspeech 2025</details>|\n", "DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation": "|**2025-5-14**|**DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation**|Siqi Yin et.al|[paper](https://arxiv.org/abs/2505.09927)|-|-|\n"}, "object detection": {"Enhancing Object Detection Robustness: Detecting and Restoring Confidence in the Presence of Adversarial Patch Attacks": "|**2025-6-27**|**Enhancing Object Detection Robustness: Detecting and Restoring Confidence in the Presence of Adversarial Patch Attacks**|Roie Kazoom et.al|[paper](https://arxiv.org/abs/2403.12988)|-|-|\n", "Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection": "|**2025-6-27**|**Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection**|Taijin Zhao et.al|[paper](https://arxiv.org/abs/2506.22161)|-|-|\n", "Embodied Domain Adaptation for Object Detection": "|**2025-6-26**|**Embodied Domain Adaptation for Object Detection**|Xiangyu Shi et.al|[paper](https://arxiv.org/abs/2506.21860)|-|<details><summary>detail</summary>Accepted by IROS 2025</details>|\n", "Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection": "|**2025-6-26**|**Towards Reliable Detection of Empty Space: Conditional Marked Point Processes for Object Detection**|Tobias J. Riedlinger et.al|[paper](https://arxiv.org/abs/2506.21486)|-|-|\n", "Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes": "|**2025-6-26**|**Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes**|Zhangjun Zhou et.al|[paper](https://arxiv.org/abs/2412.10943)|[code](https://github.com/ssecv/USCNet.)|-|\n", "DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic": "|**2025-6-26**|**DuET: Dual Incremental Object Detection via Exemplar-Free Task Arithmetic**|Munish Monga et.al|[paper](https://arxiv.org/abs/2506.21260)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "ROA-BEV: 2D Region-Oriented Attention for BEV-based 3D Object Detection": "|**2025-6-26**|**ROA-BEV: 2D Region-Oriented Attention for BEV-based 3D Object Detection**|Jiwei Chen et.al|[paper](https://arxiv.org/abs/2410.10298)|[code](https://github.com/DFLyan/ROA-BEV.)|<details><summary>detail</summary>accepted by IROS 2025</details>|\n", "LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection": "|**2025-6-26**|**LASFNet: A Lightweight Attention-Guided Self-Modulation Feature Fusion Network for Multimodal Object Detection**|Lei Hao et.al|[paper](https://arxiv.org/abs/2506.21018)|[code](https://github.com/leileilei2000/LASFNet.)|-|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos": "|**2025-6-25**|**Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos**|Yitong Quan et.al|[paper](https://arxiv.org/abs/2506.20550)|-|<details><summary>detail</summary>Submitted to ECMR 2025</details>|\n", "Align and Distill: Unifying and Improving Domain Adaptive Object Detection": "|**2025-6-23**|**Align and Distill: Unifying and Improving Domain Adaptive Object Detection**|Justin Kay et.al|[paper](https://arxiv.org/abs/2403.12029)|[code](https://github.com/justinkay/aldi)|<details><summary>detail</summary>TMLR camera ready (Featured Certification)</details>|\n", "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation": "|**2025-6-23**|**LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation**|Yang Zhou et.al|[paper](https://arxiv.org/abs/2503.13794)|-|-|\n", "On the Robustness of Human-Object Interaction Detection against Distribution Shift": "|**2025-6-22**|**On the Robustness of Human-Object Interaction Detection against Distribution Shift**|Chi Xie et.al|[paper](https://arxiv.org/abs/2506.18021)|-|<details><summary>detail</summary>This work has been submitted to the IEEE for possible publication</details>|\n", "Towards Reflected Object Detection: A Benchmark": "|**2025-6-22**|**Towards Reflected Object Detection: A Benchmark**|Yiquan Wu et.al|[paper](https://arxiv.org/abs/2407.05575)|[code](https://github.com/jirouvan/ROD.)|-|\n", "DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training": "|**2025-6-21**|**DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training**|Chen Xin et.al|[paper](https://arxiv.org/abs/2407.09174)|[code](https://github.com/chen-xin-94/DART.)|<details><summary>detail</summary>Corrected minor typos</details>|\n"}, "domain adaptation": {"Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis": "|**2025-6-27**|**Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis**|YongKyung Oh et.al|[paper](https://arxiv.org/abs/2506.22393)|-|-|\n", "Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling": "|**2025-6-27**|**Weakly-Supervised Domain Adaptation with Proportion-Constrained Pseudo-Labeling**|Takumi Okuo et.al|[paper](https://arxiv.org/abs/2506.22301)|-|<details><summary>detail</summary>IJCNN2025</details>|\n", "Gradual Domain Adaptation for Graph Learning": "|**2025-6-27**|**Gradual Domain Adaptation for Graph Learning**|Pui Ieng Lei et.al|[paper](https://arxiv.org/abs/2501.17443)|-|-|\n", "Embodied Domain Adaptation for Object Detection": "|**2025-6-26**|**Embodied Domain Adaptation for Object Detection**|Xiangyu Shi et.al|[paper](https://arxiv.org/abs/2506.21860)|-|<details><summary>detail</summary>Accepted by IROS 2025</details>|\n", "TITAN: Query-Token based Domain Adaptive Adversarial Learning": "|**2025-6-26**|**TITAN: Query-Token based Domain Adaptive Adversarial Learning**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2506.21484)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability": "|**2025-6-26**|**Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability**|Boyong He et.al|[paper](https://arxiv.org/abs/2506.21042)|[code](https://github.com/heboyong/Fitness-Generalization-Transferability)|<details><summary>detail</summary>Accepted by ICCV2025</details>|\n", "crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023": "|**2025-6-24**|**crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023**|Navodini Wijethilake et.al|[paper](https://arxiv.org/abs/2506.12006)|-|-|\n", "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning": "|**2025-6-24**|**Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning**|Harisankar Babu et.al|[paper](https://arxiv.org/abs/2506.19592)|-|-|\n", "M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition": "|**2025-6-24**|**M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition**|Ting Luo et.al|[paper](https://arxiv.org/abs/2404.15615)|-|-|\n", "Progressive Modality Cooperation for Multi-Modality Domain Adaptation": "|**2025-6-24**|**Progressive Modality Cooperation for Multi-Modality Domain Adaptation**|Weichen Zhang et.al|[paper](https://arxiv.org/abs/2506.19316)|-|-|\n", "Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation": "|**2025-6-23**|**Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation**|Weichen Zhang et.al|[paper](https://arxiv.org/abs/2506.19267)|-|-|\n", "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation": "|**2025-6-23**|**Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation**|Junjie Chen et.al|[paper](https://arxiv.org/abs/2504.01668)|-|<details><summary>detail</summary>This paper has been accepted to the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)</details>|\n", "Align and Distill: Unifying and Improving Domain Adaptive Object Detection": "|**2025-6-23**|**Align and Distill: Unifying and Improving Domain Adaptive Object Detection**|Justin Kay et.al|[paper](https://arxiv.org/abs/2403.12029)|[code](https://github.com/justinkay/aldi)|<details><summary>detail</summary>TMLR camera ready (Featured Certification)</details>|\n", "DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data": "|**2025-6-22**|**DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data**|Sabbir Ahmed et.al|[paper](https://arxiv.org/abs/2506.18173)|-|<details><summary>detail</summary>Submitted to ACPR Springer</details>|\n"}, "domain generalization": {"Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning": "|**2025-6-27**|**Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning**|Fangling Jiang et.al|[paper](https://arxiv.org/abs/2506.21895)|-|-|\n", "QT-DoG: Quantization-aware Training for Domain Generalization": "|**2025-6-26**|**QT-DoG: Quantization-aware Training for Domain Generalization**|Saqib Javed et.al|[paper](https://arxiv.org/abs/2410.06020)|[code](https://saqibjaved1.github.io/QT_DoG/.)|<details><summary>detail</summary>International Conference on Machine Learning (ICML) 2025</details>|\n", "MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models": "|**2025-6-26**|**MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models**|Yifan Liu et.al|[paper](https://arxiv.org/abs/2506.21784)|[code](https://github.com/ucla-mobility/MobiVerse.)|-|\n", "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability": "|**2025-6-26**|**Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability**|Boyong He et.al|[paper](https://arxiv.org/abs/2506.21042)|[code](https://github.com/heboyong/Fitness-Generalization-Transferability)|<details><summary>detail</summary>Accepted by ICCV2025</details>|\n", "FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization": "|**2025-6-25**|**FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization**|Ha Min Son et.al|[paper](https://arxiv.org/abs/2506.20841)|-|-|\n", "General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound": "|**2025-6-24**|**General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound**|Jakob Ambsdorf et.al|[paper](https://arxiv.org/abs/2506.19552)|-|<details><summary>detail</summary>Submitted version of paper accepted at MICCAI 2025</details>|\n", "Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey": "|**2025-6-23**|**Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey**|Xinyao Li et.al|[paper](https://arxiv.org/abs/2506.18504)|-|-|\n", "RLPR: Extrapolating RLVR to General Domains without Verifiers": "|**2025-6-22**|**RLPR: Extrapolating RLVR to General Domains without Verifiers**|Tianyu Yu et.al|[paper](https://arxiv.org/abs/2506.18254)|[code](https://github.com/openbmb/RLPR)|<details><summary>detail</summary>Project Website: https://github</details>|\n", "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation": "|**2025-6-22**|**RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation**|Tianxing Chen et.al|[paper](https://arxiv.org/abs/2506.18088)|[code](https://robotwin-platform.github.io/)|<details><summary>detail</summary>Project Page: https://robotwin-platform</details>|\n", "Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains": "|**2025-6-21**|**Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains**|Zhuo He et.al|[paper](https://arxiv.org/abs/2506.17718)|-|<details><summary>detail</summary>ICML 2025</details>|\n", "Domain Generalization using Action Sequences for Egocentric Action Recognition": "|**2025-6-21**|**Domain Generalization using Action Sequences for Egocentric Action Recognition**|Amirshayan Nasirimajd et.al|[paper](https://arxiv.org/abs/2506.17685)|[code](https://github.com/Ashayan97/SeqDG)|<details><summary>detail</summary>Pattern Recognition Letters</details>|\n", "When and How Does CLIP Enable Domain and Compositional Generalization?": "|**2025-6-20**|**When and How Does CLIP Enable Domain and Compositional Generalization?**|Elias Kempf et.al|[paper](https://arxiv.org/abs/2502.09507)|-|<details><summary>detail</summary>ICML 2025 (Spotlight)</details>|\n", "How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension": "|**2025-6-19**|**How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension**|Cynthia Dwork et.al|[paper](https://arxiv.org/abs/2506.16704)|-|-|\n", "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation": "|**2025-6-19**|**GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation**|Yilin Xiao et.al|[paper](https://arxiv.org/abs/2506.02404)|-|-|\n", "Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization": "|**2025-6-19**|**Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization**|Jiyao Wang et.al|[paper](https://arxiv.org/abs/2506.16160)|-|-|\n"}, "vision language": {"Test-Time Consistency in Vision Language Models": "|**2025-6-27**|**Test-Time Consistency in Vision Language Models**|Shih-Han Chou et.al|[paper](https://arxiv.org/abs/2506.22395)|-|-|\n", "MM-R$^3$: On (In-)Consistency of Vision-Language Models (VLMs)": "|**2025-6-27**|**MM-R$^3$: On (In-)Consistency of Vision-Language Models (VLMs)**|Shih-Han Chou et.al|[paper](https://arxiv.org/abs/2410.04778)|-|-|\n", "Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation": "|**2025-6-27**|**Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation**|Tiankai Chen et.al|[paper](https://arxiv.org/abs/2506.22375)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration": "|**2025-6-27**|**4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration**|Jiahui Zhang et.al|[paper](https://arxiv.org/abs/2506.22242)|-|-|\n", "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models": "|**2025-6-27**|**RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models**|Ronald Fecso et.al|[paper](https://arxiv.org/abs/2506.22149)|[code](https://github.com/ronnief1/RetFiner.)|<details><summary>detail</summary>Accepted for presentation at MICCAI 2025</details>|\n", "Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video Understanding": "|**2025-6-27**|**Self-ReS: Self-Reflection in Large Vision-Language Models for Long Video Understanding**|Joao Pereira et.al|[paper](https://arxiv.org/abs/2503.20362)|-|-|\n", "Benchmarking Vision Language Models on German Factual Data": "|**2025-6-27**|**Benchmarking Vision Language Models on German Factual Data**|Ren\u00e9 Peinl et.al|[paper](https://arxiv.org/abs/2504.11108)|-|<details><summary>detail</summary>Peinl</details>|\n", "Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference": "|**2025-6-27**|**Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference**|Zhuo Chen et.al|[paper](https://arxiv.org/abs/2502.18023)|[code](https://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary)|<details><summary>detail</summary>ACL25 May ARR</details>|\n", "BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language Models": "|**2025-6-27**|**BeamLLM: Vision-Empowered mmWave Beam Prediction with Large Language Models**|Can Zheng et.al|[paper](https://arxiv.org/abs/2503.10432)|-|-|\n", "ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models": "|**2025-6-27**|**ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models**|Hongbo Liu et.al|[paper](https://arxiv.org/abs/2506.21356)|-|-|\n", "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation": "|**2025-6-26**|**Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation**|Qiyue Gao et.al|[paper](https://arxiv.org/abs/2506.21876)|-|<details><summary>detail</summary>ACL 2025 (Findings)</details>|\n", "Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling": "|**2025-6-26**|**Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling**|Sungjune Park et.al|[paper](https://arxiv.org/abs/2506.21863)|-|-|\n", "Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation": "|**2025-6-26**|**Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation**|Shuo Wang et.al|[paper](https://arxiv.org/abs/2505.11886)|-|-|\n", "Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration": "|**2025-6-26**|**Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration**|Jiahe Chen et.al|[paper](https://arxiv.org/abs/2506.21509)|-|-|\n", "World-aware Planning Narratives Enhance Large Vision-Language Model Planner": "|**2025-6-26**|**World-aware Planning Narratives Enhance Large Vision-Language Model Planner**|Junhao Shi et.al|[paper](https://arxiv.org/abs/2506.21230)|-|-|\n"}}