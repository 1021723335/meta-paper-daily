{"source-free": {"Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results": "|**2025-11-1**|**Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results**|Rohan Palanikumar et.al|[paper](https://arxiv.org/abs/2511.00752)|-|-|\n", "Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising": "|**2025-10-31**|**Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising**|Kangjia Yan et.al|[paper](https://arxiv.org/abs/2510.05589)|-|-|\n", "Diffusion-Driven Progressive Target Manipulation for Source-Free Domain Adaptation": "|**2025-10-29**|**Diffusion-Driven Progressive Target Manipulation for Source-Free Domain Adaptation**|Yuyang Huang et.al|[paper](https://arxiv.org/abs/2510.25279)|-|<details><summary>detail</summary>Accepted by NeurIPS 2025</details>|\n", "Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-10-29**|**Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain Adaptation in Medical Image Segmentation**|Quang-Khai Bui-Tran et.al|[paper](https://arxiv.org/abs/2510.25227)|-|-|\n", "Training-free Source Attribution of AI-generated Images via Resynthesis": "|**2025-10-28**|**Training-free Source Attribution of AI-generated Images via Resynthesis**|Pietro Bongini et.al|[paper](https://arxiv.org/abs/2510.24278)|-|-|\n", "Attention Residual Fusion Network with Contrast for Source-free Domain Adaptation": "|**2025-10-24**|**Attention Residual Fusion Network with Contrast for Source-free Domain Adaptation**|Renrong Shao et.al|[paper](https://arxiv.org/abs/2510.22142)|-|-|\n", "Source-Free Domain Adaptation for SSVEP-based Brain-Computer Interfaces": "|**2025-10-22**|**Source-Free Domain Adaptation for SSVEP-based Brain-Computer Interfaces**|Osman Berke Guney et.al|[paper](https://arxiv.org/abs/2305.17403)|[code](https://github.com/osmanberke/SFDA-SSVEP-BCI)|-|\n", "Source-Free Object Detection with Detection Transformer": "|**2025-10-13**|**Source-Free Object Detection with Detection Transformer**|Huizai Yao et.al|[paper](https://arxiv.org/abs/2510.11090)|-|<details><summary>detail</summary>IEEE Transactions on Image Processing</details>|\n", "Robust Source-Free Domain Adaptation for Medical Image Segmentation based on Curriculum Learning": "|**2025-10-9**|**Robust Source-Free Domain Adaptation for Medical Image Segmentation based on Curriculum Learning**|Ziqi Zhang et.al|[paper](https://arxiv.org/abs/2510.08393)|-|-|\n", "ESS-Flow: Training-free guidance of flow-based models as inference in source space": "|**2025-10-7**|**ESS-Flow: Training-free guidance of flow-based models as inference in source space**|Adhithyan Kalaivanan et.al|[paper](https://arxiv.org/abs/2510.05849)|-|-|\n", "A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation": "|**2025-10-6**|**A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation**|Jiaping Yu et.al|[paper](https://arxiv.org/abs/2509.22229)|-|-|\n", "Leveraging Confident Image Regions for Source-Free Domain-Adaptive Object Detection": "|**2025-10-6**|**Leveraging Confident Image Regions for Source-Free Domain-Adaptive Object Detection**|Mohamed Lamine Mekhalfi et.al|[paper](https://arxiv.org/abs/2501.10081)|-|-|\n", "Temporal Source Recovery for Time-Series Source-Free Unsupervised Domain Adaptation": "|**2025-10-3**|**Temporal Source Recovery for Time-Series Source-Free Unsupervised Domain Adaptation**|Yucheng Wang et.al|[paper](https://arxiv.org/abs/2409.19635)|-|-|\n", "OT Score: An OT based Confidence Score for Source Free Unsupervised Domain Adaptation": "|**2025-10-2**|**OT Score: An OT based Confidence Score for Source Free Unsupervised Domain Adaptation**|Yiming Zhang et.al|[paper](https://arxiv.org/abs/2505.11669)|-|-|\n", "Source-Free Cross-Domain Continual Learning": "|**2025-10-2**|**Source-Free Cross-Domain Continual Learning**|Muhammad Tanzil Furqon et.al|[paper](https://arxiv.org/abs/2510.01649)|-|-|\n"}, "object detection": {"RIS-Assisted 3D Spherical Splatting for Object Composition Visualization using Detection Transformers": "|**2025-11-5**|**RIS-Assisted 3D Spherical Splatting for Object Composition Visualization using Detection Transformers**|Anastasios T. Sotiropoulos et.al|[paper](https://arxiv.org/abs/2511.02573)|-|<details><summary>detail</summary>Submitted to IEEE ICC 2026</details>|\n", "ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly": "|**2025-11-4**|**ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly**|Miftahur Rahman et.al|[paper](https://arxiv.org/abs/2511.03098)|-|-|\n", "Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization": "|**2025-11-4**|**Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization**|Tao Liu et.al|[paper](https://arxiv.org/abs/2511.02489)|[code](https://github.com/liutao23/ODGNNLoc.git.)|-|\n", "Deep Fourier-embedded Network for RGB and Thermal Salient Object Detection": "|**2025-11-4**|**Deep Fourier-embedded Network for RGB and Thermal Salient Object Detection**|Pengfei Lyu et.al|[paper](https://arxiv.org/abs/2411.18409)|[code](https://github.com/JoshuaLPF/FreqSal.)|<details><summary>detail</summary>Accepted by TCSVT2025</details>|\n", "3D Point Cloud Object Detection on Edge Devices for Split Computing": "|**2025-11-4**|**3D Point Cloud Object Detection on Edge Devices for Split Computing**|Taisuke Noguchi et.al|[paper](https://arxiv.org/abs/2511.02293)|-|-|\n", "Parameterized Prompt for Incremental Object Detection": "|**2025-11-4**|**Parameterized Prompt for Incremental Object Detection**|Zijia An et.al|[paper](https://arxiv.org/abs/2510.27316)|-|-|\n", "Detection and Geographic Localization of Natural Objects in the Wild: A Case Study on Palms": "|**2025-11-3**|**Detection and Geographic Localization of Natural Objects in the Wild: A Case Study on Palms**|Kangning Cui et.al|[paper](https://arxiv.org/abs/2502.13023)|-|-|\n", "WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions": "|**2025-11-3**|**WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions**|Quan Chen et.al|[paper](https://arxiv.org/abs/2508.12250)|[code](https://github.com/C-water/WXSOD)|<details><summary>detail</summary>Under review</details>|\n", "FlexEvent: Towards Flexible Event-Frame Object Detection at Varying Operational Frequencies": "|**2025-11-3**|**FlexEvent: Towards Flexible Event-Frame Object Detection at Varying Operational Frequencies**|Dongyue Lu et.al|[paper](https://arxiv.org/abs/2412.06708)|[code](https://flexevent.github.io/)|<details><summary>detail</summary>NeurIPS 2025</details>|\n", "Contrast-Guided Cross-Modal Distillation for Thermal Object Detection": "|**2025-11-3**|**Contrast-Guided Cross-Modal Distillation for Thermal Object Detection**|SiWoo Kim et.al|[paper](https://arxiv.org/abs/2511.01435)|-|-|\n", "DTAA: A Detect, Track and Avoid Architecture for navigation in spaces with Multiple Velocity Objects": "|**2025-11-3**|**DTAA: A Detect, Track and Avoid Architecture for navigation in spaces with Multiple Velocity Objects**|Samuel Nordstr\u00f6m et.al|[paper](https://arxiv.org/abs/2412.08121)|-|-|\n", "Eyes on Target: Gaze-Aware Object Detection in Egocentric Video": "|**2025-11-3**|**Eyes on Target: Gaze-Aware Object Detection in Egocentric Video**|Vishakha Lall et.al|[paper](https://arxiv.org/abs/2511.01237)|-|<details><summary>detail</summary>RAAI 2025</details>|\n", "Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds": "|**2025-11-2**|**Reflectance Prediction-based Knowledge Distillation for Robust 3D Object Detection in Compressed Point Clouds**|Hao Jing et.al|[paper](https://arxiv.org/abs/2505.17442)|[code](https://github.com/HaoJing-SX/RPKD.)|-|\n", "Space Object Detection using Multi-frame Temporal Trajectory Completion Method": "|**2025-10-31**|**Space Object Detection using Multi-frame Temporal Trajectory Completion Method**|Xiaoqing Lan et.al|[paper](https://arxiv.org/abs/2510.19220)|-|-|\n", "Conformal Object Detection by Sequential Risk Control": "|**2025-10-31**|**Conformal Object Detection by Sequential Risk Control**|L\u00e9o and\u00e9ol et.al|[paper](https://arxiv.org/abs/2505.24038)|-|-|\n"}, "domain adaptation": {"Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in Sub-Saharan MRI": "|**2025-11-4**|**Domain-Adaptive Transformer for Data-Efficient Glioma Segmentation in Sub-Saharan MRI**|Ilerioluwakiiye Abolade et.al|[paper](https://arxiv.org/abs/2511.02928)|-|-|\n", "Domain adaptation of large language models for geotechnical applications": "|**2025-11-3**|**Domain adaptation of large language models for geotechnical applications**|Lei Fan et.al|[paper](https://arxiv.org/abs/2507.05613)|-|-|\n", "AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs": "|**2025-11-3**|**AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs**|Mo El-Haj et.al|[paper](https://arxiv.org/abs/2511.01265)|[code](https://github.com/ArabicNLP-UK/AraFinNews.)|-|\n", "Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering": "|**2025-11-2**|**Saliency-Guided Domain Adaptation for Left-Hand Driving in Autonomous Steering**|Zahra Mehraban et.al|[paper](https://arxiv.org/abs/2511.01223)|-|-|\n", "Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification": "|**2025-11-2**|**Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification**|Ali Owfi et.al|[paper](https://arxiv.org/abs/2511.01172)|-|-|\n", "Data-Efficient Domain Adaptation for LLM-based MT using Contrastive Preference Optimization": "|**2025-10-31**|**Data-Efficient Domain Adaptation for LLM-based MT using Contrastive Preference Optimization**|Inacio Vieira et.al|[paper](https://arxiv.org/abs/2510.27556)|-|-|\n", "Multilingual BERT language model for medical tasks: Evaluation on domain-specific adaptation and cross-linguality": "|**2025-10-31**|**Multilingual BERT language model for medical tasks: Evaluation on domain-specific adaptation and cross-linguality**|Yinghao Luo et.al|[paper](https://arxiv.org/abs/2510.27552)|-|-|\n", "On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting": "|**2025-10-30**|**On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting**|Zhuonan Liang et.al|[paper](https://arxiv.org/abs/2506.17137)|-|-|\n", "PF-DAformer: Proximal Femur Segmentation via Domain Adaptive Transformer for Dual-Center QCT": "|**2025-10-30**|**PF-DAformer: Proximal Femur Segmentation via Domain Adaptive Transformer for Dual-Center QCT**|Rochak Dhakal et.al|[paper](https://arxiv.org/abs/2510.26903)|-|<details><summary>detail</summary>22 Pages</details>|\n", "Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems": "|**2025-10-30**|**Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems**|Georgios Kamaras et.al|[paper](https://arxiv.org/abs/2510.26656)|-|-|\n", "CATCH: A Modular Cross-domain Adaptive Template with Hook": "|**2025-10-30**|**CATCH: A Modular Cross-domain Adaptive Template with Hook**|Xinjin Li et.al|[paper](https://arxiv.org/abs/2510.26582)|-|-|\n", "Diffusion-Driven Progressive Target Manipulation for Source-Free Domain Adaptation": "|**2025-10-29**|**Diffusion-Driven Progressive Target Manipulation for Source-Free Domain Adaptation**|Yuyang Huang et.al|[paper](https://arxiv.org/abs/2510.25279)|-|<details><summary>detail</summary>Accepted by NeurIPS 2025</details>|\n", "Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA": "|**2025-10-29**|**Adapting Small Language Models to Low-Resource Domains: A Case Study in Hindi Tourism QA**|Sandipan Majhi et.al|[paper](https://arxiv.org/abs/2510.25273)|-|<details><summary>detail</summary>the Forum for Information Retrieval Evaluation 2025 (VATIKA Track)</details>|\n", "Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-10-29**|**Aligning What You Separate: Denoised Patch Mixing for Source-Free Domain Adaptation in Medical Image Segmentation**|Quang-Khai Bui-Tran et.al|[paper](https://arxiv.org/abs/2510.25227)|-|-|\n", "BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation": "|**2025-10-28**|**BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation**|Rapha\u00ebl Bagat et.al|[paper](https://arxiv.org/abs/2510.24570)|-|<details><summary>detail</summary>Submitted to ICASSP 2026</details>|\n"}, "domain generalization": {"Retrieval-Augmented Feature Generation for Domain-Specific Classification": "|**2025-11-4**|**Retrieval-Augmented Feature Generation for Domain-Specific Classification**|Xinhao Zhang et.al|[paper](https://arxiv.org/abs/2406.11177)|-|<details><summary>detail</summary>Accepted by ICDM 2025</details>|\n", "ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL": "|**2025-11-4**|**ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL**|Yiwen Jiao et.al|[paper](https://arxiv.org/abs/2511.00985)|-|-|\n", "Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image": "|**2025-11-3**|**Wonder3D++: Cross-domain Diffusion for High-fidelity 3D Generation from a Single Image**|Yuxiao Yang et.al|[paper](https://arxiv.org/abs/2511.01767)|[code](https://github.com/xxlong0/Wonder3D/tree/Wonder3D_Plus.)|-|\n", "OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting": "|**2025-11-2**|**OneCast: Structured Decomposition and Modular Generation for Cross-Domain Time Series Forecasting**|Tingyue Pan et.al|[paper](https://arxiv.org/abs/2510.24028)|-|-|\n", "SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains": "|**2025-11-2**|**SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains**|Krithika Ramesh et.al|[paper](https://arxiv.org/abs/2507.07229)|-|<details><summary>detail</summary>EMNLP 2025 System Demonstration</details>|\n", "Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective": "|**2025-11-1**|**Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective**|Wei Feng et.al|[paper](https://arxiv.org/abs/2511.00573)|-|-|\n", "Robust Atypical Mitosis Classification with DenseNet121: Stain-Aware Augmentation and Hybrid Loss for Domain Generalization": "|**2025-11-1**|**Robust Atypical Mitosis Classification with DenseNet121: Stain-Aware Augmentation and Hybrid Loss for Domain Generalization**|Adinath Dukre et.al|[paper](https://arxiv.org/abs/2510.22630)|-|<details><summary>detail</summary>MIDOG 2025 MICCAI Workshop accepted</details>|\n", "Effect of Domain Generalization Techniques in Low Resource Systems": "|**2025-10-31**|**Effect of Domain Generalization Techniques in Low Resource Systems**|Mahi Aminu et.al|[paper](https://arxiv.org/abs/2510.27512)|-|-|\n", "Continuous Domain Generalization": "|**2025-10-29**|**Continuous Domain Generalization**|Zekun Cai et.al|[paper](https://arxiv.org/abs/2505.13519)|-|-|\n", "Zero Reinforcement Learning Towards General Domains": "|**2025-10-29**|**Zero Reinforcement Learning Towards General Domains**|Yuyuan Zeng et.al|[paper](https://arxiv.org/abs/2510.25528)|-|-|\n", "Retaining Mixture Representations for Domain Generalized Anomalous Sound Detection": "|**2025-10-29**|**Retaining Mixture Representations for Domain Generalized Anomalous Sound Detection**|Phurich Saengthong et.al|[paper](https://arxiv.org/abs/2510.25182)|-|<details><summary>detail</summary>Submitted to ICASSP 2026</details>|\n", "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation": "|**2025-10-28**|**A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation**|Hao-Ran Yang et.al|[paper](https://arxiv.org/abs/2505.13043)|-|<details><summary>detail</summary>NeurIPS 2025</details>|\n", "Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain Video-to-Audio Generation": "|**2025-10-28**|**Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain Video-to-Audio Generation**|Kang Zhang et.al|[paper](https://arxiv.org/abs/2510.24103)|[code](https://github.com/pantheon5100/mgaudio)|<details><summary>detail</summary>accepted by NeurIPS 2025</details>|\n", "AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization": "|**2025-10-27**|**AdvBlur: Adversarial Blur for Robust Diabetic Retinopathy Classification and Cross-Domain Generalization**|Heethanjan Kanagalingam et.al|[paper](https://arxiv.org/abs/2510.24000)|-|-|\n", "Local Density-Based Anomaly Score Normalization for Domain Generalization": "|**2025-10-27**|**Local Density-Based Anomaly Score Normalization for Domain Generalization**|Kevin Wilkinghoff et.al|[paper](https://arxiv.org/abs/2509.10951)|-|-|\n"}, "vision language": {"Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding": "|**2025-11-5**|**Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding**|Songtao Jiang et.al|[paper](https://arxiv.org/abs/2510.08668)|[code](https://github.com/ZJUI-AI4H/Hulu-Med.)|-|\n", "Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models": "|**2025-11-5**|**Manipulation Facing Threats: Evaluating Physical Vulnerabilities in End-to-End Vision Language Action Models**|Hao Cheng et.al|[paper](https://arxiv.org/abs/2409.13174)|-|-|\n", "Revisiting Multimodal Positional Encoding in Vision-Language Models": "|**2025-11-5**|**Revisiting Multimodal Positional Encoding in Vision-Language Models**|Jie Huang et.al|[paper](https://arxiv.org/abs/2510.23095)|[code](https://github.com/JJJYmmm/Multimodal-RoPEs.)|-|\n", "Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models": "|**2025-11-5**|**Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models**|Gahyeon Kim et.al|[paper](https://arxiv.org/abs/2511.03367)|[code](https://github.com/Gahyeonkim09/AAPL)|<details><summary>detail</summary>Accepted in Pattern Recognition</details>|\n", "Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models": "|**2025-11-5**|**Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models**|Shiho Matta et.al|[paper](https://arxiv.org/abs/2510.26241)|-|-|\n", "SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment": "|**2025-11-4**|**SLIP: Structural-aware Language-Image Pretraining for Vision-Language Alignment**|Wenbo Lu et.al|[paper](https://arxiv.org/abs/2511.03019)|-|<details><summary>detail</summary>Capstone Paper</details>|\n", "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models": "|**2025-11-4**|**NaviTrace: Evaluating Embodied Navigation of Vision-Language Models**|Tim Windecker et.al|[paper](https://arxiv.org/abs/2510.26909)|[code](https://leggedrobotics.github.io/navitrace_webpage/.)|-|\n", "SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics": "|**2025-11-4**|**SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics**|Ailar Mahdizadeh et.al|[paper](https://arxiv.org/abs/2511.02996)|-|-|\n", "XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations": "|**2025-11-4**|**XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations**|Shichao Fan et.al|[paper](https://arxiv.org/abs/2511.02776)|[code](https://xr-1-vla.github.io/.)|-|\n", "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model": "|**2025-11-4**|**Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model**|John Won et.al|[paper](https://arxiv.org/abs/2510.27607)|-|-|\n", "DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding": "|**2025-11-4**|**DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding**|Zixuan Liu et.al|[paper](https://arxiv.org/abs/2511.02495)|[code](https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890)|<details><summary>detail</summary>Advances in Neural Information Processing Systems 2025 (NeurIPS 2025)</details>|\n", "Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation": "|**2025-11-4**|**Unseen from Seen: Rewriting Observation-Instruction Using Foundation Models for Augmenting Vision-Language Navigation**|Ziming Wei et.al|[paper](https://arxiv.org/abs/2503.18065)|[code](https://github.com/SaDil13/VLN-RAM.)|<details><summary>detail</summary>Accepted by IEEE Transactions on Neural Networks and Learning Systems</details>|\n", "CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning": "|**2025-11-4**|**CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning**|Jizheng Ma et.al|[paper](https://arxiv.org/abs/2511.02360)|-|-|\n", "Grounded Vision-Language Interpreter for Integrated Task and Motion Planning": "|**2025-11-4**|**Grounded Vision-Language Interpreter for Integrated Task and Motion Planning**|Jeremy Siburian et.al|[paper](https://arxiv.org/abs/2506.03270)|[code](https://omron-sinicx.github.io/ViLaIn-TAMP/)|<details><summary>detail</summary>Project website: https://omron-sinicx</details>|\n", "LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation": "|**2025-11-3**|**LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation**|Youngjin Hong et.al|[paper](https://arxiv.org/abs/2511.02239)|[code](https://vla2026.github.io/LACY/)|<details><summary>detail</summary>Preprint</details>|\n"}}