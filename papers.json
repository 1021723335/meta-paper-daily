{"source-free": {"Rank and Align: Towards Effective Source-free Graph Domain Adaptation": "|**2024-8-22**|**Rank and Align: Towards Effective Source-free Graph Domain Adaptation**|Junyu Luo et.al|[paper](https://arxiv.org/abs/2408.12185)|-|<details><summary>detail</summary>Published in IJCAI2024</details>|\n", "Source-Free Domain Adaptation Guided by Vision and Vision-Language Pre-Training": "|**2024-8-21**|**Source-Free Domain Adaptation Guided by Vision and Vision-Language Pre-Training**|Wenyu Zhang et.al|[paper](https://arxiv.org/abs/2405.02954)|-|<details><summary>detail</summary>Extension of ICCV paper arXiv:2212</details>|\n", "Source-Free Test-Time Adaptation For Online Surface-Defect Detection": "|**2024-8-18**|**Source-Free Test-Time Adaptation For Online Surface-Defect Detection**|Yiran Song et.al|[paper](https://arxiv.org/abs/2408.09494)|-|<details><summary>detail</summary>ICPR 2024</details>|\n", "Evidential Graph Contrastive Alignment for Source-Free Blending-Target Domain Adaptation": "|**2024-8-14**|**Evidential Graph Contrastive Alignment for Source-Free Blending-Target Domain Adaptation**|Juepeng Zheng et.al|[paper](https://arxiv.org/abs/2408.07527)|-|-|\n", "Source-Free Domain-Invariant Performance Prediction": "|**2024-8-6**|**Source-Free Domain-Invariant Performance Prediction**|Ekaterina Khramtsova et.al|[paper](https://arxiv.org/abs/2408.02209)|-|<details><summary>detail</summary>Accepted in ECCV 2024</details>|\n", "Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence": "|**2024-7-26**|**Learn from the Learnt: Source-Free Active Domain Adaptation via Contrastive Sampling and Visual Persistence**|Mengyao Lyu et.al|[paper](https://arxiv.org/abs/2407.18899)|[code](https://github.com/lyumengyao/lftl.)|<details><summary>detail</summary>ECCV 2024</details>|\n", "Dynamic Retraining-Updating Mean Teacher for Source-Free Object Detection": "|**2024-7-23**|**Dynamic Retraining-Updating Mean Teacher for Source-Free Object Detection**|Trinh Le Ba Khanh et.al|[paper](https://arxiv.org/abs/2407.16497)|[code](https://github.com/lbktrinh/DRU)|<details><summary>detail</summary>ECCV 2024</details>|\n", "Harmonizing Flows: Leveraging normalizing flows for unsupervised and source-free MRI harmonization": "|**2024-7-22**|**Harmonizing Flows: Leveraging normalizing flows for unsupervised and source-free MRI harmonization**|Farzad Beizaee et.al|[paper](https://arxiv.org/abs/2407.15717)|[code](https://github.com/farzad-bz/Harmonizing-Flows)|-|\n", "A Curriculum-style Self-training Approach for Source-Free Semantic Segmentation": "|**2024-7-19**|**A Curriculum-style Self-training Approach for Source-Free Semantic Segmentation**|Yuxi Wang et.al|[paper](https://arxiv.org/abs/2106.11653)|[code](https://github.com/yxiwang/ATP)|<details><summary>detail</summary>This paper is accepted by TPAMI2024</details>|\n", "Memory-Efficient Pseudo-Labeling for Online Source-Free Universal Domain Adaptation using a Gaussian Mixture Model": "|**2024-7-19**|**Memory-Efficient Pseudo-Labeling for Online Source-Free Universal Domain Adaptation using a Gaussian Mixture Model**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2407.14208)|[code](https://github.com/pascalschlachter/GMM.)|<details><summary>detail</summary>Submitted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2025</details>|\n", "Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation": "|**2024-7-18**|**Enhancing Source-Free Domain Adaptive Object Detection with Low-confidence Pseudo Label Distillation**|Ilhoon Yoon et.al|[paper](https://arxiv.org/abs/2407.13524)|[code](https://github.com/junia3/LPLD.)|<details><summary>detail</summary>ECCV 2024</details>|\n", "DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization": "|**2024-7-14**|**DPStyler: Dynamic PromptStyler for Source-Free Domain Generalization**|Yunlong Tang et.al|[paper](https://arxiv.org/abs/2403.16697)|-|<details><summary>detail</summary>Accepted by IEEE TMM</details>|\n", "FastImpute: A Baseline for Open-source, Reference-Free Genotype Imputation Methods -- A Case Study in PRS313": "|**2024-7-12**|**FastImpute: A Baseline for Open-source, Reference-Free Genotype Imputation Methods -- A Case Study in PRS313**|Aaron Ge et.al|[paper](https://arxiv.org/abs/2407.09355)|[code](https://aaronge-2020.github.io/DeepImpute/)|-|\n", "Simplifying Source-Free Domain Adaptation for Object Detection: Effective Self-Training Strategies and Performance Insights": "|**2024-7-10**|**Simplifying Source-Free Domain Adaptation for Object Detection: Effective Self-Training Strategies and Performance Insights**|Yan Hao et.al|[paper](https://arxiv.org/abs/2407.07586)|[code](https://github.com/EPFL-IMOS/simple-SFOD.)|<details><summary>detail</summary>ECCV 2024</details>|\n", "A Reference-free Metric for Language-Queried Audio Source Separation using Contrastive Language-Audio Pretraining": "|**2024-7-5**|**A Reference-free Metric for Language-Queried Audio Source Separation using Contrastive Language-Audio Pretraining**|Feiyang Xiao et.al|[paper](https://arxiv.org/abs/2407.04936)|-|<details><summary>detail</summary>Submitted to DCASE 2024 Workshop</details>|\n"}, "object detection": {"DeTPP: Leveraging Object Detection for Robust Long-Horizon Event Prediction": "|**2024-8-23**|**DeTPP: Leveraging Object Detection for Robust Long-Horizon Event Prediction**|Ivan Karpukhin et.al|[paper](https://arxiv.org/abs/2408.13131)|-|-|\n", "BoostTrack++: using tracklet information to detect more objects in multiple object tracking": "|**2024-8-23**|**BoostTrack++: using tracklet information to detect more objects in multiple object tracking**|Vuka\u0161in Stanojevi\u0107 et.al|[paper](https://arxiv.org/abs/2408.13003)|[code](https://github.com/vukasin-stanojevic/BoostTrack)|-|\n", "CatFree3D: Category-agnostic 3D Object Detection with Diffusion": "|**2024-8-22**|**CatFree3D: Category-agnostic 3D Object Detection with Diffusion**|Wenjing Bian et.al|[paper](https://arxiv.org/abs/2408.12747)|[code](https://bianwenjing.github.io/CatFree3D)|<details><summary>detail</summary>Project page: https://bianwenjing</details>|\n", "Revisiting Cross-Domain Problem for LiDAR-based 3D Object Detection": "|**2024-8-22**|**Revisiting Cross-Domain Problem for LiDAR-based 3D Object Detection**|Ruixiao Zhang et.al|[paper](https://arxiv.org/abs/2408.12708)|-|<details><summary>detail</summary>Accepted by the ICONIP 2024</details>|\n", "StreamLTS: Query-based Temporal-Spatial LiDAR Fusion for Cooperative Object Detection": "|**2024-8-22**|**StreamLTS: Query-based Temporal-Spatial LiDAR Fusion for Cooperative Object Detection**|Yunshuang Yuan et.al|[paper](https://arxiv.org/abs/2407.03825)|[code](https://github.com/YuanYunshuang/CoSense3D)|-|\n", "Class-balanced Open-set Semi-supervised Object Detection for Medical Images": "|**2024-8-22**|**Class-balanced Open-set Semi-supervised Object Detection for Medical Images**|Zhanyun Lu et.al|[paper](https://arxiv.org/abs/2408.12355)|-|-|\n", "OVA-DETR: Open Vocabulary Aerial Object Detection Using Image-Text Alignment and Fusion": "|**2024-8-22**|**OVA-DETR: Open Vocabulary Aerial Object Detection Using Image-Text Alignment and Fusion**|Guoting Wei et.al|[paper](https://arxiv.org/abs/2408.12246)|[code](https://github.com/GT-Wei/OVA-DETR.)|-|\n", "CARLA Drone: Monocular 3D Object Detection from a Different Perspective": "|**2024-8-21**|**CARLA Drone: Monocular 3D Object Detection from a Different Perspective**|Johannes Meier et.al|[paper](https://arxiv.org/abs/2408.11958)|-|-|\n", "Domain-invariant Progressive Knowledge Distillation for UAV-based Object Detection": "|**2024-8-21**|**Domain-invariant Progressive Knowledge Distillation for UAV-based Object Detection**|Liang Yao et.al|[paper](https://arxiv.org/abs/2408.11407)|-|-|\n", "S$^3$-MonoDETR: Supervised Shape&Scale-perceptive Deformable Transformer for Monocular 3D Object Detection": "|**2024-8-20**|**S$^3$-MonoDETR: Supervised Shape&Scale-perceptive Deformable Transformer for Monocular 3D Object Detection**|Xuan He et.al|[paper](https://arxiv.org/abs/2309.00928)|[code](https://github.com/mikasa3lili/S3-MonoDETR.)|<details><summary>detail</summary>The source code will be made publicly available at https://github</details>|\n", "On the Potential of Open-Vocabulary Models for Object Detection in Unusual Street Scenes": "|**2024-8-20**|**On the Potential of Open-Vocabulary Models for Object Detection in Unusual Street Scenes**|Sadia Ilyas et.al|[paper](https://arxiv.org/abs/2408.11221)|-|-|\n", "Domain Adaptation based Object Detection for Autonomous Driving in Foggy and Rainy Weather": "|**2024-8-20**|**Domain Adaptation based Object Detection for Autonomous Driving in Foggy and Rainy Weather**|Jinlong Li et.al|[paper](https://arxiv.org/abs/2307.09676)|-|<details><summary>detail</summary>the final version</details>|\n", "Quantum Inverse Contextual Vision Transformers (Q-ICVT): A New Frontier in 3D Object Detection for AVs": "|**2024-8-20**|**Quantum Inverse Contextual Vision Transformers (Q-ICVT): A New Frontier in 3D Object Detection for AVs**|Sanjay Bhargav Dharavath et.al|[paper](https://arxiv.org/abs/2408.11207)|[code](https://github.com/sanjay-810/Qicvt)|<details><summary>detail</summary>The paper has been accepted as a short paper at CIKM '24</details>|\n", "Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance": "|**2024-8-20**|**Weakly Supervised 3D Object Detection via Multi-Level Visual Guidance**|Kuan-Chih Huang et.al|[paper](https://arxiv.org/abs/2312.07530)|[code](https://github.com/kuanchihhuang/VG-W3D.)|<details><summary>detail</summary>Accepted by ECCV'24</details>|\n", "A Closer Look at Data Augmentation Strategies for Finetuning-Based Low/Few-Shot Object Detection": "|**2024-8-20**|**A Closer Look at Data Augmentation Strategies for Finetuning-Based Low/Few-Shot Object Detection**|Vladislav Li et.al|[paper](https://arxiv.org/abs/2408.10940)|-|-|\n"}, "domain adaptation": {"Cross-Domain Foundation Model Adaptation: Pioneering Computer Vision Models for Geophysical Data Analysis": "|**2024-8-22**|**Cross-Domain Foundation Model Adaptation: Pioneering Computer Vision Models for Geophysical Data Analysis**|Zhixiang Guo et.al|[paper](https://arxiv.org/abs/2408.12396)|-|-|\n", "A Personalized Zero-Shot ECG Arrhythmia Monitoring System: From Sparse Representation Based Domain Adaption to Energy Efficient Abnormal Beat Detection for Practical ECG Surveillance": "|**2024-8-22**|**A Personalized Zero-Shot ECG Arrhythmia Monitoring System: From Sparse Representation Based Domain Adaption to Energy Efficient Abnormal Beat Detection for Practical ECG Surveillance**|Mehmet Yama\u00e7 et.al|[paper](https://arxiv.org/abs/2207.07089)|[code](https://github.com/MertDuman/Zero-Shot-ECG)|<details><summary>detail</summary>Software implementation: https://github</details>|\n", "Rank and Align: Towards Effective Source-free Graph Domain Adaptation": "|**2024-8-22**|**Rank and Align: Towards Effective Source-free Graph Domain Adaptation**|Junyu Luo et.al|[paper](https://arxiv.org/abs/2408.12185)|-|<details><summary>detail</summary>Published in IJCAI2024</details>|\n", "Domain Adaptation for Offline Reinforcement Learning with Limited Samples": "|**2024-8-22**|**Domain Adaptation for Offline Reinforcement Learning with Limited Samples**|Weiqin Chen et.al|[paper](https://arxiv.org/abs/2408.12136)|-|-|\n", "Source-Free Domain Adaptation Guided by Vision and Vision-Language Pre-Training": "|**2024-8-21**|**Source-Free Domain Adaptation Guided by Vision and Vision-Language Pre-Training**|Wenyu Zhang et.al|[paper](https://arxiv.org/abs/2405.02954)|-|<details><summary>detail</summary>Extension of ICCV paper arXiv:2212</details>|\n", "Lighter, Better, Faster Multi-Source Domain Adaptation with Gaussian Mixture Models and Optimal Transport": "|**2024-8-21**|**Lighter, Better, Faster Multi-Source Domain Adaptation with Gaussian Mixture Models and Optimal Transport**|Eduardo Fernandes Montesuma et.al|[paper](https://arxiv.org/abs/2404.10261)|[code](https://github.com/eddardd/gmm_msda)|-|\n", "Domain Adaptation based Object Detection for Autonomous Driving in Foggy and Rainy Weather": "|**2024-8-20**|**Domain Adaptation based Object Detection for Autonomous Driving in Foggy and Rainy Weather**|Jinlong Li et.al|[paper](https://arxiv.org/abs/2307.09676)|-|<details><summary>detail</summary>the final version</details>|\n", "Unified Domain Adaptive Semantic Segmentation": "|**2024-8-20**|**Unified Domain Adaptive Semantic Segmentation**|Zhe Zhang et.al|[paper](https://arxiv.org/abs/2311.13254)|[code](https://github.com/ZHE-SAPI/UDASS)|-|\n", "Collaborative Multi-source Domain Adaptation Through Optimal Transport": "|**2024-8-19**|**Collaborative Multi-source Domain Adaptation Through Optimal Transport**|Omar Ghannou et.al|[paper](https://arxiv.org/abs/2404.06599)|-|-|\n", "DomainForensics: Exposing Face Forgery across Domains via Bi-directional Adaptation": "|**2024-8-19**|**DomainForensics: Exposing Face Forgery across Domains via Bi-directional Adaptation**|Qingxuan Lv et.al|[paper](https://arxiv.org/abs/2312.10680)|-|<details><summary>detail</summary>TIFS 2024</details>|\n", "Adversarial Attacked Teacher for Unsupervised Domain Adaptive Object Detection": "|**2024-8-18**|**Adversarial Attacked Teacher for Unsupervised Domain Adaptive Object Detection**|Kaiwen Wang et.al|[paper](https://arxiv.org/abs/2408.09431)|-|-|\n", "SA-GDA: Spectral Augmentation for Graph Domain Adaptation": "|**2024-8-17**|**SA-GDA: Spectral Augmentation for Graph Domain Adaptation**|Jinhui Pang et.al|[paper](https://arxiv.org/abs/2408.09189)|-|-|\n", "Get Your Embedding Space in Order: Domain-Adaptive Regression for Forest Monitoring": "|**2024-8-15**|**Get Your Embedding Space in Order: Domain-Adaptive Regression for Forest Monitoring**|Sizhuo Li et.al|[paper](https://arxiv.org/abs/2405.00514)|-|<details><summary>detail</summary>Updated with review comments</details>|\n", "Evidential Graph Contrastive Alignment for Source-Free Blending-Target Domain Adaptation": "|**2024-8-14**|**Evidential Graph Contrastive Alignment for Source-Free Blending-Target Domain Adaptation**|Juepeng Zheng et.al|[paper](https://arxiv.org/abs/2408.07527)|-|-|\n", "MLoRA: Multi-Domain Low-Rank Adaptive Network for CTR Prediction": "|**2024-8-14**|**MLoRA: Multi-Domain Low-Rank Adaptive Network for CTR Prediction**|Zhiming Yang et.al|[paper](https://arxiv.org/abs/2408.08913)|-|-|\n"}, "domain generalization": {"DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation": "|**2024-8-23**|**DOMAINEVAL: An Auto-Constructed Benchmark for Multi-Domain Code Generation**|Qiming Zhu et.al|[paper](https://arxiv.org/abs/2408.13204)|[code](https://domaineval.github.io/.)|-|\n", "Domain Generalization through Meta-Learning: A Survey": "|**2024-8-22**|**Domain Generalization through Meta-Learning: A Survey**|Arsham Gholamzadeh Khoee et.al|[paper](https://arxiv.org/abs/2404.02785)|-|-|\n", "Mixstyle-Entropy: Domain Generalization with Causal Intervention and Perturbation": "|**2024-8-22**|**Mixstyle-Entropy: Domain Generalization with Causal Intervention and Perturbation**|Luyao Tang et.al|[paper](https://arxiv.org/abs/2408.03608)|-|<details><summary>detail</summary>Accepted by BMVC2024</details>|\n", "DGMamba: Domain Generalization via Generalized State Space Model": "|**2024-8-21**|**DGMamba: Domain Generalization via Generalized State Space Model**|Shaocong Long et.al|[paper](https://arxiv.org/abs/2404.07794)|[code](https://github.com/longshaocong/DGMamba.)|<details><summary>detail</summary>ACM MM 2024</details>|\n", "PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting and Permitting domain": "|**2024-8-21**|**PermitQA: A Benchmark for Retrieval Augmented Generation in Wind Siting and Permitting domain**|Rounak Meyur et.al|[paper](https://arxiv.org/abs/2408.11800)|-|-|\n", "NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation": "|**2024-8-21**|**NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation**|Zhenye Lou et.al|[paper](https://arxiv.org/abs/2408.11787)|[code](https://github.com/xq141839/NuSegDG.)|<details><summary>detail</summary>Under Reivew</details>|\n", "LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain": "|**2024-8-19**|**LegalBench-RAG: A Benchmark for Retrieval-Augmented Generation in the Legal Domain**|Nicholas Pipitone et.al|[paper](https://arxiv.org/abs/2408.10343)|[code](https://github.com/zeroentropy-cc/legalbenchrag.)|-|\n", "Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics": "|**2024-8-19**|**Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics**|Jiaqi Yue et.al|[paper](https://arxiv.org/abs/2403.14362)|-|<details><summary>detail</summary>This work has been submitted to the IEEE for possible publication</details>|\n", "ESP-MedSAM: Efficient Self-Prompting SAM for Universal Domain-Generalized Medical Image Segmentation": "|**2024-8-17**|**ESP-MedSAM: Efficient Self-Prompting SAM for Universal Domain-Generalized Medical Image Segmentation**|Qing Xu et.al|[paper](https://arxiv.org/abs/2407.14153)|[code](https://github.com/xq141839/ESP-MedSAM.)|<details><summary>detail</summary>Under Review</details>|\n", "StylePrompter: Enhancing Domain Generalization with Test-Time Style Priors": "|**2024-8-17**|**StylePrompter: Enhancing Domain Generalization with Test-Time Style Priors**|Jiao Zhang et.al|[paper](https://arxiv.org/abs/2408.09138)|-|-|\n", "Model Attribution in LLM-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning": "|**2024-8-14**|**Model Attribution in LLM-Generated Disinformation: A Domain Generalization Approach with Supervised Contrastive Learning**|Alimohammad Beigi et.al|[paper](https://arxiv.org/abs/2407.21264)|-|-|\n", "Robust Domain Generalization for Multi-modal Object Recognition": "|**2024-8-11**|**Robust Domain Generalization for Multi-modal Object Recognition**|Yuxin Qiao et.al|[paper](https://arxiv.org/abs/2408.05831)|-|-|\n", "Moment&Cross: Next-Generation Real-Time Cross-Domain CTR Prediction for Live-Streaming Recommendation at Kuaishou": "|**2024-8-11**|**Moment&Cross: Next-Generation Real-Time Cross-Domain CTR Prediction for Live-Streaming Recommendation at Kuaishou**|Jiangxia Cao et.al|[paper](https://arxiv.org/abs/2408.05709)|-|<details><summary>detail</summary>Work in progress</details>|\n", "Generalizing Few Data to Unseen Domains Flexibly Based on Label Smoothing Integrated with Distributionally Robust Optimization": "|**2024-8-9**|**Generalizing Few Data to Unseen Domains Flexibly Based on Label Smoothing Integrated with Distributionally Robust Optimization**|Yangdi Wang et.al|[paper](https://arxiv.org/abs/2408.05082)|-|-|\n", "HiLo: A Learning Framework for Generalized Category Discovery Robust to Domain Shifts": "|**2024-8-8**|**HiLo: A Learning Framework for Generalized Category Discovery Robust to Domain Shifts**|Hongjun Wang et.al|[paper](https://arxiv.org/abs/2408.04591)|-|-|\n"}, "vision language": {"Foundational Model for Electron Micrograph Analysis: Instruction-Tuning Small-Scale Language-and-Vision Assistant for Enterprise Adoption": "|**2024-8-23**|**Foundational Model for Electron Micrograph Analysis: Instruction-Tuning Small-Scale Language-and-Vision Assistant for Enterprise Adoption**|Sakhinana Sagar Srinivas et.al|[paper](https://arxiv.org/abs/2408.13248)|-|<details><summary>detail</summary>Our paper is published at ICML 2024 Workshop ML for Life and Material Science: From Theory to Industry Applications</details>|\n", "Solving Robotics Problems in Zero-Shot with Vision-Language Models": "|**2024-8-23**|**Solving Robotics Problems in Zero-Shot with Vision-Language Models**|Zidan Wang et.al|[paper](https://arxiv.org/abs/2407.19094)|-|<details><summary>detail</summary>aka Wonderful Team</details>|\n", "ParGo: Bridging Vision-Language with Partial and Global Views": "|**2024-8-23**|**ParGo: Bridging Vision-Language with Partial and Global Views**|An-Lan Wang et.al|[paper](https://arxiv.org/abs/2408.12928)|-|-|\n", "SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models": "|**2024-8-23**|**SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models**|Youngjoon Yu et.al|[paper](https://arxiv.org/abs/2408.12114)|[code](https://github.com/top-yun/SPARK)|<details><summary>detail</summary>Codes and data are available at https://github</details>|\n", "LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial Description": "|**2024-8-23**|**LLaVA-VSD: Large Language-and-Vision Assistant for Visual Spatial Description**|Yizhang Jin et.al|[paper](https://arxiv.org/abs/2408.04957)|-|-|\n", "Building and better understanding vision-language models: insights and future directions": "|**2024-8-22**|**Building and better understanding vision-language models: insights and future directions**|Hugo Lauren\u00e7on et.al|[paper](https://arxiv.org/abs/2408.12637)|-|-|\n", "Video-to-Text Pedestrian Monitoring (VTPM): Leveraging Computer Vision and Large Language Models for Privacy-Preserve Pedestrian Activity Monitoring at Intersections": "|**2024-8-21**|**Video-to-Text Pedestrian Monitoring (VTPM): Leveraging Computer Vision and Large Language Models for Privacy-Preserve Pedestrian Activity Monitoring at Intersections**|Ahmed S. Abdelrahman et.al|[paper](https://arxiv.org/abs/2408.11649)|-|-|\n", "Source-Free Domain Adaptation Guided by Vision and Vision-Language Pre-Training": "|**2024-8-21**|**Source-Free Domain Adaptation Guided by Vision and Vision-Language Pre-Training**|Wenyu Zhang et.al|[paper](https://arxiv.org/abs/2405.02954)|-|<details><summary>detail</summary>Extension of ICCV paper arXiv:2212</details>|\n", "AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors": "|**2024-8-21**|**AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors**|You-Ming Chang et.al|[paper](https://arxiv.org/abs/2310.17419)|[code](https://github.com/nctu-eva-lab/AntifakePrompt.)|-|\n", "Reflex-Based Open-Vocabulary Navigation without Prior Knowledge Using Omnidirectional Camera and Multiple Vision-Language Models": "|**2024-8-21**|**Reflex-Based Open-Vocabulary Navigation without Prior Knowledge Using Omnidirectional Camera and Multiple Vision-Language Models**|Kento Kawaharazuka et.al|[paper](https://arxiv.org/abs/2408.11380)|[code](https://haraduka.github.io/omnidirectional-vlm/)|<details><summary>detail</summary>Advanced Robotics</details>|\n", "Swarm Intelligence in Geo-Localization: A Multi-Agent Large Vision-Language Model Collaborative Framework": "|**2024-8-20**|**Swarm Intelligence in Geo-Localization: A Multi-Agent Large Vision-Language Model Collaborative Framework**|Xiao Han et.al|[paper](https://arxiv.org/abs/2408.11312)|-|-|\n", "UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation": "|**2024-8-20**|**UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation**|Xiangyu Zhao et.al|[paper](https://arxiv.org/abs/2408.11305)|[code](https://github.com/xiangyu-mm/UniFashion.)|-|\n", "Making Large Vision Language Models to be Good Few-shot Learners": "|**2024-8-20**|**Making Large Vision Language Models to be Good Few-shot Learners**|Fan Liu et.al|[paper](https://arxiv.org/abs/2408.11297)|-|-|\n", "Towards Analyzing and Mitigating Sycophancy in Large Vision-Language Models": "|**2024-8-20**|**Towards Analyzing and Mitigating Sycophancy in Large Vision-Language Models**|Yunpu Zhao et.al|[paper](https://arxiv.org/abs/2408.11261)|-|-|\n", "Creative Problem Solving in Large Language and Vision Models -- What Would it Take?": "|**2024-8-20**|**Creative Problem Solving in Large Language and Vision Models -- What Would it Take?**|Lakshmi Nair et.al|[paper](https://arxiv.org/abs/2405.01453)|[code](https://github.com/lnairGT/creative-problem-solving-LLMs)|-|\n"}}