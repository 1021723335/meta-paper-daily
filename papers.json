{"source-free": {"Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment": "|**2025-9-12**|**Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment**|Rini Smita Thakur et.al|[paper](https://arxiv.org/abs/2509.10134)|[code](https://visdomlab.github.io/GCL/.)|<details><summary>detail</summary>Accepted in BMVC 2025</details>|\n", "Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models": "|**2025-9-10**|**Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models**|Kosuke Kihara et.al|[paper](https://arxiv.org/abs/2509.08372)|-|<details><summary>detail</summary>Accepted by the IEEE ICIP 2025 Satellite Workshop 1: Edge Intelligence: Smart</details>|\n", "StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails": "|**2025-9-2**|**StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails**|Hritik Arasu et.al|[paper](https://arxiv.org/abs/2509.02982)|-|<details><summary>detail</summary>5 page paper</details>|\n", "Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration": "|**2025-8-28**|**Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2508.20836)|-|-|\n", "VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection": "|**2025-8-26**|**VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection**|Jianhong Han et.al|[paper](https://arxiv.org/abs/2508.11167)|-|<details><summary>detail</summary>Manuscript submitted to IEEE TCSVT</details>|\n", "Towards Source-Free Machine Unlearning": "|**2025-8-20**|**Towards Source-Free Machine Unlearning**|Sk Miraj Ahmed et.al|[paper](https://arxiv.org/abs/2508.15127)|-|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method": "|**2025-8-14**|**Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2508.09202)|-|-|\n", "Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation": "|**2025-8-7**|**Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation**|Jianming Liu et.al|[paper](https://arxiv.org/abs/2508.05213)|[code](https://github.com/ljm198134/TVGTANet.)|-|\n", "Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning": "|**2025-8-4**|**Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning**|Sijia Wang et.al|[paper](https://arxiv.org/abs/2508.02039)|-|-|\n", "Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments": "|**2025-7-30**|**Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2501.02184)|-|-|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-7-28**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning": "|**2025-7-26**|**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**|Sanqing Qu et.al|[paper](https://arxiv.org/abs/2403.14410)|[code](https://github.com/ispc-lab/GLC-plus.)|<details><summary>detail</summary>A substantial extension of the CVPR paper \"Upcycling Models under Domain and Category Shift\"</details>|\n", "SFUOD: Source-Free Unknown Object Detection": "|**2025-7-23**|**SFUOD: Source-Free Unknown Object Detection**|Keon-Hee Park et.al|[paper](https://arxiv.org/abs/2507.17373)|-|<details><summary>detail</summary>This paper has been accepted by ICCV 2025</details>|\n", "Text-Driven Causal Representation Learning for Source-Free Domain Generalization": "|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|\n", "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting": "|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|\n"}, "object detection": {"Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures": "|**2025-9-12**|**Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures**|Waqar Ahmad et.al|[paper](https://arxiv.org/abs/2509.08926)|-|-|\n", "HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection": "|**2025-9-11**|**HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection**|Harris Song et.al|[paper](https://arxiv.org/abs/2508.21135)|-|<details><summary>detail</summary>fix typos</details>|\n", "A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images": "|**2025-9-11**|**A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images**|Hossein Yazdanjouei et.al|[paper](https://arxiv.org/abs/2509.09750)|-|-|\n", "Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception": "|**2025-9-11**|**Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception**|Spyridon Loukovitis et.al|[paper](https://arxiv.org/abs/2509.09297)|-|-|\n", "Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection": "|**2025-9-11**|**Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection**|Jiasheng Guo et.al|[paper](https://arxiv.org/abs/2509.09183)|-|-|\n", "RT-DETR++ for UAV Object Detection": "|**2025-9-11**|**RT-DETR++ for UAV Object Detection**|Yuan Shufang et.al|[paper](https://arxiv.org/abs/2509.09157)|-|-|\n", "AdvReal: Physical Adversarial Patch Generation Framework for Security Evaluation of Object Detection Systems": "|**2025-9-10**|**AdvReal: Physical Adversarial Patch Generation Framework for Security Evaluation of Object Detection Systems**|Yuanhao Huang et.al|[paper](https://arxiv.org/abs/2505.16402)|[code](https://github.com/Huangyh98/AdvReal.git.)|-|\n", "IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection": "|**2025-9-10**|**IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection**|Jifeng Shen et.al|[paper](https://arxiv.org/abs/2509.09085)|[code](https://github.com/61s61min/IRDFusion.git.)|-|\n", "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation": "|**2025-9-10**|**LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation**|Yang Zhou et.al|[paper](https://arxiv.org/abs/2503.13794)|-|-|\n", "A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models": "|**2025-9-10**|**A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models**|Edwine Nabahirwa et.al|[paper](https://arxiv.org/abs/2509.08490)|-|<details><summary>detail</summary>72 Pages</details>|\n", "InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection": "|**2025-9-10**|**InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection**|Zhongyu Xia et.al|[paper](https://arxiv.org/abs/2509.08374)|-|-|\n", "Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection": "|**2025-9-10**|**Dual-Thresholding Heatmaps to Cluster Proposals for Weakly Supervised Object Detection**|Yuelin Guo et.al|[paper](https://arxiv.org/abs/2509.08289)|[code](https://github.com/gyl2565309278/DTH-CP.)|<details><summary>detail</summary>This work has been submitted to the IEEE for possible publication</details>|\n", "Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning": "|**2025-9-9**|**Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning**|Houjian Yu et.al|[paper](https://arxiv.org/abs/2509.08126)|[code](https://z.umn.edu/ogrg)|<details><summary>detail</summary>2025 IEEE-RAS 24th International Conference on Humanoid Robots</details>|\n", "Light-Weight Cross-Modal Enhancement Method with Benchmark Construction for UAV-based Open-Vocabulary Object Detection": "|**2025-9-9**|**Light-Weight Cross-Modal Enhancement Method with Benchmark Construction for UAV-based Open-Vocabulary Object Detection**|Zhenhai Weng et.al|[paper](https://arxiv.org/abs/2509.06011)|-|-|\n", "GCRPNet: Graph-Enhanced Contextual and Regional Perception Network for Salient Object Detection in Optical Remote Sensing Images": "|**2025-9-9**|**GCRPNet: Graph-Enhanced Contextual and Regional Perception Network for Salient Object Detection in Optical Remote Sensing Images**|Mengyu Ren et.al|[paper](https://arxiv.org/abs/2508.10542)|-|-|\n"}, "domain adaptation": {"WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers": "|**2025-9-12**|**WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers**|Akshat Pandey et.al|[paper](https://arxiv.org/abs/2509.10452)|-|-|\n", "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment": "|**2025-9-12**|**Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment**|Rini Smita Thakur et.al|[paper](https://arxiv.org/abs/2509.10134)|[code](https://visdomlab.github.io/GCL/.)|<details><summary>detail</summary>Accepted in BMVC 2025</details>|\n", "FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification": "|**2025-9-12**|**FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification**|Weitao Tang et.al|[paper](https://arxiv.org/abs/2509.10082)|-|-|\n", "SCoDA: Self-supervised Continual Domain Adaptation": "|**2025-9-11**|**SCoDA: Self-supervised Continual Domain Adaptation**|Chirayu Agrawal et.al|[paper](https://arxiv.org/abs/2509.09935)|-|<details><summary>detail</summary>Submitted to ICVGIP 2025</details>|\n", "Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization": "|**2025-9-11**|**Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization**|Hangyi Jia et.al|[paper](https://arxiv.org/abs/2509.09321)|-|-|\n", "E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting": "|**2025-9-10**|**E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting**|Samuel Felipe dos Santos et.al|[paper](https://arxiv.org/abs/2509.09006)|-|<details><summary>detail</summary>Journal ref:38th SIBGRAPI - Conference on Graphics</details>|\n", "Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models": "|**2025-9-10**|**Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models**|Kosuke Kihara et.al|[paper](https://arxiv.org/abs/2509.08372)|-|<details><summary>detail</summary>Accepted by the IEEE ICIP 2025 Satellite Workshop 1: Edge Intelligence: Smart</details>|\n", "GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation": "|**2025-9-9**|**GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation**|Seongho Kim et.al|[paper](https://arxiv.org/abs/2509.08232)|[code](https://github.com/ta-ho/GTA-Crime.)|-|\n", "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning": "|**2025-9-8**|**Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning**|Yingxu Wang et.al|[paper](https://arxiv.org/abs/2508.00716)|-|-|\n", "DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series": "|**2025-9-7**|**DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series**|Zahra Zamanzadeh Darban et.al|[paper](https://arxiv.org/abs/2404.11269)|-|-|\n", "Domain Adaptation for Different Sensor Configurations in 3D Object Detection": "|**2025-9-4**|**Domain Adaptation for Different Sensor Configurations in 3D Object Detection**|Satoshi Tanaka et.al|[paper](https://arxiv.org/abs/2509.04711)|-|-|\n", "Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation": "|**2025-9-4**|**Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation**|Jianhua Liu et.al|[paper](https://arxiv.org/abs/2504.05774)|-|-|\n", "In-Context Policy Adaptation via Cross-Domain Skill Diffusion": "|**2025-9-4**|**In-Context Policy Adaptation via Cross-Domain Skill Diffusion**|Minjong Yoo et.al|[paper](https://arxiv.org/abs/2509.04535)|-|-|\n", "Domain Adaptation of LLMs for Process Data": "|**2025-9-3**|**Domain Adaptation of LLMs for Process Data**|Rafael Seidi Oyamada et.al|[paper](https://arxiv.org/abs/2509.03161)|-|-|\n", "Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression": "|**2025-9-3**|**Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression**|Uddeshya Upadhyay et.al|[paper](https://arxiv.org/abs/2509.03012)|-|-|\n"}, "domain generalization": {"When and How Does CLIP Enable Domain and Compositional Generalization?": "|**2025-9-12**|**When and How Does CLIP Enable Domain and Compositional Generalization?**|Elias Kempf et.al|[paper](https://arxiv.org/abs/2502.09507)|-|<details><summary>detail</summary>ICML 2025 (Spotlight)</details>|\n", "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method": "|**2025-9-12**|**GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method**|Hailong Yang et.al|[paper](https://arxiv.org/abs/2509.10018)|-|-|\n", "Diffusion-Based Action Recognition Generalizes to Untrained Domains": "|**2025-9-10**|**Diffusion-Based Action Recognition Generalizes to Untrained Domains**|Rogerio Guimaraes et.al|[paper](https://arxiv.org/abs/2509.08908)|[code](https://www.vision.caltech.edu/actiondiff/)|-|\n", "GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation": "|**2025-9-9**|**GTA-Crime: A Synthetic Dataset and Generation Framework for Fatal Violence Detection with Adversarial Snippet-Level Domain Adaptation**|Seongho Kim et.al|[paper](https://arxiv.org/abs/2509.08232)|[code](https://github.com/ta-ho/GTA-Crime.)|-|\n", "One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation": "|**2025-9-9**|**One View, Many Worlds: Single-Image to 3D Object Meets Generative Domain Randomization for One-Shot 6D Pose Estimation**|Zheng Geng et.al|[paper](https://arxiv.org/abs/2509.07978)|[code](https://gzwsama.github.io/OnePoseviaGen.github.io/)|<details><summary>detail</summary>CoRL 2025 Oral</details>|\n", "Process-Supervised Reward Models for Verifying Clinical Note Generation: A Scalable Approach Guided by Domain Expertise": "|**2025-9-8**|**Process-Supervised Reward Models for Verifying Clinical Note Generation: A Scalable Approach Guided by Domain Expertise**|Hanyin Wang et.al|[paper](https://arxiv.org/abs/2412.12583)|-|-|\n", "Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI Harmonization Method": "|**2025-9-8**|**Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI Harmonization Method**|Daniel Scholz et.al|[paper](https://arxiv.org/abs/2509.06592)|-|-|\n", "ConstStyle: Robust Domain Generalization with Unified Style Transformation": "|**2025-9-7**|**ConstStyle: Robust Domain Generalization with Unified Style Transformation**|Nam Duong Tran et.al|[paper](https://arxiv.org/abs/2509.05975)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain": "|**2025-9-3**|**Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain**|Shakiba Amirshahi et.al|[paper](https://arxiv.org/abs/2509.03787)|[code](https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL)|-|\n", "Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach": "|**2025-9-2**|**Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach**|Midhat Urooj et.al|[paper](https://arxiv.org/abs/2509.02918)|-|<details><summary>detail</summary>Accepted in ANSyA 2025: 1st International Workshop on Advanced Neuro-Symbolic Applications</details>|\n", "SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images": "|**2025-9-2**|**SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images**|Pushpendra Dhakara et.al|[paper](https://arxiv.org/abs/2509.02287)|-|-|\n", "FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain": "|**2025-9-2**|**FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain**|Anum Afzal et.al|[paper](https://arxiv.org/abs/2509.02198)|-|-|\n", "REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization": "|**2025-9-1**|**REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization**|Maximilian P. Oppelt et.al|[paper](https://arxiv.org/abs/2509.01642)|-|-|\n", "Target-Oriented Single Domain Generalization": "|**2025-8-30**|**Target-Oriented Single Domain Generalization**|Marzi Heidari et.al|[paper](https://arxiv.org/abs/2509.00351)|-|-|\n", "MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification": "|**2025-8-29**|**MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification**|Hikmat Khan et.al|[paper](https://arxiv.org/abs/2509.00311)|[code](https://github.com/hikmatkhan/MorphGen)|-|\n"}, "vision language": {"GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation": "|**2025-9-12**|**GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation**|Hang Yin et.al|[paper](https://arxiv.org/abs/2509.10454)|[code](https://bagh2178.github.io/GC-VLN/))|<details><summary>detail</summary>CoRL 2025</details>|\n", "JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse": "|**2025-9-12**|**JARVIS-VLA: Post-Training Large-Scale Vision Language Models to Play Visual Games with Keyboards and Mouse**|Muyao Li et.al|[paper](https://arxiv.org/abs/2503.16365)|[code](https://craftjarvis.github.io/JarvisVLA.)|<details><summary>detail</summary>Accepted by ACL 2025</details>|\n", "Detecting Text Manipulation in Images using Vision Language Models": "|**2025-9-12**|**Detecting Text Manipulation in Images using Vision Language Models**|Vidit Vidit et.al|[paper](https://arxiv.org/abs/2509.10278)|[code](https://www.idiap.ch/paper/textvlmdet/)|<details><summary>detail</summary>Accepted in Synthetic Realities and Biometric Security Workshop BMVC-2025</details>|\n", "MoPD: Mixture-of-Prompts Distillation for Vision-Language Models": "|**2025-9-12**|**MoPD: Mixture-of-Prompts Distillation for Vision-Language Models**|Yang Chen et.al|[paper](https://arxiv.org/abs/2412.19087)|-|-|\n", "Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation": "|**2025-9-12**|**Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation**|Hongji Yang et.al|[paper](https://arxiv.org/abs/2505.16763)|-|<details><summary>detail</summary>Accepted by ACL2025 Findings</details>|\n", "Data Matters Most: Auditing Social Bias in Contrastive Vision Language Models": "|**2025-9-11**|**Data Matters Most: Auditing Social Bias in Contrastive Vision Language Models**|Zahraa Al Sahili et.al|[paper](https://arxiv.org/abs/2501.13223)|-|-|\n", "Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models": "|**2025-9-11**|**Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models**|Zahraa Al Sahili et.al|[paper](https://arxiv.org/abs/2505.14160)|-|-|\n", "Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics": "|**2025-9-11**|**Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics**|Dikshant Sagar et.al|[paper](https://arxiv.org/abs/2509.08461)|-|-|\n", "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model": "|**2025-9-11**|**VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model**|Yihao Wang et.al|[paper](https://arxiv.org/abs/2509.09372)|[code](https://vla-adapter.github.io/.)|-|\n", "Image Recognition with Vision and Language Embeddings of VLMs": "|**2025-9-11**|**Image Recognition with Vision and Language Embeddings of VLMs**|Illia Volkov et.al|[paper](https://arxiv.org/abs/2509.09311)|[code](https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.)|-|\n", "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models": "|**2025-9-11**|**villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models**|Xiaoyu Chen et.al|[paper](https://arxiv.org/abs/2507.23682)|[code](https://aka.ms/villa-x)|<details><summary>detail</summary>Project page: https://aka</details>|\n", "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves": "|**2025-9-11**|**IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves**|Ruofan Wang et.al|[paper](https://arxiv.org/abs/2411.00827)|[code](https://roywang021.github.io/VLJailbreakBench.)|-|\n", "Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models": "|**2025-9-10**|**Imagine, Verify, Execute: Memory-guided Agentic Exploration with Vision-Language Models**|Seungjae Lee et.al|[paper](https://arxiv.org/abs/2505.07815)|[code](https://ive-robot.github.io/)|<details><summary>detail</summary>Project webpage: https://ive-robot</details>|\n", "SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models": "|**2025-9-10**|**SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models**|Hengyu Fang et.al|[paper](https://arxiv.org/abs/2509.09090)|-|-|\n", "Can Vision-Language Models Solve Visual Math Equations?": "|**2025-9-10**|**Can Vision-Language Models Solve Visual Math Equations?**|Monjoy Narayan Choudhury et.al|[paper](https://arxiv.org/abs/2509.09013)|-|<details><summary>detail</summary>Monjoy Narayan Choudhury and Junling Wang contributed equally to this work</details>|\n"}}