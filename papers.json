{"source-free": {"Rethinking Test-Time Training: Tilting The Latent Distribution For Few-Shot Source-Free Adaptation": "|**2026-2-2**|**Rethinking Test-Time Training: Tilting The Latent Distribution For Few-Shot Source-Free Adaptation**|Tahir Qasim Syed et.al|[paper](https://arxiv.org/abs/2602.02633)|-|-|\n", "Collision-free Source Seeking and Flocking Control of Multi-agents with Connectivity Preservation": "|**2026-1-30**|**Collision-free Source Seeking and Flocking Control of Multi-agents with Connectivity Preservation**|Tinghua Li et.al|[paper](https://arxiv.org/abs/2301.04576)|-|<details><summary>detail</summary>Published in IEEE Transactions on Automatic Control</details>|\n", "Source Coding with Free Bits and the Multi-Way Number Partitioning Problem": "|**2026-1-29**|**Source Coding with Free Bits and the Multi-Way Number Partitioning Problem**|Niloufar Ahmadypour et.al|[paper](https://arxiv.org/abs/2009.02710)|-|-|\n", "Beyond Retraining: Training-Free Unknown Class Filtering for Source-Free Open Set Domain Adaptation of Vision-Language Models": "|**2026-1-28**|**Beyond Retraining: Training-Free Unknown Class Filtering for Source-Free Open Set Domain Adaptation of Vision-Language Models**|Yongguang Li et.al|[paper](https://arxiv.org/abs/2504.14224)|-|<details><summary>detail</summary>Core methods unchanged</details>|\n", "A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency": "|**2026-1-28**|**A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency**|Debopom Sutradhar et.al|[paper](https://arxiv.org/abs/2601.20284)|-|<details><summary>detail</summary>Manuscript under review in IEEE Transactions on Image Processing</details>|\n", "Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity": "|**2026-1-24**|**Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity**|Harsharaj Pathak et.al|[paper](https://arxiv.org/abs/2601.17408)|-|-|\n", "Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results": "|**2026-1-23**|**Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results**|Rohan Palanikumar et.al|[paper](https://arxiv.org/abs/2511.00752)|-|-|\n", "Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection": "|**2026-1-20**|**Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection**|Huizai Yao et.al|[paper](https://arxiv.org/abs/2511.07301)|-|<details><summary>detail</summary>AAAI 2026</details>|\n", "Towards Unbiased Source-Free Object Detection via Vision Foundation Models": "|**2026-1-19**|**Towards Unbiased Source-Free Object Detection via Vision Foundation Models**|Zhi Cai et.al|[paper](https://arxiv.org/abs/2601.12765)|-|-|\n", "Unified Source-Free Domain Adaptation": "|**2026-1-18**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|[code](https://github.com/tntek/CausalDA.)|-|\n", "GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling": "|**2026-1-16**|**GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2601.11161)|[code](https://github.com/pascalschlachter/GMM-COMET.)|-|\n", "SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling": "|**2026-1-13**|**SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling**|Xi Chen et.al|[paper](https://arxiv.org/abs/2601.08608)|[code](https://github.com/chenxi52/SfMamba.)|-|\n", "Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation": "|**2026-1-13**|**Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation**|Yuan Gao et.al|[paper](https://arxiv.org/abs/2601.08375)|-|-|\n", "Empowering Source-Free Domain Adaptation via MLLM-Guided Reliability-Based Curriculum Learning": "|**2026-1-5**|**Empowering Source-Free Domain Adaptation via MLLM-Guided Reliability-Based Curriculum Learning**|Dongjie Chen et.al|[paper](https://arxiv.org/abs/2405.18376)|[code](https://github.com/Dong-Jie-Chen/RCL.)|-|\n", "Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection": "|**2025-12-24**|**Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection**|Sairam VCR et.al|[paper](https://arxiv.org/abs/2512.17514)|-|-|\n"}, "object detection": {"Depth as Prior Knowledge for Object Detection": "|**2026-2-5**|**Depth as Prior Knowledge for Object Detection**|Moussa Kassem Sbeyti et.al|[paper](https://arxiv.org/abs/2602.05730)|[code](https://github.com/mos-ks/DepthPrior.)|<details><summary>detail</summary>This work has been submitted to the IEEE for possible publication</details>|\n", "All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles": "|**2026-2-4**|**All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles**|Sayed Pedram Haeri Boroujeni et.al|[paper](https://arxiv.org/abs/2510.26641)|-|-|\n", "A labeled dataset of simulated phlebotomy procedures for medical AI: polygon annotations for object detection and human-object interaction": "|**2026-2-4**|**A labeled dataset of simulated phlebotomy procedures for medical AI: polygon annotations for object detection and human-object interaction**|Ra\u00fal Jim\u00e9nez Cruz et.al|[paper](https://arxiv.org/abs/2602.04624)|-|-|\n", "Beyond Global Scanning: Adaptive Visual State Space Modeling for Salient Object Detection in Optical Remote Sensing Images": "|**2026-2-4**|**Beyond Global Scanning: Adaptive Visual State Space Modeling for Salient Object Detection in Optical Remote Sensing Images**|Mengyu Ren et.al|[paper](https://arxiv.org/abs/2508.10542)|-|-|\n", "RAWDet-7: A Multi-Scenario Benchmark for Object Detection and Description on Quantized RAW Images": "|**2026-2-3**|**RAWDet-7: A Multi-Scenario Benchmark for Object Detection and Description on Quantized RAW Images**|Mishal Fatima et.al|[paper](https://arxiv.org/abs/2602.03760)|-|<details><summary>detail</summary>*Equal Contribution</details>|\n", "SPWOOD: Sparse Partial Weakly-Supervised Oriented Object Detection": "|**2026-2-3**|**SPWOOD: Sparse Partial Weakly-Supervised Oriented Object Detection**|Wei Zhang et.al|[paper](https://arxiv.org/abs/2602.03634)|[code](https://github.com/VisionXLab/SPWOOD.)|<details><summary>detail</summary>The Fourteenth International Conference on Learning Representations (ICLR 2026)</details>|\n", "High-Resolution Underwater Camouflaged Object Detection: GBU-UCOD Dataset and Topology-Aware and Frequency-Decoupled Networks": "|**2026-2-3**|**High-Resolution Underwater Camouflaged Object Detection: GBU-UCOD Dataset and Topology-Aware and Frequency-Decoupled Networks**|Wenji Wu et.al|[paper](https://arxiv.org/abs/2602.03591)|[code](https://github.com/Wuwenji18/GBU-UCOD.)|-|\n", "Inlier-Centric Post-Training Quantization for Object Detection Models": "|**2026-2-3**|**Inlier-Centric Post-Training Quantization for Object Detection Models**|Minsu Kim et.al|[paper](https://arxiv.org/abs/2602.03472)|-|-|\n", "FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion": "|**2026-2-3**|**FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion**|Chen-Bin Feng et.al|[paper](https://arxiv.org/abs/2602.03137)|[code](https://intellindust-ai-lab.github.io/projects/FSOD-VFM.)|<details><summary>detail</summary>Accepted by ICLR 2026</details>|\n", "Enhanced Detection of Tiny Objects in Aerial Images": "|**2026-2-2**|**Enhanced Detection of Tiny Objects in Aerial Images**|Kihyun Kim et.al|[paper](https://arxiv.org/abs/2509.17078)|[code](https://github.com/Kihyun11/MoonNet)|-|\n", "Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding": "|**2026-2-2**|**Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding**|Soheil Behnam Roudsari et.al|[paper](https://arxiv.org/abs/2602.02167)|-|-|\n", "Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images": "|**2026-2-2**|**Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images**|Shuai Yang et.al|[paper](https://arxiv.org/abs/2602.01954)|-|-|\n", "Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework": "|**2026-2-1**|**Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework**|Wenzhuo Zhao et.al|[paper](https://arxiv.org/abs/2602.01593)|-|-|\n", "Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT": "|**2026-2-1**|**Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT**|Ninnart Fuengfusin et.al|[paper](https://arxiv.org/abs/2601.12638)|-|-|\n", "HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection": "|**2026-1-31**|**HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection**|Junwen Chen et.al|[paper](https://arxiv.org/abs/2510.05609)|[code](https://github.com/cjw2021/HOI-R1.)|-|\n"}, "domain adaptation": {"Cross-Domain Offline Policy Adaptation via Selective Transition Correction": "|**2026-2-5**|**Cross-Domain Offline Policy Adaptation via Selective Transition Correction**|Mengbei Yan et.al|[paper](https://arxiv.org/abs/2602.05776)|-|-|\n", "Fast Private Adaptive Query Answering for Large Data Domains": "|**2026-2-5**|**Fast Private Adaptive Query Answering for Large Data Domains**|Miguel Fuentes et.al|[paper](https://arxiv.org/abs/2602.05674)|-|-|\n", "Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation": "|**2026-2-4**|**Cross-Domain Few-Shot Segmentation via Multi-view Progressive Adaptation**|Jiahao Nie et.al|[paper](https://arxiv.org/abs/2602.05217)|-|-|\n", "SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics": "|**2026-2-4**|**SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics**|Santosh Chapagain et.al|[paper](https://arxiv.org/abs/2601.12131)|-|<details><summary>detail</summary>This is preliminary work towards a broader SolarGPT framework</details>|\n", "An Example for Domain Adaptation Using CycleGAN": "|**2026-2-4**|**An Example for Domain Adaptation Using CycleGAN**|Yanhua Zhao et.al|[paper](https://arxiv.org/abs/2601.08776)|-|-|\n", "ASA: Activation Steering for Tool-Calling Domain Adaptation": "|**2026-2-4**|**ASA: Activation Steering for Tool-Calling Domain Adaptation**|Youjin Wang et.al|[paper](https://arxiv.org/abs/2602.04935)|-|-|\n", "Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks": "|**2026-2-4**|**Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks**|Masaya Tsunokake et.al|[paper](https://arxiv.org/abs/2602.04466)|-|-|\n", "Universal Robust Speech Adaptation for Cross-Domain Speech Recognition and Enhancement": "|**2026-2-4**|**Universal Robust Speech Adaptation for Cross-Domain Speech Recognition and Enhancement**|Chien-Chun Wang et.al|[paper](https://arxiv.org/abs/2602.04307)|-|<details><summary>detail</summary>IEEE Transactions on Audio</details>|\n", "DADP: Domain Adaptive Diffusion Policy": "|**2026-2-3**|**DADP: Domain Adaptive Diffusion Policy**|Pengcheng Wang et.al|[paper](https://arxiv.org/abs/2602.04037)|[code](https://outsider86.github.io/DomainAdaptiveDiffusionPolicy/.)|-|\n", "CiMRAG: CiM-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs": "|**2026-2-3**|**CiMRAG: CiM-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs**|Shih-Hsuan Chiu et.al|[paper](https://arxiv.org/abs/2601.20041)|-|<details><summary>detail</summary>Accepted by ICASSP 2026</details>|\n", "DSKC: Domain Style Modeling with Adaptive Knowledge Consolidation for Exemplar-free Lifelong Person Re-Identification": "|**2026-2-2**|**DSKC: Domain Style Modeling with Adaptive Knowledge Consolidation for Exemplar-free Lifelong Person Re-Identification**|Shiben Liu et.al|[paper](https://arxiv.org/abs/2508.03516)|[code](https://github.com/LiuShiBen/DKUA.)|<details><summary>detail</summary>11 papges</details>|\n", "When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs": "|**2026-2-2**|**When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs**|Junyi Zou et.al|[paper](https://arxiv.org/abs/2601.18350)|-|-|\n", "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation": "|**2026-2-2**|**Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation**|Zihao Wang et.al|[paper](https://arxiv.org/abs/2601.18623)|-|<details><summary>detail</summary>Paper accepted as a conference paper at ICLR 2026</details>|\n", "Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation": "|**2026-2-1**|**Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation**|Michele Antonazzi et.al|[paper](https://arxiv.org/abs/2602.01389)|-|<details><summary>detail</summary>Accepted for publication at ICRA 2026</details>|\n", "Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective": "|**2026-2-1**|**Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective**|Zhichao Chen et.al|[paper](https://arxiv.org/abs/2602.01179)|-|-|\n"}, "domain generalization": {"Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains": "|**2026-2-5**|**Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains**|Ben Isselmann et.al|[paper](https://arxiv.org/abs/2602.05527)|-|<details><summary>detail</summary>AMEE Conference Proceeding 2025</details>|\n", "Domain Generalization Under Posterior Drift": "|**2026-2-4**|**Domain Generalization Under Posterior Drift**|Yilun Zhu et.al|[paper](https://arxiv.org/abs/2510.04441)|-|-|\n", "PEPR: Privileged Event-based Predictive Regularization for Domain Generalization": "|**2026-2-4**|**PEPR: Privileged Event-based Predictive Regularization for Domain Generalization**|Gabriele Magrini et.al|[paper](https://arxiv.org/abs/2602.04583)|-|-|\n", "Self-CriTeach: LLM Self-Teaching and Self-Critiquing for Improving Robotic Planning via Automated Domain Generation": "|**2026-2-3**|**Self-CriTeach: LLM Self-Teaching and Self-Critiquing for Improving Robotic Planning via Automated Domain Generation**|Jinbang Huang et.al|[paper](https://arxiv.org/abs/2509.21543)|-|-|\n", "CiMRAG: CiM-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs": "|**2026-2-3**|**CiMRAG: CiM-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs**|Shih-Hsuan Chiu et.al|[paper](https://arxiv.org/abs/2601.20041)|-|<details><summary>detail</summary>Accepted by ICASSP 2026</details>|\n", "Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain": "|**2026-2-3**|**Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain**|Wei Zhu et.al|[paper](https://arxiv.org/abs/2602.03368)|-|-|\n", "Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains": "|**2026-2-2**|**Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains**|Jae-Sung Bae et.al|[paper](https://arxiv.org/abs/2602.02841)|-|-|\n", "Robust Domain Generalization under Divergent Marginal and Conditional Distributions": "|**2026-2-2**|**Robust Domain Generalization under Divergent Marginal and Conditional Distributions**|Jewon Yeom et.al|[paper](https://arxiv.org/abs/2602.02015)|-|-|\n", "Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry": "|**2026-1-31**|**Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry**|Seunghyun Yoo et.al|[paper](https://arxiv.org/abs/2602.00547)|-|-|\n", "Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications": "|**2026-1-29**|**Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications**|Chenhua Shi et.al|[paper](https://arxiv.org/abs/2509.25736)|-|-|\n", "Decentralized Domain Generalization with Style Sharing: Formal Model and Convergence Analysis": "|**2026-1-29**|**Decentralized Domain Generalization with Style Sharing: Formal Model and Convergence Analysis**|Shahryar Zehtabi et.al|[paper](https://arxiv.org/abs/2504.06235)|-|-|\n", "Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains": "|**2026-1-29**|**Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains**|Meng Cao et.al|[paper](https://arxiv.org/abs/2601.21999)|[code](https://github.com/Alrash/NDCL.)|-|\n", "MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging": "|**2026-1-29**|**MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging**|Tianjun Wei et.al|[paper](https://arxiv.org/abs/2601.15930)|[code](https://github.com/Joinn99/MMGRid)|<details><summary>detail</summary>https://github</details>|\n", "Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge": "|**2026-1-28**|**Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge**|Runhao Zhao et.al|[paper](https://arxiv.org/abs/2601.10485)|-|-|\n", "SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs": "|**2026-1-28**|**SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs**|Jiacheng Lin et.al|[paper](https://arxiv.org/abs/2509.20758)|-|<details><summary>detail</summary>Accepted by ICLR 2026</details>|\n"}, "vision language": {"Can vision language models learn intuitive physics from interaction?": "|**2026-2-5**|**Can vision language models learn intuitive physics from interaction?**|Luca M. Schulze Buschoff et.al|[paper](https://arxiv.org/abs/2602.06033)|-|-|\n", "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models": "|**2026-2-5**|**Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models**|Wenxuan Huang et.al|[paper](https://arxiv.org/abs/2503.06749)|[code](https://github.com/Osilly/Vision-R1)|<details><summary>detail</summary>ICLR 2026</details>|\n", "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation": "|**2026-2-5**|**Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation**|Hai Zhang et.al|[paper](https://arxiv.org/abs/2602.05827)|-|-|\n", "TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?": "|**2026-2-5**|**TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?**|Yikun Zong et.al|[paper](https://arxiv.org/abs/2602.05570)|[code](https://anonymous.4open.science/r/TangramVLM-F582/.)|-|\n", "VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator": "|**2026-2-5**|**VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator**|Bessie Dominguez-Dager et.al|[paper](https://arxiv.org/abs/2602.05552)|-|-|\n", "Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification": "|**2026-2-5**|**Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification**|Tao Huang et.al|[paper](https://arxiv.org/abs/2602.05535)|[code](https://github.com/HT86159/EUQ.)|<details><summary>detail</summary>ICLR 2026</details>|\n", "Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models": "|**2026-2-5**|**Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models**|Basel Mousi et.al|[paper](https://arxiv.org/abs/2602.05437)|-|-|\n", "VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models": "|**2026-2-5**|**VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models**|Xinlei Yu et.al|[paper](https://arxiv.org/abs/2511.11007)|[code](https://github.com/YU-deep/VisMem.git.)|-|\n", "VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis": "|**2026-2-4**|**VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis**|Meng Chu et.al|[paper](https://arxiv.org/abs/2512.19243)|-|-|\n", "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning": "|**2026-2-4**|**MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning**|Haoyu Fu et.al|[paper](https://arxiv.org/abs/2512.13636)|[code](https://xiaomi-mlab.github.io/MindDrive/)|-|\n", "SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning": "|**2026-2-4**|**SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning**|Alejandra Perez et.al|[paper](https://arxiv.org/abs/2509.10555)|-|-|\n", "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models": "|**2026-2-4**|**VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models**|Yiye Chen et.al|[paper](https://arxiv.org/abs/2602.05049)|[code](https://vista-vla.github.io/)|<details><summary>detail</summary>In submission</details>|\n", "Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?": "|**2026-2-4**|**Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?**|Ruixin Yang et.al|[paper](https://arxiv.org/abs/2602.05023)|-|<details><summary>detail</summary>Accepted by ICLR 2026</details>|\n", "When LLaVA Meets Objects: Token Composition for Vision-Language-Models": "|**2026-2-4**|**When LLaVA Meets Objects: Token Composition for Vision-Language-Models**|Soumya Jahagirdar et.al|[paper](https://arxiv.org/abs/2602.04864)|-|-|\n", "VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?": "|**2026-2-4**|**VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?**|Qing'an Liu et.al|[paper](https://arxiv.org/abs/2602.04802)|[code](https://github.com/QingAnLiu/VISTA-Bench.)|-|\n"}}