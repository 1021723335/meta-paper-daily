{"source-free": {"Source-Free Domain Adaptation via Multi-view Contrastive Learning": "|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|\n", "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-6-26**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "SFDLA: Source-Free Document Layout Analysis": "|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|\n", "Unified Source-Free Domain Adaptation": "|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|\n", "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|\n", "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization": "|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|\n", "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data": "|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|\n", "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation": "|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|\n", "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation": "|**2025-5-30**|**Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation**|Prasanna Reddy Pulakurthi et.al|[paper](https://arxiv.org/abs/2505.24216)|[code](https://github.com/PrasannaPulakurthi/SPM)|-|\n", "Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation": "|**2025-5-27**|**Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation**|Peihua Deng et.al|[paper](https://arxiv.org/abs/2411.16064)|[code](https://github.com/dengpeihua/GROTO.)|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Training-Free Multi-Step Audio Source Separation": "|**2025-5-26**|**Training-Free Multi-Step Audio Source Separation**|Yongyi Zang et.al|[paper](https://arxiv.org/abs/2505.19534)|-|-|\n", "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation": "|**2025-5-23**|**Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation**|Peiliang Gong et.al|[paper](https://arxiv.org/abs/2505.21525)|-|-|\n", "Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing": "|**2025-5-20**|**Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing**|Yang Xiao et.al|[paper](https://arxiv.org/abs/2505.14601)|-|<details><summary>detail</summary>Accepted by Interspeech 2025</details>|\n"}, "object detection": {"DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising": "|**2025-7-9**|**DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising**|Sven Teufel et.al|[paper](https://arxiv.org/abs/2507.06976)|-|-|\n", "PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection": "|**2025-7-9**|**PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection**|Xiao Li et.al|[paper](https://arxiv.org/abs/2506.23581)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "From Blurry to Brilliant Detection: YOLO-Based Aerial Object Detection with Super Resolution": "|**2025-7-9**|**From Blurry to Brilliant Detection: YOLO-Based Aerial Object Detection with Super Resolution**|Ragib Amin Nihal et.al|[paper](https://arxiv.org/abs/2401.14661)|-|-|\n", "StixelNExT: Toward Monocular Low-Weight Perception for Object Segmentation and Free Space Detection": "|**2025-7-9**|**StixelNExT: Toward Monocular Low-Weight Perception for Object Segmentation and Free Space Detection**|Marcel Vosshans et.al|[paper](https://arxiv.org/abs/2407.08277)|-|<details><summary>detail</summary>Accepted Conference Paper</details>|\n", "Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection": "|**2025-7-8**|**Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection**|Yupeng Hu et.al|[paper](https://arxiv.org/abs/2507.06510)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "CFMW: Cross-modality Fusion Mamba for Robust Object Detection under Adverse Weather": "|**2025-7-8**|**CFMW: Cross-modality Fusion Mamba for Robust Object Detection under Adverse Weather**|Haoyuan Li et.al|[paper](https://arxiv.org/abs/2404.16302)|[code](https://github.com/lhy-zjut/CFMW.)|<details><summary>detail</summary>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</details>|\n", "Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes": "|**2025-7-7**|**Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes**|Zhangjun Zhou et.al|[paper](https://arxiv.org/abs/2412.10943)|[code](https://github.com/ssecv/USCNet.)|-|\n", "CVFusion: Cross-View Fusion of 4D Radar and Camera for 3D Object Detection": "|**2025-7-6**|**CVFusion: Cross-View Fusion of 4D Radar and Camera for 3D Object Detection**|Hanzhi Zhong et.al|[paper](https://arxiv.org/abs/2507.04587)|-|-|\n", "MambaFusion: Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection": "|**2025-7-6**|**MambaFusion: Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection**|Hanshi Wang et.al|[paper](https://arxiv.org/abs/2507.04369)|-|-|\n", "DMAT: An End-to-End Framework for Joint Atmospheric Turbulence Mitigation and Object Detection": "|**2025-7-6**|**DMAT: An End-to-End Framework for Joint Atmospheric Turbulence Mitigation and Object Detection**|Paul Hill et.al|[paper](https://arxiv.org/abs/2507.04323)|-|-|\n", "Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge": "|**2025-7-5**|**Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge**|Linshen Liu et.al|[paper](https://arxiv.org/abs/2507.04123)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds": "|**2025-7-4**|**Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds**|Yanze Jiang et.al|[paper](https://arxiv.org/abs/2504.09506)|-|-|\n", "Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs": "|**2025-7-4**|**Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs**|Liwei Che et.al|[paper](https://arxiv.org/abs/2503.07772)|-|<details><summary>detail</summary>ICCV2025</details>|\n", "Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection": "|**2025-7-4**|**Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection**|Jicheng Yuan et.al|[paper](https://arxiv.org/abs/2404.01988)|[code](https://github.com/jichengyuan/Cooperitive_Students.)|<details><summary>detail</summary>Code is available at https://github</details>|\n", "2.5D Object Detection for Intelligent Roadside Infrastructure": "|**2025-7-4**|**2.5D Object Detection for Intelligent Roadside Infrastructure**|Nikolai Polley et.al|[paper](https://arxiv.org/abs/2507.03564)|[code](https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection)|<details><summary>detail</summary>2025 IEEE 28th International Conference on Intelligent Transportation Systems (ITSC)</details>|\n"}, "domain adaptation": {"Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications": "|**2025-7-9**|**Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications**|Seonwu Kim et.al|[paper](https://arxiv.org/abs/2507.06795)|-|<details><summary>detail</summary>under review</details>|\n", "On the Hardness of Unsupervised Domain Adaptation: Optimal Learners and Information-Theoretic Perspective": "|**2025-7-9**|**On the Hardness of Unsupervised Domain Adaptation: Optimal Learners and Information-Theoretic Perspective**|Zhiyi Dong et.al|[paper](https://arxiv.org/abs/2507.06552)|-|<details><summary>detail</summary>the 4th Conference on Lifelong Learning Agents (CoLLAs 2025)</details>|\n", "Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer Detection": "|**2025-7-8**|**Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer Detection**|Emerson P. Grabke et.al|[paper](https://arxiv.org/abs/2507.06384)|[code](https://github.com/grabkeem/CCELLA-plus-plus)|<details><summary>detail</summary>BT and MAH are co-senior authors on the work</details>|\n", "Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation": "|**2025-7-8**|**Safe Domain Randomization via Uncertainty-Aware Out-of-Distribution Detection and Policy Adaptation**|Mohamad H. Danesh et.al|[paper](https://arxiv.org/abs/2507.06111)|-|-|\n", "Optimal Transport for Domain Adaptation through Gaussian Mixture Models": "|**2025-7-8**|**Optimal Transport for Domain Adaptation through Gaussian Mixture Models**|Eduardo Fernandes Montesuma et.al|[paper](https://arxiv.org/abs/2403.13847)|[code](https://github.com/eddardd/gmm-otda/)|-|\n", "Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters": "|**2025-7-8**|**Improving Robustness of Foundation Models in Domain Adaptation with Soup-Adapters**|Marco Roschkowski et.al|[paper](https://arxiv.org/abs/2507.05807)|-|-|\n", "Domain adaptation of large language models for geotechnical applications": "|**2025-7-7**|**Domain adaptation of large language models for geotechnical applications**|Lei Fan et.al|[paper](https://arxiv.org/abs/2507.05613)|-|-|\n", "Domain Adaptation of VLM for Soccer Video Understanding": "|**2025-7-7**|**Domain Adaptation of VLM for Soccer Video Understanding**|Tiancheng Jiang et.al|[paper](https://arxiv.org/abs/2505.13860)|-|-|\n", "Tackling Dimensional Collapse toward Comprehensive Universal Domain Adaptation": "|**2025-7-6**|**Tackling Dimensional Collapse toward Comprehensive Universal Domain Adaptation**|Hung-Chieh Fang et.al|[paper](https://arxiv.org/abs/2410.11271)|[code](https://dc-unida.github.io/)|-|\n", "Domain Adaptation of Drag Reduction Policy to Partial Measurements": "|**2025-7-6**|**Domain Adaptation of Drag Reduction Policy to Partial Measurements**|Anton Plaksin et.al|[paper](https://arxiv.org/abs/2507.04309)|-|<details><summary>detail</summary>Journal ref:Machine Learning and the Physical Sciences Workshop</details>|\n", "Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering": "|**2025-7-5**|**Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering**|Ting-Wen Ko et.al|[paper](https://arxiv.org/abs/2507.04069)|-|-|\n", "Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection": "|**2025-7-4**|**Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection**|Jicheng Yuan et.al|[paper](https://arxiv.org/abs/2404.01988)|[code](https://github.com/jichengyuan/Cooperitive_Students.)|<details><summary>detail</summary>Code is available at https://github</details>|\n", "Source-Free Domain Adaptation via Multi-view Contrastive Learning": "|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|\n", "Global Variational Inference Enhanced Robust Domain Adaptation": "|**2025-7-4**|**Global Variational Inference Enhanced Robust Domain Adaptation**|Lingkun Luo et.al|[paper](https://arxiv.org/abs/2507.03291)|-|-|\n", "MORDA: A Synthetic Dataset to Facilitate Adaptation of Object Detectors to Unseen Real-target Domain While Preserving Performance on Real-source Domain": "|**2025-7-4**|**MORDA: A Synthetic Dataset to Facilitate Adaptation of Object Detectors to Unseen Real-target Domain While Preserving Performance on Real-source Domain**|Hojun Lim et.al|[paper](https://arxiv.org/abs/2501.04950)|-|-|\n", "Confidence sharing adaptation for out-of-domain human pose and shape estimation": "|**2025-7-8**|**Confidence sharing adaptation for out-of-domain human pose and shape estimation**|T Yue et.al|[paper](https://www.sciencedirect.com/science/article/pii/S1077314224001322)|-|<details><summary>detail</summary>Computer Vision and Image\u00a0\u2026, 2024 Elsevier</details>|\n"}, "domain generalization": {"Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis": "|**2025-7-9**|**Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis**|Srihari K B et.al|[paper](https://arxiv.org/abs/2507.06571)|-|-|\n", "Fair Domain Generalization: An Information-Theoretic View": "|**2025-7-8**|**Fair Domain Generalization: An Information-Theoretic View**|Tangzheng Lian et.al|[paper](https://arxiv.org/abs/2507.05823)|-|-|\n", "AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework": "|**2025-7-7**|**AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework**|Suoxiang Zhang et.al|[paper](https://arxiv.org/abs/2507.05621)|-|-|\n", "LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains": "|**2025-7-7**|**LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains**|Nicholas Chivaran et.al|[paper](https://arxiv.org/abs/2507.05162)|[code](https://github.com/nchivar/LAID.)|<details><summary>detail</summary>To appear in the proceedings of PST2025</details>|\n", "Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization": "|**2025-7-6**|**Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization**|Zuyu Zhang et.al|[paper](https://arxiv.org/abs/2507.04302)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning": "|**2025-7-5**|**CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning**|Jiacheng Shi et.al|[paper](https://arxiv.org/abs/2507.04048)|-|<details><summary>detail</summary>Interspeech2025</details>|\n", "Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations": "|**2025-7-4**|**Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations**|Hai Huang et.al|[paper](https://arxiv.org/abs/2507.03304)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "Set Valued Predictions For Robust Domain Generalization": "|**2025-7-3**|**Set Valued Predictions For Robust Domain Generalization**|Ron Tsibulsky et.al|[paper](https://arxiv.org/abs/2507.03146)|-|<details><summary>detail</summary>ICML 2025</details>|\n", "Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation": "|**2025-7-3**|**Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2504.12753)|[code](https://github.com/anonymouse-xzrptkvyqc/DepthForge.)|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization": "|**2025-7-2**|**Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization**|De Cheng et.al|[paper](https://arxiv.org/abs/2507.02288)|-|-|\n", "Generalized Adaptive Transfer Network: Enhancing Transfer Learning in Reinforcement Learning Across Domains": "|**2025-7-2**|**Generalized Adaptive Transfer Network: Enhancing Transfer Learning in Reinforcement Learning Across Domains**|Abhishek Verma et.al|[paper](https://arxiv.org/abs/2507.03026)|-|-|\n", "NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation": "|**2025-7-2**|**NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation**|Zhenye Lou et.al|[paper](https://arxiv.org/abs/2408.11787)|[code](https://github.com/xq141839/NuSegDG.)|-|\n", "DGSAM: Domain Generalization via Individual Sharpness-Aware Minimization": "|**2025-6-30**|**DGSAM: Domain Generalization via Individual Sharpness-Aware Minimization**|Youngjun Song et.al|[paper](https://arxiv.org/abs/2503.23430)|-|-|\n", "Calculation of Photocarrier Generation from Optical Absorption for Time-domain Simulation of Optoelectronic Devices": "|**2025-6-30**|**Calculation of Photocarrier Generation from Optical Absorption for Time-domain Simulation of Optoelectronic Devices**|Liang Chen et.al|[paper](https://arxiv.org/abs/2102.06702)|-|-|\n", "Generalizing vision-language models to novel domains: A comprehensive survey": "|**2025-6-30**|**Generalizing vision-language models to novel domains: A comprehensive survey**|Xinyao Li et.al|[paper](https://arxiv.org/abs/2506.18504)|-|-|\n", "Entity-centric multi-domain transformer for improving generalization in fake news detection": "|**2025-7-8**|**Entity-centric multi-domain transformer for improving generalization in fake news detection**|P Bazmi et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0306457324001663)|-|<details><summary>detail</summary>Information Processing &\u00a0\u2026, 2024 Elsevier</details>|\n", "Fine-Grained Domain Generalization with Feature Structuralization": "|**2025-7-7**|**Fine-Grained Domain Generalization with Feature Structuralization**|W Yu et.al|[paper](https://arxiv.org/abs/2406.09166)|[code](https://github.com/yuwlong666/fsdg)|-|\n", "Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification": "|**2025-7-5**|**Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification**|S Addepalli et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Addepalli_Leveraging_Vision-Language_Models_for_Improving_Domain_Generalization_in_Image_Classification_CVPR_2024_paper.html)|[code](https://github.com/val-iisc/VL2V-ADiP)|<details><summary>detail</summary>Proceedings of the\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "Disentangled Prompt Representation for Domain Generalization": "|**2025-7-5**|**Disentangled Prompt Representation for Domain Generalization**|D Cheng et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Cheng_Disentangled_Prompt_Representation_for_Domain_Generalization_CVPR_2024_paper.html)|[code](https://github.com/henry123-boy/SpaTracker)|<details><summary>detail</summary>Proceedings of the\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "Supplementary Materials: Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential in Open Domain Generalization": "|**2025-7-5**|**Supplementary Materials: Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential in Open Domain Generalization**|M Singha et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Singha_Unknown_Prompt_the_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>openaccess.thecvf.com</details>|\n", "Supplementary Material for DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning": "|**2025-7-5**|**Supplementary Material for DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning**|B Dataset - openaccess.thecvf.com et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Bai_DiPrompT_Disentangled_Prompt_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>openaccess.thecvf.com</details>|\n", "Supplementary Material for Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization": "|**2025-7-5**|**Supplementary Material for Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization**|K Le et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Le_Efficiently_Assemble_Normalization_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>Phuoc, KS Wong openaccess.thecvf.com</details>|\n", "Domain Generalization for Crop Segmentation with Standardized Ensemble Knowledge Distillation": "|**2025-7-5**|**Domain Generalization for Crop Segmentation with Standardized Ensemble Knowledge Distillation**|S Angarano et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024W/Vision4Ag/html/Angarano_Domain_Generalization_for_Crop_Segmentation_with_Standardized_Ensemble_Knowledge_Distillation_CVPRW_2024_paper.html)|[code](https://github.com/pic4ser/agriseg)|<details><summary>detail</summary>Proceedings of the\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "MixStyle-Based Contrastive Test-Time Adaptation: Pathway to Domain Generalization": "|**2025-7-5**|**MixStyle-Based Contrastive Test-Time Adaptation: Pathway to Domain Generalization**|K Yamashita et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024W/MAT/html/Yamashita_MixStyle-Based_Contrastive_Test-Time_Adaptation_Pathway_to_Domain_Generalization_CVPRW_2024_paper.html)|-|<details><summary>detail</summary>\u2026\u00a0of the IEEE/CVF Conference on\u00a0\u2026, 2024 openaccess.thecvf.com</details>|\n", "Fault vibration model driven fault-aware domain generalization framework for bearing fault diagnosis": "|**2025-7-5**|**Fault vibration model driven fault-aware domain generalization framework for bearing fault diagnosis**|B Pang et.al|[paper](https://www.sciencedirect.com/science/article/pii/S1474034624002684)|-|<details><summary>detail</summary>Advanced Engineering\u00a0\u2026, 2024 Elsevier</details>|\n"}, "vision language": {"Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models": "|**2025-7-9**|**Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models**|Tiezheng Zhang et.al|[paper](https://arxiv.org/abs/2507.07104)|-|-|\n", "Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu": "|**2025-7-9**|**Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu**|Yan Hon Michael Chung et.al|[paper](https://arxiv.org/abs/2507.06761)|[code](https://github.com/mic7ch1/ManchuAI-OCR.)|-|\n", "DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction": "|**2025-7-9**|**DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction**|Zhiyi Hou et.al|[paper](https://arxiv.org/abs/2507.02948)|[code](https://github.com/hzy138/DriveMRP)|-|\n", "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments": "|**2025-7-9**|**SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments**|Tianshun Li et.al|[paper](https://arxiv.org/abs/2507.06564)|-|-|\n", "Integrated Structural Prompt Learning for Vision-Language Models": "|**2025-7-9**|**Integrated Structural Prompt Learning for Vision-Language Models**|Jiahui Wang et.al|[paper](https://arxiv.org/abs/2507.05677)|-|-|\n", "Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection": "|**2025-7-8**|**Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection**|Yupeng Hu et.al|[paper](https://arxiv.org/abs/2507.06510)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "Refining Skewed Perceptions in Vision-Language Contrastive Models through Visual Representations": "|**2025-7-8**|**Refining Skewed Perceptions in Vision-Language Contrastive Models through Visual Representations**|Haocheng Dai et.al|[paper](https://arxiv.org/abs/2405.14030)|-|-|\n", "3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds": "|**2025-7-8**|**3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds**|Fan-Yun Sun et.al|[paper](https://arxiv.org/abs/2507.06484)|[code](https://ai.stanford.edu/)|<details><summary>detail</summary>project website: https://ai</details>|\n", "VisioPath: Vision-Language Enhanced Model Predictive Control for Safe Autonomous Navigation in Mixed Traffic": "|**2025-7-8**|**VisioPath: Vision-Language Enhanced Model Predictive Control for Safe Autonomous Navigation in Mixed Traffic**|Shanting Wang et.al|[paper](https://arxiv.org/abs/2507.06441)|-|-|\n", "Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs": "|**2025-7-8**|**Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs**|Selim Kuzucu et.al|[paper](https://arxiv.org/abs/2507.00754)|-|-|\n", "On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving": "|**2025-7-8**|**On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving**|Pedram MohajerAnsari et.al|[paper](https://arxiv.org/abs/2506.11472)|-|-|\n", "LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models": "|**2025-7-8**|**LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models**|Zhihao Chen et.al|[paper](https://arxiv.org/abs/2507.06140)|[code](https://github.com/hao1635/LangMamba.)|-|\n", "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation": "|**2025-7-8**|**Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation**|Andr\u00e9 Schakkal et.al|[paper](https://arxiv.org/abs/2506.22827)|[code](https://vlp-humanoid.github.io/)|<details><summary>detail</summary>the RSS 2025 Workshop on Robot Planning in the Era of Foundation Models</details>|\n", "FA: Forced Prompt Learning of Vision-Language Models for Out-of-Distribution Detection": "|**2025-7-8**|**FA: Forced Prompt Learning of Vision-Language Models for Out-of-Distribution Detection**|Xinhua Lu et.al|[paper](https://arxiv.org/abs/2507.04511)|[code](https://github.com/0xFAFA/FA.)|-|\n", "GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing": "|**2025-7-8**|**GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing**|Xianzhi Ma et.al|[paper](https://arxiv.org/abs/2507.05887)|-|-|\n", "Towards Vision-Language Geo-Foundation Model: A Survey": "|**2025-7-8**|**Towards Vision-Language Geo-Foundation Model: A Survey**|Y Zhou et.al|[paper](https://www.researchgate.net/profile/Yue-Zhou-139/publication/381403816_Towards_Vision-Language_Geo-Foundation_Model_A_Survey/links/666ba71ea54c5f0b9464c544/Towards-Vision-Language-Geo-Foundation-Model-A-Survey.pdf)|[code](https://github.com/zytx121/awesome-vlgfm)|<details><summary>detail</summary>researchgate.net</details>|\n", "VLind-Bench: Measuring Language Priors in Large Vision-Language Models": "|**2025-7-7**|**VLind-Bench: Measuring Language Priors in Large Vision-Language Models**|K Lee et.al|[paper](https://arxiv.org/abs/2406.08702)|[code](https://github.com/klee972/vlind-bench)|-|\n", "How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models": "|**2025-7-7**|**How structured are the representations in transformer-based vision encoders? An analysis of multi-object representations in vision-language models**|T Khajuria et.al|[paper](https://arxiv.org/abs/2406.09067)|-|-|\n", "AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models": "|**2025-7-7**|**AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models**|Y Wu et.al|[paper](https://arxiv.org/abs/2406.09295)|[code](https://paperswithcode.com/paper/alignmmbench-evaluating-chinese-multimodal)|-|\n", "MirrorCheck: Efficient Adversarial Defense for Vision-Language Models": "|**2025-7-7**|**MirrorCheck: Efficient Adversarial Defense for Vision-Language Models**|S Fares et.al|[paper](https://arxiv.org/abs/2406.09250)|[code](https://paperswithcode.com/paper/mirrorcheck-efficient-adversarial-defense-for)|-|\n"}}