{"source-free": {"Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection": "|**2026-1-20**|**Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection**|Huizai Yao et.al|[paper](https://arxiv.org/abs/2511.07301)|-|<details><summary>detail</summary>AAAI 2026</details>|\n", "Towards Unbiased Source-Free Object Detection via Vision Foundation Models": "|**2026-1-19**|**Towards Unbiased Source-Free Object Detection via Vision Foundation Models**|Zhi Cai et.al|[paper](https://arxiv.org/abs/2601.12765)|-|-|\n", "Unified Source-Free Domain Adaptation": "|**2026-1-18**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|[code](https://github.com/tntek/CausalDA.)|-|\n", "GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling": "|**2026-1-16**|**GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2601.11161)|[code](https://github.com/pascalschlachter/GMM-COMET.)|-|\n", "SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling": "|**2026-1-13**|**SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling**|Xi Chen et.al|[paper](https://arxiv.org/abs/2601.08608)|[code](https://github.com/chenxi52/SfMamba.)|-|\n", "Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation": "|**2026-1-13**|**Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation**|Yuan Gao et.al|[paper](https://arxiv.org/abs/2601.08375)|-|-|\n", "Empowering Source-Free Domain Adaptation via MLLM-Guided Reliability-Based Curriculum Learning": "|**2026-1-5**|**Empowering Source-Free Domain Adaptation via MLLM-Guided Reliability-Based Curriculum Learning**|Dongjie Chen et.al|[paper](https://arxiv.org/abs/2405.18376)|[code](https://github.com/Dong-Jie-Chen/RCL.)|-|\n", "Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results": "|**2025-12-28**|**Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results**|Rohan Palanikumar et.al|[paper](https://arxiv.org/abs/2511.00752)|-|-|\n", "Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection": "|**2025-12-24**|**Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection**|Sairam VCR et.al|[paper](https://arxiv.org/abs/2512.17514)|-|-|\n", "Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario": "|**2025-12-18**|**Exploiting Radio Frequency Fingerprints for Device Identification: Tackling Cross-receiver Challenges in the Source-data-free Scenario**|Liu Yang et.al|[paper](https://arxiv.org/abs/2512.16648)|-|<details><summary>detail</summary>IEEE Transactions on Mobile Computing</details>|\n", "VocSim: A Training-free Benchmark for Zero-shot Content Identity in Single-source Audio": "|**2025-12-10**|**VocSim: A Training-free Benchmark for Zero-shot Content Identity in Single-source Audio**|Maris Basha et.al|[paper](https://arxiv.org/abs/2512.10120)|-|-|\n", "FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation": "|**2025-12-7**|**FedSCAl: Leveraging Server and Client Alignment for Unsupervised Federated Source-Free Domain Adaptation**|M Yashwanth et.al|[paper](https://arxiv.org/abs/2512.06738)|-|<details><summary>detail</summary>Winter Conference on Applications of Computer Vision (WACV) 2026</details>|\n", "Source-free Video Domain Adaptation by Learning from Noisy Labels": "|**2025-11-28**|**Source-free Video Domain Adaptation by Learning from Noisy Labels**|Avijit Dasgupta et.al|[paper](https://arxiv.org/abs/2311.18572)|[code](https://avijit9.github.io/CleanAdapt.)|<details><summary>detail</summary>Our extended ICVGIP paper is now accepted in Pattern Recognition</details>|\n", "Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation": "|**2025-11-24**|**Collaborative Learning with Multiple Foundation Models for Source-Free Domain Adaptation**|Huisoo Lee et.al|[paper](https://arxiv.org/abs/2511.19147)|-|-|\n", "Unsupervised and Source-Free Ranking of Biomedical Segmentation Models": "|**2025-11-24**|**Unsupervised and Source-Free Ranking of Biomedical Segmentation Models**|Joshua Talks et.al|[paper](https://arxiv.org/abs/2503.00450)|-|-|\n"}, "object detection": {"YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection": "|**2026-1-22**|**YOLO Meets Mixture-of-Experts: Adaptive Expert Routing for Robust Object Detection**|Ori Meiraz et.al|[paper](https://arxiv.org/abs/2511.13344)|-|<details><summary>detail</summary>1 figure</details>|\n", "Performance-guided Reinforced Active Learning for Object Detection": "|**2026-1-22**|**Performance-guided Reinforced Active Learning for Object Detection**|Zhixuan Liang et.al|[paper](https://arxiv.org/abs/2601.15688)|-|<details><summary>detail</summary>Accepted by ICASSP 2026</details>|\n", "Real-Time Object Detection Meets DINOv3": "|**2026-1-21**|**Real-Time Object Detection Meets DINOv3**|Shihua Huang et.al|[paper](https://arxiv.org/abs/2509.20787)|[code](https://github.com/Intellindust-AI-Lab/DEIMv2)|<details><summary>detail</summary>Source code available at https://github</details>|\n", "Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing for Weakly-Supervised Camouflaged Object Detection with Scribble Annotations": "|**2026-1-21**|**Debate-Enhanced Pseudo Labeling and Frequency-Aware Progressive Debiasing for Weakly-Supervised Camouflaged Object Detection with Scribble Annotations**|Jiawei Ge et.al|[paper](https://arxiv.org/abs/2512.20260)|-|-|\n", "M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention": "|**2026-1-21**|**M2I2HA: A Multi-modal Object Detection Method Based on Intra- and Inter-Modal Hypergraph Attention**|Xiaofan Yang et.al|[paper](https://arxiv.org/abs/2601.14776)|-|-|\n", "A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection": "|**2026-1-21**|**A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection**|Guiying Zhu et.al|[paper](https://arxiv.org/abs/2601.11910)|-|-|\n", "A comprehensive overview of deep learning models for object detection from videos/images": "|**2026-1-21**|**A comprehensive overview of deep learning models for object detection from videos/images**|Sukana Zulfqar et.al|[paper](https://arxiv.org/abs/2601.14677)|-|<details><summary>detail</summary>N/A</details>|\n", "Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection": "|**2026-1-20**|**Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection**|Huizai Yao et.al|[paper](https://arxiv.org/abs/2511.07301)|-|<details><summary>detail</summary>AAAI 2026</details>|\n", "DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging": "|**2026-1-20**|**DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging**|Adrien Meyer et.al|[paper](https://arxiv.org/abs/2601.13954)|-|-|\n", "Leveraging Transformer Decoder for Automotive Radar Object Detection": "|**2026-1-19**|**Leveraging Transformer Decoder for Automotive Radar Object Detection**|Changxu Zhang et.al|[paper](https://arxiv.org/abs/2601.13386)|-|-|\n", "Practical Insights into Semi-Supervised Object Detection Approaches": "|**2026-1-19**|**Practical Insights into Semi-Supervised Object Detection Approaches**|Chaoxin Wang et.al|[paper](https://arxiv.org/abs/2601.13380)|-|-|\n", "Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?": "|**2026-1-19**|**Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?**|Srikanth Muralidharan et.al|[paper](https://arxiv.org/abs/2508.02927)|-|<details><summary>detail</summary>WACV 2026</details>|\n", "AsyncBEV: Cross-modal Flow Alignment in Asynchronous 3D Object Detection": "|**2026-1-19**|**AsyncBEV: Cross-modal Flow Alignment in Asynchronous 3D Object Detection**|Shiming Wang et.al|[paper](https://arxiv.org/abs/2601.12994)|-|-|\n", "YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection": "|**2026-1-19**|**YOLO26: An Analysis of NMS-Free End to End Framework for Real-Time Object Detection**|Sudip Chakrabarty et.al|[paper](https://arxiv.org/abs/2601.12882)|-|-|\n", "Towards Unbiased Source-Free Object Detection via Vision Foundation Models": "|**2026-1-19**|**Towards Unbiased Source-Free Object Detection via Vision Foundation Models**|Zhi Cai et.al|[paper](https://arxiv.org/abs/2601.12765)|-|-|\n"}, "domain adaptation": {"Multi-View Projection for Unsupervised Domain Adaptation in 3D Semantic Segmentation": "|**2026-1-22**|**Multi-View Projection for Unsupervised Domain Adaptation in 3D Semantic Segmentation**|Andrew Caunes et.al|[paper](https://arxiv.org/abs/2505.15545)|-|-|\n", "EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning": "|**2026-1-21**|**EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning**|Songlin Zhao et.al|[paper](https://arxiv.org/abs/2511.19935)|-|-|\n", "BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation": "|**2026-1-21**|**BEST-RQ-Based Self-Supervised Learning for Whisper Domain Adaptation**|Rapha\u00ebl Bagat et.al|[paper](https://arxiv.org/abs/2510.24570)|-|<details><summary>detail</summary>ICASSP 2026</details>|\n", "Transfer Learning from One Cancer to Another via Deep Learning Domain Adaptation": "|**2026-1-21**|**Transfer Learning from One Cancer to Another via Deep Learning Domain Adaptation**|Justin Cheung et.al|[paper](https://arxiv.org/abs/2601.14678)|-|-|\n", "Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation": "|**2026-1-20**|**Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation**|Justin Cheung et.al|[paper](https://arxiv.org/abs/2510.06584)|-|-|\n", "XD-MAP: Cross-Modal Domain Adaptation using Semantic Parametric Mapping": "|**2026-1-20**|**XD-MAP: Cross-Modal Domain Adaptation using Semantic Parametric Mapping**|Frank Bieder et.al|[paper](https://arxiv.org/abs/2601.14477)|-|-|\n", "Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law": "|**2026-1-20**|**Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law**|Ali Hamza Bashir et.al|[paper](https://arxiv.org/abs/2601.14160)|-|-|\n", "Back2Color: Domain-Adaptive Synthetic-to-Real Monocular Depth Estimation for Dynamic Traffic Scenes": "|**2026-1-20**|**Back2Color: Domain-Adaptive Synthetic-to-Real Monocular Depth Estimation for Dynamic Traffic Scenes**|Yufan Zhu et.al|[paper](https://arxiv.org/abs/2406.07741)|-|-|\n", "More Than Efficiency: Embedding Compression Improves Domain Adaptation in Dense Retrieval": "|**2026-1-19**|**More Than Efficiency: Embedding Compression Improves Domain Adaptation in Dense Retrieval**|Chunsheng Zuo et.al|[paper](https://arxiv.org/abs/2601.13525)|-|-|\n", "Unified Source-Free Domain Adaptation": "|**2026-1-18**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|[code](https://github.com/tntek/CausalDA.)|-|\n", "MMedExpert-R1: Strengthening Multimodal Medical Reasoning via Domain-Specific Adaptation and Clinical Guideline Reinforcement": "|**2026-1-18**|**MMedExpert-R1: Strengthening Multimodal Medical Reasoning via Domain-Specific Adaptation and Clinical Guideline Reinforcement**|Meidan Ding et.al|[paper](https://arxiv.org/abs/2601.10949)|-|-|\n", "Fine-Tuning Cycle-GAN for Domain Adaptation of MRI Images": "|**2026-1-18**|**Fine-Tuning Cycle-GAN for Domain Adaptation of MRI Images**|Mohd Usama et.al|[paper](https://arxiv.org/abs/2601.12512)|-|-|\n", "Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation": "|**2026-1-18**|**Histopath-C: Towards Realistic Domain Shifts for Histopathology Vision-Language Adaptation**|Mehrdad Noori et.al|[paper](https://arxiv.org/abs/2601.12493)|[code](https://github.com/Mehrdad-Noori/Histopath-C.)|<details><summary>detail</summary>WACV 2026</details>|\n", "SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics": "|**2026-1-17**|**SolarGPT-QA: A Domain-Adaptive Large Language Model for Educational Question Answering in Space Weather and Heliophysics**|Santosh Chapagain et.al|[paper](https://arxiv.org/abs/2601.12131)|-|<details><summary>detail</summary>This is preliminary work towards a broader SolarGPT framework</details>|\n", "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training": "|**2026-1-16**|**Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training**|Shuo Cheng et.al|[paper](https://arxiv.org/abs/2509.18631)|[code](https://ot-sim2real.github.io/.)|<details><summary>detail</summary>NeurIPS 2025</details>|\n"}, "domain generalization": {"MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging": "|**2026-1-22**|**MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging**|Tianjun Wei et.al|[paper](https://arxiv.org/abs/2601.15930)|[code](https://github.com/Joinn99/MMGRid)|<details><summary>detail</summary>https://github</details>|\n", "Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition": "|**2026-1-21**|**Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition**|Weiwei Wu et.al|[paper](https://arxiv.org/abs/2601.15615)|[code](https://github.com/RyanLi-X/RSM-CoDG.)|-|\n", "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR": "|**2026-1-21**|**From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR**|Juan Castorena et.al|[paper](https://arxiv.org/abs/2509.16346)|-|-|\n", "Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases": "|**2026-1-21**|**Reliability by design: quantifying and eliminating fabrication risk in LLMs. From generative to consultative AI: a comparative analysis in the legal domain and lessons for high-stakes knowledge bases**|Alex Dantart et.al|[paper](https://arxiv.org/abs/2601.15476)|-|-|\n", "Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization": "|**2026-1-21**|**Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization**|Hanyu Li et.al|[paper](https://arxiv.org/abs/2601.06052)|-|-|\n", "Log anomaly detection via Meta Learning and Prototypical Networks for Cross domain generalization": "|**2026-1-20**|**Log anomaly detection via Meta Learning and Prototypical Networks for Cross domain generalization**|Krishna Sharma et.al|[paper](https://arxiv.org/abs/2601.14336)|-|-|\n", "From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs": "|**2026-1-19**|**From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs**|Yingjian Chen et.al|[paper](https://arxiv.org/abs/2601.03597)|-|-|\n", "PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain": "|**2026-1-19**|**PhaseMark: A Post-hoc, Optimization-Free Watermarking of AI-generated Images in the Latent Frequency Domain**|Sung Ju Lee et.al|[paper](https://arxiv.org/abs/2601.13128)|-|<details><summary>detail</summary>the IEEE International Conference on Acoustics</details>|\n", "GLAP: General contrastive audio-text pretraining across domains and languages": "|**2026-1-18**|**GLAP: General contrastive audio-text pretraining across domains and languages**|Heinrich Dinkel et.al|[paper](https://arxiv.org/abs/2506.11350)|[code](https://github.com/xiaomi-research/dasheng-glap.)|<details><summary>detail</summary>ICASSP 2026</details>|\n", "Federated Joint Learning for Domain and Class Generalization": "|**2026-1-17**|**Federated Joint Learning for Domain and Class Generalization**|Haoran Xu et.al|[paper](https://arxiv.org/abs/2601.12253)|-|<details><summary>detail</summary>ICASSP 2026</details>|\n", "SemAlign: Language Guided Semi-supervised Domain Generalization": "|**2026-1-16**|**SemAlign: Language Guided Semi-supervised Domain Generalization**|Muditha Fernando et.al|[paper](https://arxiv.org/abs/2601.11724)|-|-|\n", "A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints": "|**2026-1-16**|**A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints**|Youssef Tawfilis et.al|[paper](https://arxiv.org/abs/2507.12979)|[code](https://distributed-gen-ai.github.io/huscf-gan.github.io/.)|<details><summary>detail</summary>Accepted and published in Transactions on Machine Learning Research (TMLR)</details>|\n", "Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings": "|**2026-1-16**|**Learn Before Represent: Bridging Generative and Contrastive Learning for Domain-Specific LLM Embeddings**|Xiaoyu Liang et.al|[paper](https://arxiv.org/abs/2601.11124)|-|-|\n", "Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge": "|**2026-1-15**|**Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge**|Runhao Zhao et.al|[paper](https://arxiv.org/abs/2601.10485)|-|-|\n", "Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization": "|**2026-1-15**|**Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization**|Xiaohan Wang et.al|[paper](https://arxiv.org/abs/2511.20258)|-|-|\n"}, "vision language": {"GutenOCR: A Grounded Vision-Language Front-End for Documents": "|**2026-1-22**|**GutenOCR: A Grounded Vision-Language Front-End for Documents**|Hunter Heidenreich et.al|[paper](https://arxiv.org/abs/2601.14490)|-|-|\n", "BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries": "|**2026-1-22**|**BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries**|Shijie Lian et.al|[paper](https://arxiv.org/abs/2601.15197)|-|-|\n", "Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources": "|**2026-1-22**|**Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources**|Marzieh Adeli Shamsabad et.al|[paper](https://arxiv.org/abs/2601.16108)|-|-|\n", "DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models": "|**2026-1-22**|**DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models**|Chenyang Li et.al|[paper](https://arxiv.org/abs/2601.16065)|[code](https://anonymous.4open.science/r/CBD3.)|-|\n", "Vision-Language Models Align with Human Neural Representations in Concept Processing": "|**2026-1-22**|**Vision-Language Models Align with Human Neural Representations in Concept Processing**|Anna Bavaresco et.al|[paper](https://arxiv.org/abs/2407.17914)|[code](https://github.com/dmg-illc/vl-concept-processing.)|<details><summary>detail</summary>EACL 2026 main</details>|\n", "Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment": "|**2026-1-22**|**Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment**|Libo Wang et.al|[paper](https://arxiv.org/abs/2512.00783)|[code](https://huggingface.co/Veltraxor/Sigma)|<details><summary>detail</summary>The Sigma model has been open-sourced on Hugging Face</details>|\n", "Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization": "|**2026-1-22**|**Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization**|Jiwei Guan et.al|[paper](https://arxiv.org/abs/2601.01747)|-|<details><summary>detail</summary>EACL</details>|\n", "Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework": "|**2026-1-22**|**Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework**|Shubham Shukla et.al|[paper](https://arxiv.org/abs/2601.15711)|-|<details><summary>detail</summary>WACV 2026 Workshop on Physical Retail AI (PRAW)</details>|\n", "Towards Understanding Best Practices for Quantization of Vision-Language Models": "|**2026-1-21**|**Towards Understanding Best Practices for Quantization of Vision-Language Models**|Gautom Das et.al|[paper](https://arxiv.org/abs/2601.15287)|[code](https://github.com/gautomdas/mmq.)|-|\n", "PROGRESSLM: Towards Progress Reasoning in Vision-Language Models": "|**2026-1-21**|**PROGRESSLM: Towards Progress Reasoning in Vision-Language Models**|Jianshu Zhang et.al|[paper](https://arxiv.org/abs/2601.15224)|[code](https://progresslm.github.io/ProgressLM/)|<details><summary>detail</summary>Website: https://progresslm</details>|\n", "GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis": "|**2026-1-21**|**GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis**|Angelos Zavras et.al|[paper](https://arxiv.org/abs/2502.09598)|[code](https://github.com/Orion-AI-Lab/GAIA.)|-|\n", "Vision-Language Models on the Edge for Real-Time Robotic Perception": "|**2026-1-21**|**Vision-Language Models on the Edge for Real-Time Robotic Perception**|Sarat Ahmad et.al|[paper](https://arxiv.org/abs/2601.14921)|-|-|\n", "HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation": "|**2026-1-21**|**HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation**|Yara Mahmoud et.al|[paper](https://arxiv.org/abs/2601.14874)|-|<details><summary>detail</summary>This paper has been accepted for publication at LBR of HRI 2026 conference</details>|\n", "Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies": "|**2026-1-21**|**Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies**|Ben Schaper et.al|[paper](https://arxiv.org/abs/2601.14827)|-|-|\n", "A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection": "|**2026-1-21**|**A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection**|Guiying Zhu et.al|[paper](https://arxiv.org/abs/2601.11910)|-|-|\n"}}