{"source-free": {"SFUOD: Source-Free Unknown Object Detection": "|**2025-7-23**|**SFUOD: Source-Free Unknown Object Detection**|Keon-Hee Park et.al|[paper](https://arxiv.org/abs/2507.17373)|-|<details><summary>detail</summary>This paper has been accepted by ICCV 2025</details>|\n", "Model-Free and Real-Time Bioinspired Unicycle-Based Source Seeking: Differential Wheeled Robotic Experiments": "|**2025-7-21**|**Model-Free and Real-Time Bioinspired Unicycle-Based Source Seeking: Differential Wheeled Robotic Experiments**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2501.02184)|-|-|\n", "Text-Driven Causal Representation Learning for Source-Free Domain Generalization": "|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|\n", "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting": "|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|\n", "Source-Free Domain Adaptation via Multi-view Contrastive Learning": "|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|\n", "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-6-26**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "SFDLA: Source-Free Document Layout Analysis": "|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|\n", "Unified Source-Free Domain Adaptation": "|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|\n", "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|\n", "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization": "|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|\n", "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data": "|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|\n", "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation": "|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|\n", "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation": "|**2025-5-30**|**Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation**|Prasanna Reddy Pulakurthi et.al|[paper](https://arxiv.org/abs/2505.24216)|[code](https://github.com/PrasannaPulakurthi/SPM)|-|\n", "Source-Free Domain Adaptation for Speaker Verification in Data-Scarce Languages and Noisy Channels": "|**2025-7-21**|**Source-Free Domain Adaptation for Speaker Verification in Data-Scarce Languages and Noisy Channels**|S Salo Elia et.al|[paper](https://ui.adsabs.harvard.edu/abs/2024arXiv240605863S/abstract)|[code](https://paperswithcode.com/paper/source-free-domain-adaptation-for-speaker)|-|\n", "Global self-sustaining and local inheritance for source-free unsupervised domain adaptation": "|**2025-7-20**|**Global self-sustaining and local inheritance for source-free unsupervised domain adaptation**|L Peng et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0031320324004308)|-|<details><summary>detail</summary>Pattern Recognition, 2024 Elsevier</details>|\n"}, "object detection": {"Perspective-Invariant 3D Object Detection": "|**2025-7-23**|**Perspective-Invariant 3D Object Detection**|Ao Liang et.al|[paper](https://arxiv.org/abs/2507.17665)|[code](https://pi3det.github.io)|<details><summary>detail</summary>ICCV 2025</details>|\n", "BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion": "|**2025-7-23**|**BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion**|Yuqing Lan et.al|[paper](https://arxiv.org/abs/2506.15610)|[code](https://lanlan96.github.io/BoxFusion/)|<details><summary>detail</summary>Project page: https://lanlan96</details>|\n", "RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet": "|**2025-7-23**|**RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet**|Eliraz Orfaig et.al|[paper](https://arxiv.org/abs/2505.02586)|-|-|\n", "Illicit object detection in X-ray imaging using deep learning techniques: A comparative evaluation": "|**2025-7-23**|**Illicit object detection in X-ray imaging using deep learning techniques: A comparative evaluation**|Jorgen Cani et.al|[paper](https://arxiv.org/abs/2507.17508)|[code](https://github.com/jgenc/xray-comparative-evaluation.)|-|\n", "Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object Interaction Detection": "|**2025-7-23**|**Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object Interaction Detection**|Francesco Tonini et.al|[paper](https://arxiv.org/abs/2507.17456)|[code](https://github.com/francescotonini/dysco.)|<details><summary>detail</summary>ACM Multimedia 2025</details>|\n", "IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception": "|**2025-7-23**|**IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception**|Haichuan Li et.al|[paper](https://arxiv.org/abs/2507.17445)|-|-|\n", "Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection": "|**2025-7-23**|**Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection**|Yehao Lu et.al|[paper](https://arxiv.org/abs/2507.17436)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "SFUOD: Source-Free Unknown Object Detection": "|**2025-7-23**|**SFUOD: Source-Free Unknown Object Detection**|Keon-Hee Park et.al|[paper](https://arxiv.org/abs/2507.17373)|-|<details><summary>detail</summary>This paper has been accepted by ICCV 2025</details>|\n", "Few-Shot Learning in Video and 3D Object Detection: A Survey": "|**2025-7-22**|**Few-Shot Learning in Video and 3D Object Detection: A Survey**|Md Meftahul Ferdaus et.al|[paper](https://arxiv.org/abs/2507.17079)|-|<details><summary>detail</summary>Under review in ACM Computing Surveys</details>|\n", "Task-Specific Zero-shot Quantization-Aware Training for Object Detection": "|**2025-7-22**|**Task-Specific Zero-shot Quantization-Aware Training for Object Detection**|Changhao Li et.al|[paper](https://arxiv.org/abs/2507.16782)|[code](https://github.com/DFQ-Dojo/dfq-toolkit)|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium Specimens": "|**2025-7-22**|**PlantSAM: An Object Detection-Driven Segmentation Pipeline for Herbarium Specimens**|Youcef Sklab et.al|[paper](https://arxiv.org/abs/2507.16506)|-|-|\n", "Edge-case Synthesis for Fisheye Object Detection: A Data-centric Perspective": "|**2025-7-22**|**Edge-case Synthesis for Fisheye Object Detection: A Data-centric Perspective**|Seunghyeon Kim et.al|[paper](https://arxiv.org/abs/2507.16254)|-|-|\n", "LDRFusion: A LiDAR-Dominant multimodal refinement framework for 3D object detection": "|**2025-7-22**|**LDRFusion: A LiDAR-Dominant multimodal refinement framework for 3D object detection**|Jijun Wang et.al|[paper](https://arxiv.org/abs/2507.16224)|-|-|\n", "Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge": "|**2025-7-21**|**Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge**|Linshen Liu et.al|[paper](https://arxiv.org/abs/2507.04123)|[code](https://github.com/LinshenLiu622/EMC2.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement": "|**2025-7-21**|**UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement**|Xiao Zhang et.al|[paper](https://arxiv.org/abs/2507.00721)|[code](https://github.com/AMAP-ML/UPRE.)|<details><summary>detail</summary>ICCV2025</details>|\n"}, "domain adaptation": {"Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder": "|**2025-7-23**|**Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder**|Wonwoong Cho et.al|[paper](https://arxiv.org/abs/2503.11937)|[code](https://tri-mac.github.io/att-adapter/)|<details><summary>detail</summary>ICCV'25</details>|\n", "SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition": "|**2025-7-23**|**SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition**|Jiahao Tang et.al|[paper](https://arxiv.org/abs/2507.17524)|[code](https://github.com/XuanSuTrum/SDC-Net.)|-|\n", "Towards Railway Domain Adaptation for LiDAR-based 3D Detection: Road-to-Rail and Sim-to-Real via SynDRA-BBox": "|**2025-7-22**|**Towards Railway Domain Adaptation for LiDAR-based 3D Detection: Road-to-Rail and Sim-to-Real via SynDRA-BBox**|Xavier Diaz et.al|[paper](https://arxiv.org/abs/2507.16413)|[code](https://syndra.retis.santannapisa.it.)|<details><summary>detail</summary>IEEE International Conference on Intelligent Rail Transportation (ICIRT) 2025</details>|\n", "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement": "|**2025-7-21**|**UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement**|Xiao Zhang et.al|[paper](https://arxiv.org/abs/2507.00721)|[code](https://github.com/AMAP-ML/UPRE.)|<details><summary>detail</summary>ICCV2025</details>|\n", "MORDA: A Synthetic Dataset to Facilitate Adaptation of Object Detectors to Unseen Real-target Domain While Preserving Performance on Real-source Domain": "|**2025-7-21**|**MORDA: A Synthetic Dataset to Facilitate Adaptation of Object Detectors to Unseen Real-target Domain While Preserving Performance on Real-source Domain**|Hojun Lim et.al|[paper](https://arxiv.org/abs/2501.04950)|-|-|\n", "PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing": "|**2025-7-20**|**PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing**|Fu-Jen Tsai et.al|[paper](https://arxiv.org/abs/2507.14826)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "Domain-Adaptive Small Language Models for Structured Tax Code Prediction": "|**2025-7-19**|**Domain-Adaptive Small Language Models for Structured Tax Code Prediction**|Souvik Nath et.al|[paper](https://arxiv.org/abs/2507.10880)|-|-|\n", "Fourier Domain Adaptation for Traffic Light Detection in Adverse Weather": "|**2025-7-19**|**Fourier Domain Adaptation for Traffic Light Detection in Adverse Weather**|Ishaan Gakhar et.al|[paper](https://arxiv.org/abs/2411.07901)|-|<details><summary>detail</summary>the 2COOOL Workshop</details>|\n", "When few labeled target data suffice: a theory of semi-supervised domain adaptation via fine-tuning from multiple adaptive starts": "|**2025-7-19**|**When few labeled target data suffice: a theory of semi-supervised domain adaptation via fine-tuning from multiple adaptive starts**|Wooseok Ha et.al|[paper](https://arxiv.org/abs/2507.14661)|-|-|\n", "SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering": "|**2025-7-18**|**SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering**|Durgesh Singh et.al|[paper](https://arxiv.org/abs/2507.13779)|-|<details><summary>detail</summary>Journal ref:Pattern Recognition 2025</details>|\n", "Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica": "|**2025-7-17**|**Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica**|Jaber Daneshamooz et.al|[paper](https://arxiv.org/abs/2507.13476)|-|-|\n", "SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with Equivariant Neural Networks": "|**2025-7-17**|**SIDDA: SInkhorn Dynamic Domain Adaptation for Image Classification with Equivariant Neural Networks**|Sneh Pandya et.al|[paper](https://arxiv.org/abs/2501.14048)|[code](https://github.com/deepskies/SIDDA)|-|\n", "A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique": "|**2025-7-16**|**A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique**|Homare Sueyoshi et.al|[paper](https://arxiv.org/abs/2507.12730)|-|-|\n", "Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms": "|**2025-7-16**|**Prominent Roles of Conditionally Invariant Components in Domain Adaptation: Theory and Algorithms**|Keru Wu et.al|[paper](https://arxiv.org/abs/2309.10301)|-|-|\n", "SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection": "|**2025-7-16**|**SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection**|Xiwei Zhang et.al|[paper](https://arxiv.org/abs/2507.12017)|-|-|\n"}, "domain generalization": {"Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation": "|**2025-7-23**|**Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation**|Huanli Zhuo et.al|[paper](https://arxiv.org/abs/2507.17281)|-|<details><summary>detail</summary>This manuscript has been accepted for presentation at the IEEE International Conference on Systems</details>|\n", "Gradient-Guided Annealing for Domain Generalization": "|**2025-7-21**|**Gradient-Guided Annealing for Domain Generalization**|Aristotelis Ballas et.al|[paper](https://arxiv.org/abs/2502.20162)|-|<details><summary>detail</summary>Paper accepted in CVPR2025</details>|\n", "DGSSA: Domain generalization with structural and stylistic augmentation for retinal vessel segmentation": "|**2025-7-20**|**DGSSA: Domain generalization with structural and stylistic augmentation for retinal vessel segmentation**|Bo Liu et.al|[paper](https://arxiv.org/abs/2501.03466)|-|-|\n", "Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards": "|**2025-7-19**|**Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards**|Derek Li et.al|[paper](https://arxiv.org/abs/2507.14783)|-|-|\n", "Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification": "|**2025-7-19**|**Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification**|Subhendu Khatuya et.al|[paper](https://arxiv.org/abs/2506.06806)|-|<details><summary>detail</summary>This work has been accepted to appear at the Association for Computational Linguistics (ACL)</details>|\n", "Generative Multi-Target Cross-Domain Recommendation": "|**2025-7-17**|**Generative Multi-Target Cross-Domain Recommendation**|Jinqiu Jin et.al|[paper](https://arxiv.org/abs/2507.12871)|-|<details><summary>detail</summary>fix author information</details>|\n", "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning": "|**2025-7-17**|**Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning**|Simon Ouellette et.al|[paper](https://arxiv.org/abs/2507.15877)|-|-|\n", "Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica": "|**2025-7-17**|**Addressing the ML Domain Adaptation Problem for Networking: Realistic and Controllable Training Data Generation with NetReplica**|Jaber Daneshamooz et.al|[paper](https://arxiv.org/abs/2507.13476)|-|-|\n", "CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings": "|**2025-7-17**|**CoDet-M4: Detecting Machine-Generated Code in Multi-Lingual, Multi-Generator and Multi-Domain Settings**|Daniil Orel et.al|[paper](https://arxiv.org/abs/2503.13733)|-|-|\n", "Generative AI Models for Learning Flow Maps of Stochastic Dynamical Systems in Bounded Domains": "|**2025-7-17**|**Generative AI Models for Learning Flow Maps of Stochastic Dynamical Systems in Bounded Domains**|Minglei Yang et.al|[paper](https://arxiv.org/abs/2507.15990)|-|-|\n", "A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints": "|**2025-7-17**|**A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints**|Youssef Tawfilis et.al|[paper](https://arxiv.org/abs/2507.12979)|[code](https://github.com/youssefga28/HuSCF-GAN.)|-|\n", "Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization": "|**2025-7-17**|**Simulate, Refocus and Ensemble: An Attention-Refocusing Scheme for Domain Generalization**|Ziyi Wang et.al|[paper](https://arxiv.org/abs/2507.12851)|[code](https://github.com/bitPrincy/SRE-DG.)|<details><summary>detail</summary>\\c{opyright} 20XX IEEE</details>|\n", "Domain Generalization via Pareto Optimal Gradient Matching": "|**2025-7-16**|**Domain Generalization via Pareto Optimal Gradient Matching**|Khoi Do et.al|[paper](https://arxiv.org/abs/2507.14227)|-|-|\n", "Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation": "|**2025-7-15**|**Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2504.12753)|[code](https://github.com/anonymouse-xzrptkvyqc/DepthForge.)|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "Text-Driven Causal Representation Learning for Source-Free Domain Generalization": "|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|\n"}, "vision language": {"InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation": "|**2025-7-23**|**InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation**|Shuai Yang et.al|[paper](https://arxiv.org/abs/2507.17520)|-|-|\n", "Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls": "|**2025-7-23**|**Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls**|Elena Pitta et.al|[paper](https://arxiv.org/abs/2507.17467)|-|<details><summary>detail</summary>LUHME: 2nd Workshop on Language Understanding in the Human-Machine Era</details>|\n", "AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference": "|**2025-7-23**|**AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference**|Kai Huang et.al|[paper](https://arxiv.org/abs/2503.23956)|-|-|\n", "Confidence Calibration in Vision-Language-Action Models": "|**2025-7-23**|**Confidence Calibration in Vision-Language-Action Models**|Thomas P Zollo et.al|[paper](https://arxiv.org/abs/2507.17383)|-|-|\n", "RoadBench: A Vision-Language Foundation Model and Benchmark for Road Damage Understanding": "|**2025-7-23**|**RoadBench: A Vision-Language Foundation Model and Benchmark for Road Damage Understanding**|Xi Xiao et.al|[paper](https://arxiv.org/abs/2507.17353)|-|-|\n", "TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition": "|**2025-7-23**|**TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition**|Guangzhu Xu et.al|[paper](https://arxiv.org/abs/2507.17335)|-|-|\n", "VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback": "|**2025-7-23**|**VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback**|Jianxin Bi et.al|[paper](https://arxiv.org/abs/2507.17294)|[code](https://github.com/jxbi1010/VLA-Touch)|-|\n", "MaskedCLIP: Bridging the Masked and CLIP Space for Semi-Supervised Medical Vision-Language Pre-training": "|**2025-7-23**|**MaskedCLIP: Bridging the Masked and CLIP Space for Semi-Supervised Medical Vision-Language Pre-training**|Lei Zhu et.al|[paper](https://arxiv.org/abs/2507.17239)|-|<details><summary>detail</summary>MedAGI 2025 (Oral)</details>|\n", "LEGO Co-builder: Exploring Fine-Grained Vision-Language Modeling for Multimodal LEGO Assembly Assistants": "|**2025-7-23**|**LEGO Co-builder: Exploring Fine-Grained Vision-Language Modeling for Multimodal LEGO Assembly Assistants**|Haochen Huang et.al|[paper](https://arxiv.org/abs/2507.05515)|-|<details><summary>detail</summary>This version has been anonymized for double-blind review</details>|\n", "SurgXBench: Explainable Vision-Language Model Benchmark for Surgery": "|**2025-7-23**|**SurgXBench: Explainable Vision-Language Model Benchmark for Surgery**|Jiajun Cheng et.al|[paper](https://arxiv.org/abs/2505.10764)|-|-|\n", "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models": "|**2025-7-22**|**MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models**|Qiyan Zhao et.al|[paper](https://arxiv.org/abs/2507.09184)|[code](https://github.com/ErikZ719/MCA-LLaVA.)|<details><summary>detail</summary>Accepted in ACM MM 2025</details>|\n", "FedVLM: Scalable Personalized Vision-Language Models through Federated Learning": "|**2025-7-22**|**FedVLM: Scalable Personalized Vision-Language Models through Federated Learning**|Arkajyoti Mitra et.al|[paper](https://arxiv.org/abs/2507.17088)|-|-|\n", "VL-Explore: Zero-shot Vision-Language Exploration and Target Discovery by Mobile Robots": "|**2025-7-22**|**VL-Explore: Zero-shot Vision-Language Exploration and Target Discovery by Mobile Robots**|Yuxuan Zhang et.al|[paper](https://arxiv.org/abs/2502.08791)|-|<details><summary>detail</summary>V2</details>|\n", "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles": "|**2025-7-22**|**OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles**|Yihe Deng et.al|[paper](https://arxiv.org/abs/2503.17352)|[code](https://github.com/yihedeng9/OpenVLThinker.)|-|\n", "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning": "|**2025-7-22**|**ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning**|Chi-Pin Huang et.al|[paper](https://arxiv.org/abs/2507.16815)|[code](https://jasper0314-huang.github.io/thinkact-vla/)|<details><summary>detail</summary>Project page: https://jasper0314-huang</details>|\n"}}