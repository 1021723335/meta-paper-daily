{"source-free": {"StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails": "|**2025-9-2**|**StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails**|Hritik Arasu et.al|[paper](https://arxiv.org/abs/2509.02982)|-|<details><summary>detail</summary>5 page paper</details>|\n", "Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration": "|**2025-8-28**|**Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2508.20836)|-|-|\n", "VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection": "|**2025-8-26**|**VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection**|Jianhong Han et.al|[paper](https://arxiv.org/abs/2508.11167)|-|<details><summary>detail</summary>Manuscript submitted to IEEE TCSVT</details>|\n", "Towards Source-Free Machine Unlearning": "|**2025-8-20**|**Towards Source-Free Machine Unlearning**|Sk Miraj Ahmed et.al|[paper](https://arxiv.org/abs/2508.15127)|-|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method": "|**2025-8-14**|**Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2508.09202)|-|-|\n", "Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation": "|**2025-8-7**|**Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation**|Jianming Liu et.al|[paper](https://arxiv.org/abs/2508.05213)|[code](https://github.com/ljm198134/TVGTANet.)|-|\n", "Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning": "|**2025-8-4**|**Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning**|Sijia Wang et.al|[paper](https://arxiv.org/abs/2508.02039)|-|-|\n", "Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments": "|**2025-7-30**|**Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2501.02184)|-|-|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-7-28**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning": "|**2025-7-26**|**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**|Sanqing Qu et.al|[paper](https://arxiv.org/abs/2403.14410)|[code](https://github.com/ispc-lab/GLC-plus.)|<details><summary>detail</summary>A substantial extension of the CVPR paper \"Upcycling Models under Domain and Category Shift\"</details>|\n", "SFUOD: Source-Free Unknown Object Detection": "|**2025-7-23**|**SFUOD: Source-Free Unknown Object Detection**|Keon-Hee Park et.al|[paper](https://arxiv.org/abs/2507.17373)|-|<details><summary>detail</summary>This paper has been accepted by ICCV 2025</details>|\n", "Text-Driven Causal Representation Learning for Source-Free Domain Generalization": "|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|\n", "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting": "|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|\n", "Source-Free Domain Adaptation via Multi-view Contrastive Learning": "|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|\n", "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n"}, "object detection": {"Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM": "|**2025-9-8**|**Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM**|Hua Zhang et.al|[paper](https://arxiv.org/abs/2509.06422)|-|-|\n", "Exploring Light-Weight Object Recognition for Real-Time Document Detection": "|**2025-9-7**|**Exploring Light-Weight Object Recognition for Real-Time Document Detection**|Lucas Wojcik et.al|[paper](https://arxiv.org/abs/2509.06246)|[code](https://github.com/BOVIFOCR/iwpod-doc-corners.git)|-|\n", "Cross-Modal Enhancement and Benchmark for UAV-based Open-Vocabulary Object Detection": "|**2025-9-7**|**Cross-Modal Enhancement and Benchmark for UAV-based Open-Vocabulary Object Detection**|Zhenhai Weng et.al|[paper](https://arxiv.org/abs/2509.06011)|-|-|\n", "S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion": "|**2025-9-7**|**S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion**|Diana-Alexandra Sas et.al|[paper](https://arxiv.org/abs/2509.05999)|-|-|\n", "StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud": "|**2025-9-7**|**StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud**|Weichao Wang et.al|[paper](https://arxiv.org/abs/2509.05954)|-|-|\n", "3DPillars: Pillar-based two-stage 3D object detection": "|**2025-9-6**|**3DPillars: Pillar-based two-stage 3D object detection**|Jongyoun Noh et.al|[paper](https://arxiv.org/abs/2509.05780)|-|-|\n", "Sense4FL: Vehicular Crowdsensing Enhanced Federated Learning for Object Detection in Autonomous Driving": "|**2025-9-5**|**Sense4FL: Vehicular Crowdsensing Enhanced Federated Learning for Object Detection in Autonomous Driving**|Yanan Ma et.al|[paper](https://arxiv.org/abs/2503.17697)|-|-|\n", "Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection": "|**2025-9-5**|**Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection**|Bryce Grant et.al|[paper](https://arxiv.org/abs/2509.05512)|-|<details><summary>detail</summary>IROS 2025</details>|\n", "3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection": "|**2025-9-5**|**3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection**|Yung-Hsu Yang et.al|[paper](https://arxiv.org/abs/2507.23567)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception": "|**2025-9-5**|**YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception**|Mengqi Lei et.al|[paper](https://arxiv.org/abs/2506.17733)|[code](https://github.com/iMoonLab/yolov13.)|-|\n", "GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images": "|**2025-9-5**|**GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images**|Mengyu Ren et.al|[paper](https://arxiv.org/abs/2508.10542)|-|-|\n", "FADE: A Dataset for Detecting Falling Objects around Buildings in Video": "|**2025-9-4**|**FADE: A Dataset for Detecting Falling Objects around Buildings in Video**|Zhigang Tu et.al|[paper](https://arxiv.org/abs/2408.05750)|[code](https://fadedataset.github.io/FADE.github.io/.)|<details><summary>detail</summary>Accepted by IEEE Transactions on Information Forensics and Security (TIFS)</details>|\n", "SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection": "|**2025-9-4**|**SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection**|Xinxin Huang et.al|[paper](https://arxiv.org/abs/2509.03786)|-|-|\n", "RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images": "|**2025-9-4**|**RS-TinyNet: Stage-wise Feature Fusion Network for Detecting Tiny Objects in Remote Sensing Images**|Xiaozheng Jiang et.al|[paper](https://arxiv.org/abs/2507.13120)|-|<details><summary>detail</summary>The content of the thesis requires supplementation to make it more substantial</details>|\n", "Domain Adaptation for Different Sensor Configurations in 3D Object Detection": "|**2025-9-4**|**Domain Adaptation for Different Sensor Configurations in 3D Object Detection**|Satoshi Tanaka et.al|[paper](https://arxiv.org/abs/2509.04711)|-|-|\n"}, "domain adaptation": {"Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning": "|**2025-9-8**|**Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning**|Yingxu Wang et.al|[paper](https://arxiv.org/abs/2508.00716)|-|-|\n", "DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series": "|**2025-9-7**|**DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series**|Zahra Zamanzadeh Darban et.al|[paper](https://arxiv.org/abs/2404.11269)|-|-|\n", "Domain Adaptation for Different Sensor Configurations in 3D Object Detection": "|**2025-9-4**|**Domain Adaptation for Different Sensor Configurations in 3D Object Detection**|Satoshi Tanaka et.al|[paper](https://arxiv.org/abs/2509.04711)|-|-|\n", "Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation": "|**2025-9-4**|**Transferable Mask Transformer: Cross-domain Semantic Segmentation with Region-adaptive Transferability Estimation**|Jianhua Liu et.al|[paper](https://arxiv.org/abs/2504.05774)|-|-|\n", "In-Context Policy Adaptation via Cross-Domain Skill Diffusion": "|**2025-9-4**|**In-Context Policy Adaptation via Cross-Domain Skill Diffusion**|Minjong Yoo et.al|[paper](https://arxiv.org/abs/2509.04535)|-|-|\n", "Domain Adaptation of LLMs for Process Data": "|**2025-9-3**|**Domain Adaptation of LLMs for Process Data**|Rafael Seidi Oyamada et.al|[paper](https://arxiv.org/abs/2509.03161)|-|-|\n", "Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression": "|**2025-9-3**|**Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly Domain Adaptive Dense Regression**|Uddeshya Upadhyay et.al|[paper](https://arxiv.org/abs/2509.03012)|-|-|\n", "DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data": "|**2025-9-1**|**DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data**|Sabbir Ahmed et.al|[paper](https://arxiv.org/abs/2506.18173)|-|<details><summary>detail</summary>Accepted in 8th ACPR Springer</details>|\n", "Domain Adaptation for Big Data in Agricultural Image Analysis: A Comprehensive Review": "|**2025-8-30**|**Domain Adaptation for Big Data in Agricultural Image Analysis: A Comprehensive Review**|Xing Hu et.al|[paper](https://arxiv.org/abs/2506.05972)|-|-|\n", "Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation": "|**2025-8-30**|**Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation**|Jialiang Kang et.al|[paper](https://arxiv.org/abs/2509.00379)|-|<details><summary>detail</summary>ICRA 2025</details>|\n", "MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation": "|**2025-8-29**|**MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation**|Francisco Caetano et.al|[paper](https://arxiv.org/abs/2508.21435)|[code](https://caetas.github.io/medshift.html)|<details><summary>detail</summary>the ICCV 2025 AIM Workshop</details>|\n", "A Knowledge Distillation-empowered Adaptive Federated Reinforcement Learning Framework for Multi-Domain IoT Applications Scheduling": "|**2025-8-29**|**A Knowledge Distillation-empowered Adaptive Federated Reinforcement Learning Framework for Multi-Domain IoT Applications Scheduling**|Zhiyu Wang et.al|[paper](https://arxiv.org/abs/2508.21328)|-|-|\n", "ODES: Domain Adaptation with Expert Guidance for Online Medical Image Segmentation": "|**2025-8-28**|**ODES: Domain Adaptation with Expert Guidance for Online Medical Image Segmentation**|Md Shazid Islam et.al|[paper](https://arxiv.org/abs/2312.05407)|-|-|\n", "Gradual Domain Adaptation for Graph Learning": "|**2025-8-28**|**Gradual Domain Adaptation for Graph Learning**|Pui Ieng Lei et.al|[paper](https://arxiv.org/abs/2501.17443)|-|-|\n", "Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data": "|**2025-8-28**|**Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data**|Jiahao Xiao et.al|[paper](https://arxiv.org/abs/2508.20557)|[code](https://github.com/jiahaoxiao1228/AdaFD.)|-|\n"}, "domain generalization": {"Process-Supervised Reward Models for Verifying Clinical Note Generation: A Scalable Approach Guided by Domain Expertise": "|**2025-9-8**|**Process-Supervised Reward Models for Verifying Clinical Note Generation: A Scalable Approach Guided by Domain Expertise**|Hanyin Wang et.al|[paper](https://arxiv.org/abs/2412.12583)|-|-|\n", "Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI Harmonization Method": "|**2025-9-8**|**Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI Harmonization Method**|Daniel Scholz et.al|[paper](https://arxiv.org/abs/2509.06592)|-|-|\n", "ConstStyle: Robust Domain Generalization with Unified Style Transformation": "|**2025-9-7**|**ConstStyle: Robust Domain Generalization with Unified Style Transformation**|Nam Duong Tran et.al|[paper](https://arxiv.org/abs/2509.05975)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain": "|**2025-9-3**|**Evaluating the Robustness of Retrieval-Augmented Generation to Adversarial Evidence in the Health Domain**|Shakiba Amirshahi et.al|[paper](https://arxiv.org/abs/2509.03787)|[code](https://github.com/shakibaam/RAG_ROBUSTNESS_EVAL)|-|\n", "Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach": "|**2025-9-2**|**Single Domain Generalization in Diabetic Retinopathy: A Neuro-Symbolic Learning Approach**|Midhat Urooj et.al|[paper](https://arxiv.org/abs/2509.02918)|-|<details><summary>detail</summary>Accepted in ANSyA 2025: 1st International Workshop on Advanced Neuro-Symbolic Applications</details>|\n", "SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images": "|**2025-9-2**|**SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images**|Pushpendra Dhakara et.al|[paper](https://arxiv.org/abs/2509.02287)|-|-|\n", "FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain": "|**2025-9-2**|**FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain**|Anum Afzal et.al|[paper](https://arxiv.org/abs/2509.02198)|-|-|\n", "REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization": "|**2025-9-1**|**REVELIO -- Universal Multimodal Task Load Estimation for Cross-Domain Generalization**|Maximilian P. Oppelt et.al|[paper](https://arxiv.org/abs/2509.01642)|-|-|\n", "Target-Oriented Single Domain Generalization": "|**2025-8-30**|**Target-Oriented Single Domain Generalization**|Marzi Heidari et.al|[paper](https://arxiv.org/abs/2509.00351)|-|-|\n", "MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification": "|**2025-8-29**|**MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification**|Hikmat Khan et.al|[paper](https://arxiv.org/abs/2509.00311)|[code](https://github.com/hikmatkhan/MorphGen)|-|\n", "Single Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement": "|**2025-8-29**|**Single Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement**|Jia-Xuan Jiang et.al|[paper](https://arxiv.org/abs/2507.08340)|[code](https://github.com/HopkinsKwong/MCCSDG)|<details><summary>detail</summary>Accepted by ACMMM 25</details>|\n", "Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations": "|**2025-8-29**|**Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations**|Ha Min Son et.al|[paper](https://arxiv.org/abs/2508.21769)|-|-|\n", "PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification": "|**2025-8-29**|**PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification**|Hao Yang et.al|[paper](https://arxiv.org/abs/2508.20835)|-|-|\n", "Decentralized Domain Generalization with Style Sharing: Formal Model and Convergence Analysis": "|**2025-8-28**|**Decentralized Domain Generalization with Style Sharing: Formal Model and Convergence Analysis**|Shahryar Zehtabi et.al|[paper](https://arxiv.org/abs/2504.06235)|-|-|\n", "Pan-Cancer mitotic figures detection and domain generalization: MIDOG 2025 Challenge": "|**2025-8-28**|**Pan-Cancer mitotic figures detection and domain generalization: MIDOG 2025 Challenge**|Zhuoyan Shen et.al|[paper](https://arxiv.org/abs/2509.02585)|-|-|\n"}, "vision language": {"F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions": "|**2025-9-8**|**F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions**|Qi Lv et.al|[paper](https://arxiv.org/abs/2509.06951)|-|-|\n", "LLaDA-VLA: Vision Language Diffusion Action Models": "|**2025-9-8**|**LLaDA-VLA: Vision Language Diffusion Action Models**|Yuqing Wen et.al|[paper](https://arxiv.org/abs/2509.06932)|-|-|\n", "Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots": "|**2025-9-8**|**Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots**|Oluwadamilola Sotomi et.al|[paper](https://arxiv.org/abs/2509.06768)|-|-|\n", "Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization": "|**2025-9-8**|**Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization**|Thanh Thi Nguyen et.al|[paper](https://arxiv.org/abs/2509.06759)|-|<details><summary>detail</summary>Accepted for publication in the Proceedings of the 8th International Conference on Algorithms</details>|\n", "T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation": "|**2025-9-8**|**T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation**|Xiaobei Zhao et.al|[paper](https://arxiv.org/abs/2509.06644)|[code](https://github.com/AlexTraveling/T-araVLN.)|-|\n", "On the Reproducibility of \"FairCLIP: Harnessing Fairness in Vision-Language Learning''": "|**2025-9-8**|**On the Reproducibility of \"FairCLIP: Harnessing Fairness in Vision-Language Learning''**|Hua Chang Bakker et.al|[paper](https://arxiv.org/abs/2509.06535)|-|-|\n", "When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection": "|**2025-9-8**|**When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection**|Rabin Dulal et.al|[paper](https://arxiv.org/abs/2509.06427)|-|<details><summary>detail</summary>Journal ref:Australasian Joint Conference on Artificial Intelligence 2025</details>|\n", "Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models": "|**2025-9-8**|**Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models**|Jaemin Son et.al|[paper](https://arxiv.org/abs/2509.06415)|-|<details><summary>detail</summary>Submitted to ICASSP 2026</details>|\n", "Error Notebook-Guided, Training-Free Part Retrieval in 3D CAD Assemblies via Vision-Language Models": "|**2025-9-7**|**Error Notebook-Guided, Training-Free Part Retrieval in 3D CAD Assemblies via Vision-Language Models**|Yunqing Liu et.al|[paper](https://arxiv.org/abs/2509.01350)|-|-|\n", "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes": "|**2025-9-7**|**Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes**|Mohsen Gholami et.al|[paper](https://arxiv.org/abs/2509.06266)|-|-|\n", "PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology": "|**2025-9-7**|**PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology**|Yating Huang et.al|[paper](https://arxiv.org/abs/2509.06105)|-|<details><summary>detail</summary>Accept by EMNLP2025</details>|\n", "Analysis of Blood Report Images Using General Purpose Vision-Language Models": "|**2025-9-7**|**Analysis of Blood Report Images Using General Purpose Vision-Language Models**|Nadia Bakhsheshi et.al|[paper](https://arxiv.org/abs/2509.06033)|-|-|\n", "Can Machines Imitate Humans? Integrative Turing-like tests for Language and Vision Demonstrate a Narrowing Gap": "|**2025-9-7**|**Can Machines Imitate Humans? Integrative Turing-like tests for Language and Vision Demonstrate a Narrowing Gap**|Mengmi Zhang et.al|[paper](https://arxiv.org/abs/2211.13087)|-|-|\n", "Planning with Reasoning using Vision Language World Model": "|**2025-9-6**|**Planning with Reasoning using Vision Language World Model**|Delong Chen et.al|[paper](https://arxiv.org/abs/2509.02722)|-|-|\n", "VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models": "|**2025-9-6**|**VisBias: Measuring Explicit and Implicit Social Biases in Vision Language Models**|Jen-tse Huang et.al|[paper](https://arxiv.org/abs/2503.07575)|[code](https://github.com/uscnlp-lime/VisBias.)|<details><summary>detail</summary>EMNLP 2025 (Main)</details>|\n"}}