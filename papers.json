{"source-free": {"Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-6-26**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "SFDLA: Source-Free Document Layout Analysis": "|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|\n", "Unified Source-Free Domain Adaptation": "|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|\n", "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|\n", "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization": "|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|\n", "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data": "|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|\n", "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation": "|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|\n", "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation": "|**2025-5-30**|**Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation**|Prasanna Reddy Pulakurthi et.al|[paper](https://arxiv.org/abs/2505.24216)|[code](https://github.com/PrasannaPulakurthi/SPM)|-|\n", "Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation": "|**2025-5-27**|**Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation**|Peihua Deng et.al|[paper](https://arxiv.org/abs/2411.16064)|[code](https://github.com/dengpeihua/GROTO.)|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Training-Free Multi-Step Audio Source Separation": "|**2025-5-26**|**Training-Free Multi-Step Audio Source Separation**|Yongyi Zang et.al|[paper](https://arxiv.org/abs/2505.19534)|-|-|\n", "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation": "|**2025-5-23**|**Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation**|Peiliang Gong et.al|[paper](https://arxiv.org/abs/2505.21525)|-|-|\n", "Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing": "|**2025-5-20**|**Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing**|Yang Xiao et.al|[paper](https://arxiv.org/abs/2505.14601)|-|<details><summary>detail</summary>Accepted by Interspeech 2025</details>|\n", "DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation": "|**2025-5-14**|**DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation**|Siqi Yin et.al|[paper](https://arxiv.org/abs/2505.09927)|-|-|\n"}, "object detection": {"Partial Weakly-Supervised Oriented Object Detection": "|**2025-7-3**|**Partial Weakly-Supervised Oriented Object Detection**|Mingxin Liu et.al|[paper](https://arxiv.org/abs/2507.02751)|[code](https://github.com/VisionXLab/PWOOD)|-|\n", "PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection": "|**2025-7-3**|**PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection**|Seokyeong Lee et.al|[paper](https://arxiv.org/abs/2507.02393)|-|-|\n", "MV2DFusion: Leveraging Modality-Specific Object Semantics for Multi-Modal 3D Detection": "|**2025-7-3**|**MV2DFusion: Leveraging Modality-Specific Object Semantics for Multi-Modal 3D Detection**|Zitian Wang et.al|[paper](https://arxiv.org/abs/2408.05945)|-|-|\n", "Anyview: Generalizable Indoor 3D Object Detection with Variable Frames": "|**2025-7-2**|**Anyview: Generalizable Indoor 3D Object Detection with Variable Frames**|Zhenyu Wu et.al|[paper](https://arxiv.org/abs/2310.05346)|-|-|\n", "Rapid Salient Object Detection with Difference Convolutional Neural Networks": "|**2025-7-1**|**Rapid Salient Object Detection with Difference Convolutional Neural Networks**|Zhuo Su et.al|[paper](https://arxiv.org/abs/2507.01182)|[code](https://github.com/hellozhuo/stdnet.git.)|-|\n", "UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial Vehicle Imagery": "|**2025-7-1**|**UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial Vehicle Imagery**|Huaxiang Zhang et.al|[paper](https://arxiv.org/abs/2501.01855)|[code](https://github.com/ValiantDiligent/UAV-DETR)|-|\n", "Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting": "|**2025-7-1**|**Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting**|Fatemeh Sadat Daneshmand et.al|[paper](https://arxiv.org/abs/2507.00852)|-|-|\n", "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement": "|**2025-7-1**|**UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement**|Xiao Zhang et.al|[paper](https://arxiv.org/abs/2507.00721)|[code](https://github.com/AMAP-ML/UPRE.)|<details><summary>detail</summary>ICCV2025</details>|\n", "De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection": "|**2025-7-1**|**De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection**|Zehua Fu et.al|[paper](https://arxiv.org/abs/2507.00608)|-|<details><summary>detail</summary>Accepted by IEEE Transactions on Intelligent Transportation Systems</details>|\n", "Rethink 3D Object Detection from Physical World": "|**2025-6-30**|**Rethink 3D Object Detection from Physical World**|Satoshi Tanaka et.al|[paper](https://arxiv.org/abs/2507.00190)|-|-|\n", "Mitigating Hallucinations in YOLO-based Object Detection Models: A Revisit to Out-of-Distribution Detection": "|**2025-6-30**|**Mitigating Hallucinations in YOLO-based Object Detection Models: A Revisit to Out-of-Distribution Detection**|Weicheng He et.al|[paper](https://arxiv.org/abs/2503.07330)|[code](https://gricad-gitlab.univ-grenoble-alpes.fr/dnn-safety/m-hood.)|<details><summary>detail</summary>Camera-ready version for IROS 2025</details>|\n", "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios": "|**2025-6-30**|**Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios**|Deng Li et.al|[paper](https://arxiv.org/abs/2506.24063)|-|-|\n", "Visual Textualization for Image Prompted Object Detection": "|**2025-6-30**|**Visual Textualization for Image Prompted Object Detection**|Yongjian Wu et.al|[paper](https://arxiv.org/abs/2506.23785)|[code](https://github.com/WitGotFlg/VisTex-OVLM.)|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "Methodology for an Analysis of Influencing Factors on 3D Object Detection Performance": "|**2025-6-30**|**Methodology for an Analysis of Influencing Factors on 3D Object Detection Performance**|Anton Kuznietsov et.al|[paper](https://arxiv.org/abs/2411.08482)|-|<details><summary>detail</summary>IEEE International Conference on Autonomous and Trusted Computing (IEEE ATC)</details>|\n", "PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection": "|**2025-6-30**|**PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection**|Xiao Li et.al|[paper](https://arxiv.org/abs/2506.23581)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n"}, "domain adaptation": {"F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning": "|**2025-7-3**|**F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning**|Wei Li et.al|[paper](https://arxiv.org/abs/2507.02437)|[code](https://github.com/mar-cry/F2TTA.)|<details><summary>detail</summary>This paper has been submitted to relevant journals</details>|\n", "Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability": "|**2025-7-3**|**Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability**|Mark Atta Mensah et.al|[paper](https://arxiv.org/abs/2507.02407)|-|<details><summary>detail</summary>This version has been reviewed and accepted for presentation at the Future Technologies Conference (FTC) 2025</details>|\n", "Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation": "|**2025-7-2**|**Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation**|Yuxiang Zhang et.al|[paper](https://arxiv.org/abs/2507.02268)|[code](https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.)|-|\n", "Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation": "|**2025-7-1**|**Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation**|Xihang Yu et.al|[paper](https://arxiv.org/abs/2507.00984)|-|-|\n", "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement": "|**2025-7-1**|**UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement**|Xiao Zhang et.al|[paper](https://arxiv.org/abs/2507.00721)|[code](https://github.com/AMAP-ML/UPRE.)|<details><summary>detail</summary>ICCV2025</details>|\n", "UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions": "|**2025-7-1**|**UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions**|Siyuan Yao et.al|[paper](https://arxiv.org/abs/2507.00648)|[code](https://github.com/Z-Z188/UMDATrack.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language": "|**2025-7-1**|**Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language**|Anastasia Zhukova et.al|[paper](https://arxiv.org/abs/2504.19856)|-|<details><summary>detail</summary>accepted to TSD 2025</details>|\n", "De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection": "|**2025-7-1**|**De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection**|Zehua Fu et.al|[paper](https://arxiv.org/abs/2507.00608)|-|<details><summary>detail</summary>Accepted by IEEE Transactions on Intelligent Transportation Systems</details>|\n", "Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving": "|**2025-6-30**|**Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving**|Chinmay Vilas Samak et.al|[paper](https://arxiv.org/abs/2507.00236)|-|-|\n", "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning": "|**2025-6-30**|**Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning**|Harisankar Babu et.al|[paper](https://arxiv.org/abs/2506.19592)|-|<details><summary>detail</summary>IEEE CASE 2025</details>|\n", "Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation": "|**2025-6-30**|**Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation**|Patrick Glandorf et.al|[paper](https://arxiv.org/abs/2506.23675)|-|<details><summary>detail</summary>ICCV'25 Workshops</details>|\n", "HASD: Hierarchical Adaption for pathology Slide-level Domain-shift": "|**2025-6-30**|**HASD: Hierarchical Adaption for pathology Slide-level Domain-shift**|Jingsong Liu et.al|[paper](https://arxiv.org/abs/2506.23673)|-|-|\n", "Relating Events and Frames Based on Self-Supervised Learning and Uncorrelated Conditioning for Unsupervised Domain Adaptation": "|**2025-6-29**|**Relating Events and Frames Based on Self-Supervised Learning and Uncorrelated Conditioning for Unsupervised Domain Adaptation**|Mohammad Rostami et.al|[paper](https://arxiv.org/abs/2401.01042)|-|-|\n", "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability": "|**2025-6-27**|**Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability**|Boyong He et.al|[paper](https://arxiv.org/abs/2506.21042)|[code](https://github.com/heboyong/Fitness-Generalization-Transferability)|<details><summary>detail</summary>Accepted by ICCV2025</details>|\n", "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis": "|**2025-6-27**|**Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis**|YongKyung Oh et.al|[paper](https://arxiv.org/abs/2506.22393)|-|-|\n"}, "domain generalization": {"Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation": "|**2025-7-3**|**Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2504.12753)|[code](https://github.com/anonymouse-xzrptkvyqc/DepthForge.)|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization": "|**2025-7-2**|**Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization**|De Cheng et.al|[paper](https://arxiv.org/abs/2507.02288)|-|-|\n", "NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation": "|**2025-7-2**|**NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation**|Zhenye Lou et.al|[paper](https://arxiv.org/abs/2408.11787)|[code](https://github.com/xq141839/NuSegDG.)|-|\n", "DGSAM: Domain Generalization via Individual Sharpness-Aware Minimization": "|**2025-6-30**|**DGSAM: Domain Generalization via Individual Sharpness-Aware Minimization**|Youngjun Song et.al|[paper](https://arxiv.org/abs/2503.23430)|-|-|\n", "Calculation of Photocarrier Generation from Optical Absorption for Time-domain Simulation of Optoelectronic Devices": "|**2025-6-30**|**Calculation of Photocarrier Generation from Optical Absorption for Time-domain Simulation of Optoelectronic Devices**|Liang Chen et.al|[paper](https://arxiv.org/abs/2102.06702)|-|-|\n", "Generalizing vision-language models to novel domains: A comprehensive survey": "|**2025-6-30**|**Generalizing vision-language models to novel domains: A comprehensive survey**|Xinyao Li et.al|[paper](https://arxiv.org/abs/2506.18504)|-|-|\n", "Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains": "|**2025-6-28**|**Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains**|Zhuo He et.al|[paper](https://arxiv.org/abs/2506.17718)|-|<details><summary>detail</summary>ICML 2025</details>|\n", "Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability": "|**2025-6-27**|**Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability**|Boyong He et.al|[paper](https://arxiv.org/abs/2506.21042)|[code](https://github.com/heboyong/Fitness-Generalization-Transferability)|<details><summary>detail</summary>Accepted by ICCV2025</details>|\n", "Video-Guided Text-to-Music Generation Using Public Domain Movie Collections": "|**2025-6-27**|**Video-Guided Text-to-Music Generation Using Public Domain Movie Collections**|Haven Kim et.al|[paper](https://arxiv.org/abs/2506.12573)|[code](https://havenpersona.github.io/ossl-v1)|<details><summary>detail</summary>ISMIR 2025 regular paper</details>|\n", "Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning": "|**2025-6-27**|**Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning**|Fangling Jiang et.al|[paper](https://arxiv.org/abs/2506.21895)|-|-|\n", "QT-DoG: Quantization-aware Training for Domain Generalization": "|**2025-6-26**|**QT-DoG: Quantization-aware Training for Domain Generalization**|Saqib Javed et.al|[paper](https://arxiv.org/abs/2410.06020)|[code](https://saqibjaved1.github.io/QT_DoG/.)|<details><summary>detail</summary>International Conference on Machine Learning (ICML) 2025</details>|\n", "MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models": "|**2025-6-26**|**MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models**|Yifan Liu et.al|[paper](https://arxiv.org/abs/2506.21784)|[code](https://github.com/ucla-mobility/MobiVerse.)|-|\n", "FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization": "|**2025-6-25**|**FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization**|Ha Min Son et.al|[paper](https://arxiv.org/abs/2506.20841)|-|-|\n", "General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound": "|**2025-6-24**|**General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound**|Jakob Ambsdorf et.al|[paper](https://arxiv.org/abs/2506.19552)|-|<details><summary>detail</summary>Submitted version of paper accepted at MICCAI 2025</details>|\n", "RLPR: Extrapolating RLVR to General Domains without Verifiers": "|**2025-6-22**|**RLPR: Extrapolating RLVR to General Domains without Verifiers**|Tianyu Yu et.al|[paper](https://arxiv.org/abs/2506.18254)|[code](https://github.com/openbmb/RLPR)|<details><summary>detail</summary>Project Website: https://github</details>|\n"}, "vision language": {"DexVLG: Dexterous Vision-Language-Grasp Model at Scale": "|**2025-7-3**|**DexVLG: Dexterous Vision-Language-Grasp Model at Scale**|Jiawei He et.al|[paper](https://arxiv.org/abs/2507.02747)|-|-|\n", "TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control": "|**2025-7-3**|**TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control**|Zhenyang Liu et.al|[paper](https://arxiv.org/abs/2507.01424)|-|-|\n", "Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning": "|**2025-7-3**|**Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning**|Yunpeng Gao et.al|[paper](https://arxiv.org/abs/2410.08500)|-|-|\n", "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping": "|**2025-7-3**|**Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping**|Weili Zeng et.al|[paper](https://arxiv.org/abs/2503.21817)|-|<details><summary>detail</summary>Accepted by ICCV2025</details>|\n", "Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models": "|**2025-7-2**|**Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models**| Hyoseo et.al|[paper](https://arxiv.org/abs/2507.01201)|-|-|\n", "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective": "|**2025-7-2**|**A Survey on Vision-Language-Action Models: An Action Tokenization Perspective**|Yifan Zhong et.al|[paper](https://arxiv.org/abs/2507.01925)|-|-|\n", "How Do Vision-Language Models Process Conflicting Information Across Modalities?": "|**2025-7-2**|**How Do Vision-Language Models Process Conflicting Information Across Modalities?**|Tianze Hua et.al|[paper](https://arxiv.org/abs/2507.01790)|[code](https://github.com/ethahtz/vlm_conflicting_info_processing)|<details><summary>detail</summary>All code and resources are available at: https://github</details>|\n", "World-aware Planning Narratives Enhance Large Vision-Language Model Planner": "|**2025-7-2**|**World-aware Planning Narratives Enhance Large Vision-Language Model Planner**|Junhao Shi et.al|[paper](https://arxiv.org/abs/2506.21230)|-|-|\n", "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models": "|**2025-7-2**|**Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models**|Junjie Wu et.al|[paper](https://arxiv.org/abs/2410.23114)|[code](https://github.com/wujunjie1998/Tri-HE.)|<details><summary>detail</summary>Accepted by TMLR 2025</details>|\n", "Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition": "|**2025-7-2**|**Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition**|Muzammil Behzad et.al|[paper](https://arxiv.org/abs/2507.01673)|-|-|\n", "olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models": "|**2025-7-2**|**olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models**|Jake Poznanski et.al|[paper](https://arxiv.org/abs/2502.18443)|-|-|\n", "Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model": "|**2025-7-2**|**Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model**|Chaoxiang Cai et.al|[paper](https://arxiv.org/abs/2507.01351)|-|-|\n", "Beyond-Labels: Advancing Open-Vocabulary Segmentation With Vision-Language Models": "|**2025-7-1**|**Beyond-Labels: Advancing Open-Vocabulary Segmentation With Vision-Language Models**|Muhammad Atta ur Rahman et.al|[paper](https://arxiv.org/abs/2501.16769)|-|<details><summary>detail</summary>the 17th IEEE International Conference on Advanced Computational Intelligence (ICACI 2025)</details>|\n", "A Survey on Efficient Vision-Language Models": "|**2025-7-1**|**A Survey on Efficient Vision-Language Models**|Gaurav Shinde et.al|[paper](https://arxiv.org/abs/2504.09724)|[code](https://github.com/MPSCUMBC/Efficient-Vision-Language-Models-A-Survey)|-|\n", "VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers": "|**2025-7-1**|**VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers**|Yating Wang et.al|[paper](https://arxiv.org/abs/2507.01016)|[code](https://xiaoxiao0406.github.io/vqvla.github.io)|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n"}}