{"source-free": {"SFDLA: Source-Free Document Layout Analysis": "|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|\n", "Unified Source-Free Domain Adaptation": "|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|\n", "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|\n", "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization": "|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|\n", "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data": "|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|\n", "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation": "|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|\n", "Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation": "|**2025-5-30**|**Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation**|Prasanna Reddy Pulakurthi et.al|[paper](https://arxiv.org/abs/2505.24216)|[code](https://github.com/PrasannaPulakurthi/SPM)|-|\n", "Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation": "|**2025-5-27**|**Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation**|Peihua Deng et.al|[paper](https://arxiv.org/abs/2411.16064)|[code](https://github.com/dengpeihua/GROTO.)|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Training-Free Multi-Step Audio Source Separation": "|**2025-5-26**|**Training-Free Multi-Step Audio Source Separation**|Yongyi Zang et.al|[paper](https://arxiv.org/abs/2505.19534)|-|-|\n", "Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation": "|**2025-5-23**|**Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation**|Peiliang Gong et.al|[paper](https://arxiv.org/abs/2505.21525)|-|-|\n", "Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing": "|**2025-5-20**|**Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing**|Yang Xiao et.al|[paper](https://arxiv.org/abs/2505.14601)|-|<details><summary>detail</summary>Accepted by Interspeech 2025</details>|\n", "DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation": "|**2025-5-14**|**DDFP: Data-dependent Frequency Prompt for Source Free Domain Adaptation of Medical Image Segmentation**|Siqi Yin et.al|[paper](https://arxiv.org/abs/2505.09927)|-|-|\n", "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting": "|**2025-5-13**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|-|\n", "CORSTITCH - A free, open source software for stitching and georeferencing underwater coral reef videos": "|**2025-5-1**|**CORSTITCH - A free, open source software for stitching and georeferencing underwater coral reef videos**|Julian Christopher L. Maypa et.al|[paper](https://arxiv.org/abs/2505.00462)|-|-|\n", "Energy-Based Pseudo-Label Refining for Source-free Domain Adaptation": "|**2025-4-23**|**Energy-Based Pseudo-Label Refining for Source-free Domain Adaptation**|Xinru Meng et.al|[paper](https://arxiv.org/abs/2504.16692)|[code](https://github.com/Sthen111/EBPR)|-|\n"}, "object detection": {"Align and Distill: Unifying and Improving Domain Adaptive Object Detection": "|**2025-6-23**|**Align and Distill: Unifying and Improving Domain Adaptive Object Detection**|Justin Kay et.al|[paper](https://arxiv.org/abs/2403.12029)|[code](https://github.com/justinkay/aldi)|<details><summary>detail</summary>TMLR camera ready (Featured Certification)</details>|\n", "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation": "|**2025-6-23**|**LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation**|Yang Zhou et.al|[paper](https://arxiv.org/abs/2503.13794)|-|-|\n", "On the Robustness of Human-Object Interaction Detection against Distribution Shift": "|**2025-6-22**|**On the Robustness of Human-Object Interaction Detection against Distribution Shift**|Chi Xie et.al|[paper](https://arxiv.org/abs/2506.18021)|-|<details><summary>detail</summary>This work has been submitted to the IEEE for possible publication</details>|\n", "Towards Reflected Object Detection: A Benchmark": "|**2025-6-22**|**Towards Reflected Object Detection: A Benchmark**|Yiquan Wu et.al|[paper](https://arxiv.org/abs/2407.05575)|[code](https://github.com/jirouvan/ROD.)|-|\n", "DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training": "|**2025-6-21**|**DART: An Automated End-to-End Object Detection Pipeline with Data Diversification, Open-Vocabulary Bounding Box Annotation, Pseudo-Label Review, and Model Training**|Chen Xin et.al|[paper](https://arxiv.org/abs/2407.09174)|[code](https://github.com/chen-xin-94/DART.)|<details><summary>detail</summary>Corrected minor typos</details>|\n", "Efficient Feature Aggregation and Scale-Aware Regression for Monocular 3D Object Detection": "|**2025-6-21**|**Efficient Feature Aggregation and Scale-Aware Regression for Monocular 3D Object Detection**|Yifan Wang et.al|[paper](https://arxiv.org/abs/2411.02747)|-|-|\n", "YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception": "|**2025-6-21**|**YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception**|Mengqi Lei et.al|[paper](https://arxiv.org/abs/2506.17733)|[code](https://github.com/iMoonLab/yolov13.)|-|\n", "CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection": "|**2025-6-21**|**CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection**|Wei Haolin et.al|[paper](https://arxiv.org/abs/2506.17679)|-|-|\n", "Cross-modal Offset-guided Dynamic Alignment and Fusion for Weakly Aligned UAV Object Detection": "|**2025-6-20**|**Cross-modal Offset-guided Dynamic Alignment and Fusion for Weakly Aligned UAV Object Detection**|Liu Zongzhen et.al|[paper](https://arxiv.org/abs/2506.16737)|-|-|\n", "Efficient Event-Based Object Detection: A Hybrid Neural Network with Spatial and Temporal Attention": "|**2025-6-19**|**Efficient Event-Based Object Detection: A Hybrid Neural Network with Spatial and Temporal Attention**|Soikat Hasan Ahmed et.al|[paper](https://arxiv.org/abs/2403.10173)|-|-|\n", "BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion": "|**2025-6-18**|**BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion**|Yuqing Lan et.al|[paper](https://arxiv.org/abs/2506.15610)|-|-|\n", "YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework": "|**2025-6-18**|**YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework**|Dahang Wan et.al|[paper](https://arxiv.org/abs/2506.14696)|[code](https://github.com/wandahangFY/YOLOv11-RGBT.)|-|\n", "Retrospective Memory for Camouflaged Object Detection": "|**2025-6-18**|**Retrospective Memory for Camouflaged Object Detection**|Chenxi Zhang et.al|[paper](https://arxiv.org/abs/2506.15244)|-|-|\n", "Concept Guided Co-salient Object Detection": "|**2025-6-17**|**Concept Guided Co-salient Object Detection**|Jiayi Zhu et.al|[paper](https://arxiv.org/abs/2412.16609)|-|-|\n", "Egocentric Human-Object Interaction Detection: A New Benchmark and Method": "|**2025-6-17**|**Egocentric Human-Object Interaction Detection: A New Benchmark and Method**|Kunyuan Deng et.al|[paper](https://arxiv.org/abs/2506.14189)|[code](https://dengkunyuan.github.io/EgoHOIBench/)|-|\n"}, "domain adaptation": {"crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023": "|**2025-6-24**|**crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023**|Navodini Wijethilake et.al|[paper](https://arxiv.org/abs/2506.12006)|-|-|\n", "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning": "|**2025-6-24**|**Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to Task Planning**|Harisankar Babu et.al|[paper](https://arxiv.org/abs/2506.19592)|-|-|\n", "M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition": "|**2025-6-24**|**M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition**|Ting Luo et.al|[paper](https://arxiv.org/abs/2404.15615)|-|-|\n", "Progressive Modality Cooperation for Multi-Modality Domain Adaptation": "|**2025-6-24**|**Progressive Modality Cooperation for Multi-Modality Domain Adaptation**|Weichen Zhang et.al|[paper](https://arxiv.org/abs/2506.19316)|-|-|\n", "Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation": "|**2025-6-23**|**Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation**|Weichen Zhang et.al|[paper](https://arxiv.org/abs/2506.19267)|-|-|\n", "Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation": "|**2025-6-23**|**Overlap-Aware Feature Learning for Robust Unsupervised Domain Adaptation for 3D Semantic Segmentation**|Junjie Chen et.al|[paper](https://arxiv.org/abs/2504.01668)|-|<details><summary>detail</summary>This paper has been accepted to the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)</details>|\n", "Align and Distill: Unifying and Improving Domain Adaptive Object Detection": "|**2025-6-23**|**Align and Distill: Unifying and Improving Domain Adaptive Object Detection**|Justin Kay et.al|[paper](https://arxiv.org/abs/2403.12029)|[code](https://github.com/justinkay/aldi)|<details><summary>detail</summary>TMLR camera ready (Featured Certification)</details>|\n", "DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data": "|**2025-6-22**|**DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data**|Sabbir Ahmed et.al|[paper](https://arxiv.org/abs/2506.18173)|-|<details><summary>detail</summary>Submitted to ACPR Springer</details>|\n", "Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction": "|**2025-6-22**|**Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction**|Rui An et.al|[paper](https://arxiv.org/abs/2506.18939)|-|-|\n", "IDAL: Improved Domain Adaptive Learning for Natural Images Dataset": "|**2025-6-22**|**IDAL: Improved Domain Adaptive Learning for Natural Images Dataset**|Ravi Kant Gupta et.al|[paper](https://arxiv.org/abs/2506.17931)|-|<details><summary>detail</summary>Accepted in ICPR'24 (International Conference on Pattern Recognition)</details>|\n", "When can isotropy help adapt LLMs' next word prediction to numerical domains?": "|**2025-6-20**|**When can isotropy help adapt LLMs' next word prediction to numerical domains?**|Rashed Shelim et.al|[paper](https://arxiv.org/abs/2505.17135)|-|-|\n", "Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation": "|**2025-6-20**|**Adaptive Control Attention Network for Underwater Acoustic Localization and Domain Adaptation**|Quoc Thinh Vo et.al|[paper](https://arxiv.org/abs/2506.17409)|-|<details><summary>detail</summary>This paper has been accepted for the 33rd European Signal Processing Conference (EUSIPCO) 2025 in Palermo</details>|\n", "On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting": "|**2025-6-20**|**On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting**|Zhuonan Liang et.al|[paper](https://arxiv.org/abs/2506.17137)|-|-|\n", "Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments": "|**2025-6-20**|**Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments**|Yasir Ali Farrukh et.al|[paper](https://arxiv.org/abs/2506.16994)|-|-|\n", "Bridging Domain Gaps in Agricultural Image Analysis: A Comprehensive Review From Shallow Adaptation to Deep Learning": "|**2025-6-20**|**Bridging Domain Gaps in Agricultural Image Analysis: A Comprehensive Review From Shallow Adaptation to Deep Learning**|Xing Hu et.al|[paper](https://arxiv.org/abs/2506.05972)|-|-|\n"}, "domain generalization": {"General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound": "|**2025-6-24**|**General Methods Make Great Domain-specific Foundation Models: A Case-study on Fetal Ultrasound**|Jakob Ambsdorf et.al|[paper](https://arxiv.org/abs/2506.19552)|-|<details><summary>detail</summary>Submitted version of paper accepted at MICCAI 2025</details>|\n", "Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey": "|**2025-6-23**|**Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey**|Xinyao Li et.al|[paper](https://arxiv.org/abs/2506.18504)|-|-|\n", "RLPR: Extrapolating RLVR to General Domains without Verifiers": "|**2025-6-22**|**RLPR: Extrapolating RLVR to General Domains without Verifiers**|Tianyu Yu et.al|[paper](https://arxiv.org/abs/2506.18254)|[code](https://github.com/openbmb/RLPR)|<details><summary>detail</summary>Project Website: https://github</details>|\n", "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation": "|**2025-6-22**|**RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation**|Tianxing Chen et.al|[paper](https://arxiv.org/abs/2506.18088)|[code](https://robotwin-platform.github.io/)|<details><summary>detail</summary>Project Page: https://robotwin-platform</details>|\n", "Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains": "|**2025-6-21**|**Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains**|Zhuo He et.al|[paper](https://arxiv.org/abs/2506.17718)|-|<details><summary>detail</summary>ICML 2025</details>|\n", "Domain Generalization using Action Sequences for Egocentric Action Recognition": "|**2025-6-21**|**Domain Generalization using Action Sequences for Egocentric Action Recognition**|Amirshayan Nasirimajd et.al|[paper](https://arxiv.org/abs/2506.17685)|[code](https://github.com/Ashayan97/SeqDG)|<details><summary>detail</summary>Pattern Recognition Letters</details>|\n", "When and How Does CLIP Enable Domain and Compositional Generalization?": "|**2025-6-20**|**When and How Does CLIP Enable Domain and Compositional Generalization?**|Elias Kempf et.al|[paper](https://arxiv.org/abs/2502.09507)|-|<details><summary>detail</summary>ICML 2025 (Spotlight)</details>|\n", "How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension": "|**2025-6-19**|**How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension**|Cynthia Dwork et.al|[paper](https://arxiv.org/abs/2506.16704)|-|-|\n", "GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation": "|**2025-6-19**|**GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation**|Yilin Xiao et.al|[paper](https://arxiv.org/abs/2506.02404)|-|-|\n", "Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization": "|**2025-6-19**|**Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization**|Jiyao Wang et.al|[paper](https://arxiv.org/abs/2506.16160)|-|-|\n", "Video-Guided Text-to-Music Generation Using Public Domain Movie Collections": "|**2025-6-18**|**Video-Guided Text-to-Music Generation Using Public Domain Movie Collections**|Haven Kim et.al|[paper](https://arxiv.org/abs/2506.12573)|[code](https://havenpersona.github.io/ossl-v1.)|<details><summary>detail</summary>ISMIR 2025 regular paper</details>|\n", "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation": "|**2025-6-18**|**Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2506.09881)|[code](https://github.com/anonymouse-9c53tp182bvz/Vireo.)|-|\n", "Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains": "|**2025-6-17**|**Controlling Context: Generative AI at Work in Integrated Circuit Design and Other High-Precision Domains**|Emanuel Moss et.al|[paper](https://arxiv.org/abs/2506.14567)|-|-|\n", "From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents": "|**2025-6-17**|**From What to Respond to When to Respond: Timely Response Generation for Open-domain Dialogue Agents**|Seongbo Jang et.al|[paper](https://arxiv.org/abs/2506.14285)|-|<details><summary>detail</summary>Work in progress</details>|\n", "Gradient-Guided Annealing for Domain Generalization": "|**2025-6-15**|**Gradient-Guided Annealing for Domain Generalization**|Aristotelis Ballas et.al|[paper](https://arxiv.org/abs/2502.20162)|-|<details><summary>detail</summary>Paper accepted in CVPR2025</details>|\n"}, "vision language": {"Unified Vision-Language-Action Model": "|**2025-6-24**|**Unified Vision-Language-Action Model**|Yuqi Wang et.al|[paper](https://arxiv.org/abs/2506.19850)|-|<details><summary>detail</summary>technical report</details>|\n", "Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models": "|**2025-6-24**|**Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models**|Johannes R\u00fcckert et.al|[paper](https://arxiv.org/abs/2506.19825)|-|<details><summary>detail</summary>ICDAR 2025</details>|\n", "FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models": "|**2025-6-24**|**FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models**|Xinting Liao et.al|[paper](https://arxiv.org/abs/2506.16218)|-|<details><summary>detail</summary>Accepted by ICML25</details>|\n", "PEVLM: Parallel Encoding for Vision-Language Models": "|**2025-6-24**|**PEVLM: Parallel Encoding for Vision-Language Models**|Letian Kang et.al|[paper](https://arxiv.org/abs/2506.19651)|-|-|\n", "Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects": "|**2025-6-24**|**Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects**|Federico Tavella et.al|[paper](https://arxiv.org/abs/2506.19579)|-|-|\n", "Visual hallucination detection in large vision-language models via evidential conflict": "|**2025-6-24**|**Visual hallucination detection in large vision-language models via evidential conflict**|Tao Huang et.al|[paper](https://arxiv.org/abs/2506.19513)|[code](https://github.com/HT86159/Evidential-Conflict.)|<details><summary>detail</summary>Journal ref:International Journal of Approximate Reasoning</details>|\n", "T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models": "|**2025-6-24**|**T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models**|Yiteng Chen et.al|[paper](https://arxiv.org/abs/2506.19498)|-|<details><summary>detail</summary>submitted to NeurIPS 2025</details>|\n", "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System": "|**2025-6-24**|**Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System**|Lixuan He et.al|[paper](https://arxiv.org/abs/2506.19433)|[code](https://github.com/tsinghua-fib-lab/Mem4Nav.)|-|\n", "Emergence of Text Readability in Vision Language Models": "|**2025-6-24**|**Emergence of Text Readability in Vision Language Models**|Jaeyoo Park et.al|[paper](https://arxiv.org/abs/2506.19389)|-|<details><summary>detail</summary>EVAL-FoMo Workshop @ CVPR 2025</details>|\n", "Continual Retinal Vision-Language Pre-training upon Incremental Imaging Modalities": "|**2025-6-24**|**Continual Retinal Vision-Language Pre-training upon Incremental Imaging Modalities**|Yuang Yao et.al|[paper](https://arxiv.org/abs/2506.19320)|[code](https://github.com/Yuang-Yao/RetCoP.)|<details><summary>detail</summary>Accepted by MICCAI 2025</details>|\n", "Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference": "|**2025-6-24**|**Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference**|Zexiang Guo et.al|[paper](https://arxiv.org/abs/2506.19303)|-|<details><summary>detail</summary>This paper has been accepted by the 2025 International Conference on Climbing and Walking Robots (CLAWAR)</details>|\n", "Open-Vocabulary Camouflaged Object Segmentation with Cascaded Vision Language Models": "|**2025-6-24**|**Open-Vocabulary Camouflaged Object Segmentation with Cascaded Vision Language Models**|Kai Zhao et.al|[paper](https://arxiv.org/abs/2506.19300)|-|-|\n", "ClimateIQA: A New Dataset and Benchmark to Advance Vision-Language Models in Meteorology Anomalies Analysis": "|**2025-6-23**|**ClimateIQA: A New Dataset and Benchmark to Advance Vision-Language Models in Meteorology Anomalies Analysis**|Jian Chen et.al|[paper](https://arxiv.org/abs/2406.09838)|-|-|\n", "MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models": "|**2025-6-23**|**MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models**|Yinan Xia et.al|[paper](https://arxiv.org/abs/2506.19257)|[code](https://huggingface.co/datasets/Leigest/MSR-Align.)|-|\n", "Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models": "|**2025-6-23**|**Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models**|Xuelin Shen et.al|[paper](https://arxiv.org/abs/2506.15201)|-|-|\n"}}