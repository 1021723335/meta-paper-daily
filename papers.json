{"source-free": {"Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-7-28**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning": "|**2025-7-26**|**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**|Sanqing Qu et.al|[paper](https://arxiv.org/abs/2403.14410)|[code](https://github.com/ispc-lab/GLC-plus.)|<details><summary>detail</summary>A substantial extension of the CVPR paper \"Upcycling Models under Domain and Category Shift\"</details>|\n", "SFUOD: Source-Free Unknown Object Detection": "|**2025-7-23**|**SFUOD: Source-Free Unknown Object Detection**|Keon-Hee Park et.al|[paper](https://arxiv.org/abs/2507.17373)|-|<details><summary>detail</summary>This paper has been accepted by ICCV 2025</details>|\n", "Model-Free and Real-Time Bioinspired Unicycle-Based Source Seeking: Differential Wheeled Robotic Experiments": "|**2025-7-21**|**Model-Free and Real-Time Bioinspired Unicycle-Based Source Seeking: Differential Wheeled Robotic Experiments**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2501.02184)|-|-|\n", "Text-Driven Causal Representation Learning for Source-Free Domain Generalization": "|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|\n", "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting": "|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|\n", "Source-Free Domain Adaptation via Multi-view Contrastive Learning": "|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|\n", "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "SFDLA: Source-Free Document Layout Analysis": "|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|\n", "Unified Source-Free Domain Adaptation": "|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|\n", "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|\n", "DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization": "|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|\n", "Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data": "|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|\n", "Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation": "|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|\n"}, "object detection": {"Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy": "|**2025-7-30**|**Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy**|Jicheng Yuan et.al|[paper](https://arxiv.org/abs/2507.21358)|[code](https://github.com/jichengyuan/Collaborative-Perceiver.)|<details><summary>detail</summary>The manuscript has been accepted by ICONIP2025</details>|\n", "RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection": "|**2025-7-30**|**RaGS: Unleashing 3D Gaussian Splatting from 4D Radar and Monocular Cues for 3D Object Detection**|Xiaokai Bai et.al|[paper](https://arxiv.org/abs/2507.19856)|-|-|\n", "Interpretable Open-Vocabulary Referring Object Detection with Reverse Contrast Attention": "|**2025-7-30**|**Interpretable Open-Vocabulary Referring Object Detection with Reverse Contrast Attention**|Drandreb Earl O. Juanico et.al|[paper](https://arxiv.org/abs/2507.19891)|[code](https://github.com/earl-juanico/rca)|<details><summary>detail</summary>To be published in the ICCVW 2025 Proceedings</details>|\n", "DriveIndia: An Object Detection Dataset for Diverse Indian Traffic Scenes": "|**2025-7-29**|**DriveIndia: An Object Detection Dataset for Diverse Indian Traffic Scenes**|Rishav Kumar et.al|[paper](https://arxiv.org/abs/2507.19912)|[code](https://tihan.iith.ac.in/tiand-datasets/).)|<details><summary>detail</summary>ITSC 2025 Conference</details>|\n", "Unleashing the Power of Motion and Depth: A Selective Fusion Strategy for RGB-D Video Salient Object Detection": "|**2025-7-29**|**Unleashing the Power of Motion and Depth: A Selective Fusion Strategy for RGB-D Video Salient Object Detection**|Jiahao He et.al|[paper](https://arxiv.org/abs/2507.21857)|[code](https://github.com/Jia-hao999/SMFNet.)|<details><summary>detail</summary>submitted to TMM on 11-Jun-2024</details>|\n", "YOLO-PRO: Enhancing Instance-Specific Object Detection with Full-Channel Global Self-Attention": "|**2025-7-29**|**YOLO-PRO: Enhancing Instance-Specific Object Detection with Full-Channel Global Self-Attention**|Lin Huang et.al|[paper](https://arxiv.org/abs/2503.02348)|-|-|\n", "Tracking Moose using Aerial Object Detection": "|**2025-7-28**|**Tracking Moose using Aerial Object Detection**|Christopher Indris et.al|[paper](https://arxiv.org/abs/2507.21256)|[code](https://github.com/chrisindris/Moose.)|-|\n", "Rethinking Multi-Modal Object Detection from the Perspective of Mono-Modality Feature Learning": "|**2025-7-28**|**Rethinking Multi-Modal Object Detection from the Perspective of Mono-Modality Feature Learning**|Tianyi Zhao et.al|[paper](https://arxiv.org/abs/2503.11780)|[code](https://github.com/Zhao-Tian-yi/M2D-LIF.)|-|\n", "Hoi2Threat: An Interpretable Threat Detection Method for Human Violence Scenarios Guided by Human-Object Interaction": "|**2025-7-28**|**Hoi2Threat: An Interpretable Threat Detection Method for Human Violence Scenarios Guided by Human-Object Interaction**|Yuhan Wang et.al|[paper](https://arxiv.org/abs/2503.10508)|-|-|\n", "Synthetic-to-Real Camouflaged Object Detection": "|**2025-7-28**|**Synthetic-to-Real Camouflaged Object Detection**|Zhihao Luo et.al|[paper](https://arxiv.org/abs/2507.18911)|[code](https://github.com/Muscape/S2R-COD.)|-|\n", "Wavelet-guided Misalignment-aware Network for Visible-Infrared Object Detection": "|**2025-7-27**|**Wavelet-guided Misalignment-aware Network for Visible-Infrared Object Detection**|Haote Zhang et.al|[paper](https://arxiv.org/abs/2507.20146)|-|-|\n", "OW-CLIP: Data-Efficient Visual Supervision for Open-World Object Detection via Human-AI Collaboration": "|**2025-7-26**|**OW-CLIP: Data-Efficient Visual Supervision for Open-World Object Detection via Human-AI Collaboration**|Junwen Duan et.al|[paper](https://arxiv.org/abs/2507.19870)|-|-|\n", "DS-Det: Single-Query Paradigm and Attention Disentangled Learning for Flexible Object Detection": "|**2025-7-26**|**DS-Det: Single-Query Paradigm and Attention Disentangled Learning for Flexible Object Detection**|Guiping Cao et.al|[paper](https://arxiv.org/abs/2507.19807)|[code](https://github.com/Med-Process/DS-Det/.)|<details><summary>detail</summary>Journal ref:The 33rd ACM International Conference on Multimedia 2025</details>|\n", "TransFlow: Motion Knowledge Transfer from Video Diffusion Models to Video Salient Object Detection": "|**2025-7-26**|**TransFlow: Motion Knowledge Transfer from Video Diffusion Models to Video Salient Object Detection**|Suhwan Cho et.al|[paper](https://arxiv.org/abs/2507.19789)|-|<details><summary>detail</summary>ICCVW 2025</details>|\n", "Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing": "|**2025-7-25**|**Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing**|Haichuan Li et.al|[paper](https://arxiv.org/abs/2507.19691)|-|-|\n"}, "domain adaptation": {"A Unified Analysis of Generalization and Sample Complexity for Semi-Supervised Domain Adaptation": "|**2025-7-30**|**A Unified Analysis of Generalization and Sample Complexity for Semi-Supervised Domain Adaptation**|Elif Vural et.al|[paper](https://arxiv.org/abs/2507.22632)|-|-|\n", "From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras": "|**2025-7-30**|**From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras**|Youngho Kim et.al|[paper](https://arxiv.org/abs/2507.22438)|[code](https://github.com/kmax2001/EvSharp2Blur.)|-|\n", "Learning from Heterogeneous Structural MRI via Collaborative Domain Adaptation for Late-Life Depression Assessment": "|**2025-7-29**|**Learning from Heterogeneous Structural MRI via Collaborative Domain Adaptation for Late-Life Depression Assessment**|Yuzhen Gao et.al|[paper](https://arxiv.org/abs/2507.22321)|-|-|\n", "Domain Generalization and Adaptation in Intensive Care with Anchor Regression": "|**2025-7-29**|**Domain Generalization and Adaptation in Intensive Care with Anchor Regression**|Malte Londschien et.al|[paper](https://arxiv.org/abs/2507.21783)|-|-|\n", "GDAIP: A Graph-Based Domain Adaptive Framework for Individual Brain Parcellation": "|**2025-7-29**|**GDAIP: A Graph-Based Domain Adaptive Framework for Individual Brain Parcellation**|Jianfei Zhu et.al|[paper](https://arxiv.org/abs/2507.21727)|-|-|\n", "ST-DAI: Single-shot 2.5D Spatial Transcriptomics with Intra-Sample Domain Adaptive Imputation for Cost-efficient 3D Reconstruction": "|**2025-7-29**|**ST-DAI: Single-shot 2.5D Spatial Transcriptomics with Intra-Sample Domain Adaptive Imputation for Cost-efficient 3D Reconstruction**|Jiahe Qian et.al|[paper](https://arxiv.org/abs/2507.21516)|-|-|\n", "Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision": "|**2025-7-28**|**Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision**|Xiao Fang et.al|[paper](https://arxiv.org/abs/2507.20976)|[code](https://humansensinglab.github.io/AGenDA)|<details><summary>detail</summary>ICCV 2025</details>|\n", "From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation": "|**2025-7-28**|**From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation**|Rongyao Cai et.al|[paper](https://arxiv.org/abs/2507.20968)|-|-|\n", "Partial Domain Adaptation via Importance Sampling-based Shift Correction": "|**2025-7-27**|**Partial Domain Adaptation via Importance Sampling-based Shift Correction**|Cheng-Jun Guo et.al|[paper](https://arxiv.org/abs/2507.20191)|-|-|\n", "GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning": "|**2025-7-26**|**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**|Sanqing Qu et.al|[paper](https://arxiv.org/abs/2403.14410)|[code](https://github.com/ispc-lab/GLC-plus.)|<details><summary>detail</summary>A substantial extension of the CVPR paper \"Upcycling Models under Domain and Category Shift\"</details>|\n", "Deep Unsupervised Domain Adaptation for Time Series Classification: a Benchmark": "|**2025-7-25**|**Deep Unsupervised Domain Adaptation for Time Series Classification: a Benchmark**|Hassan Ismail Fawaz et.al|[paper](https://arxiv.org/abs/2312.09857)|[code](https://github.com/EricssonResearch/UDA-4-TSC.)|<details><summary>detail</summary>Published in Data Mining and Knowledge Discovery</details>|\n", "Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection": "|**2025-7-25**|**Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection**|Jianhong Han et.al|[paper](https://arxiv.org/abs/2504.20498)|-|<details><summary>detail</summary>Manuscript submitted to IEEE Transactions on Circuits and Systems for Video Technology</details>|\n", "SIDA: Synthetic Image Driven Zero-shot Domain Adaptation": "|**2025-7-24**|**SIDA: Synthetic Image Driven Zero-shot Domain Adaptation**|Ye-Chan Kim et.al|[paper](https://arxiv.org/abs/2507.18632)|-|<details><summary>detail</summary>ACM MM 2025</details>|\n", "crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023": "|**2025-7-24**|**crossMoDA Challenge: Evolution of Cross-Modality Domain Adaptation Techniques for Vestibular Schwannoma and Cochlea Segmentation from 2021 to 2023**|Navodini Wijethilake et.al|[paper](https://arxiv.org/abs/2506.12006)|-|-|\n", "Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder": "|**2025-7-24**|**Att-Adapter: A Robust and Precise Domain-Specific Multi-Attributes T2I Diffusion Adapter via Conditional Variational Autoencoder**|Wonwoong Cho et.al|[paper](https://arxiv.org/abs/2503.11937)|[code](https://tri-mac.github.io/att-adapter/)|<details><summary>detail</summary>ICCV'25 (Highlight)</details>|\n"}, "domain generalization": {"A Unified Analysis of Generalization and Sample Complexity for Semi-Supervised Domain Adaptation": "|**2025-7-30**|**A Unified Analysis of Generalization and Sample Complexity for Semi-Supervised Domain Adaptation**|Elif Vural et.al|[paper](https://arxiv.org/abs/2507.22632)|-|-|\n", "Domain Generalization and Adaptation in Intensive Care with Anchor Regression": "|**2025-7-29**|**Domain Generalization and Adaptation in Intensive Care with Anchor Regression**|Malte Londschien et.al|[paper](https://arxiv.org/abs/2507.21783)|-|-|\n", "Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation": "|**2025-7-28**|**Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation**|I-Hsiang Chen et.al|[paper](https://arxiv.org/abs/2507.21367)|-|<details><summary>detail</summary>Accepted by ICCV2025</details>|\n", "FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving": "|**2025-7-26**|**FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving**|Tao Lian et.al|[paper](https://arxiv.org/abs/2507.19881)|-|-|\n", "PennyCoder: Efficient Domain-Specific LLMs for PennyLane-Based Quantum Code Generation": "|**2025-7-25**|**PennyCoder: Efficient Domain-Specific LLMs for PennyLane-Based Quantum Code Generation**|Abdul Basit et.al|[paper](https://arxiv.org/abs/2507.19562)|-|-|\n", "From Continuous to Discrete: Cross-Domain Collaborative General Speech Enhancement via Hierarchical Language Models": "|**2025-7-25**|**From Continuous to Discrete: Cross-Domain Collaborative General Speech Enhancement via Hierarchical Language Models**|Zhaoxi Mu et.al|[paper](https://arxiv.org/abs/2507.19062)|-|<details><summary>detail</summary>ACMMM 2025</details>|\n", "Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection": "|**2025-7-25**|**Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection**|Jianhong Han et.al|[paper](https://arxiv.org/abs/2504.20498)|-|<details><summary>detail</summary>Manuscript submitted to IEEE Transactions on Circuits and Systems for Video Technology</details>|\n", "Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards": "|**2025-7-24**|**Omni-Thinker: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards**|Derek Li et.al|[paper](https://arxiv.org/abs/2507.14783)|-|-|\n", "VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks": "|**2025-7-24**|**VolDoGer: LLM-assisted Datasets for Domain Generalization in Vision-Language Tasks**|Juhwan Choi et.al|[paper](https://arxiv.org/abs/2407.19795)|-|<details><summary>detail</summary>ICCV 2025 Workshop on Curated Data for Efficient Learning (CDEL)</details>|\n", "Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation": "|**2025-7-23**|**Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation**|Huanli Zhuo et.al|[paper](https://arxiv.org/abs/2507.17281)|-|<details><summary>detail</summary>This manuscript has been accepted for presentation at the IEEE International Conference on Systems</details>|\n", "Gradient-Guided Annealing for Domain Generalization": "|**2025-7-21**|**Gradient-Guided Annealing for Domain Generalization**|Aristotelis Ballas et.al|[paper](https://arxiv.org/abs/2502.20162)|-|<details><summary>detail</summary>Paper accepted in CVPR2025</details>|\n", "DGSSA: Domain generalization with structural and stylistic augmentation for retinal vessel segmentation": "|**2025-7-20**|**DGSSA: Domain generalization with structural and stylistic augmentation for retinal vessel segmentation**|Bo Liu et.al|[paper](https://arxiv.org/abs/2501.03466)|-|-|\n", "Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification": "|**2025-7-19**|**Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification**|Subhendu Khatuya et.al|[paper](https://arxiv.org/abs/2506.06806)|-|<details><summary>detail</summary>This work has been accepted to appear at the Association for Computational Linguistics (ACL)</details>|\n", "VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization": "|**2025-7-18**|**VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization**|Ayan Biswas et.al|[paper](https://arxiv.org/abs/2507.21124)|-|-|\n", "Generative Multi-Target Cross-Domain Recommendation": "|**2025-7-17**|**Generative Multi-Target Cross-Domain Recommendation**|Jinqiu Jin et.al|[paper](https://arxiv.org/abs/2507.12871)|-|<details><summary>detail</summary>fix author information</details>|\n"}, "vision language": {"Application of Vision-Language Model to Pedestrians Behavior and Scene Understanding in Autonomous Driving": "|**2025-7-30**|**Application of Vision-Language Model to Pedestrians Behavior and Scene Understanding in Autonomous Driving**|Haoxiang Gao et.al|[paper](https://arxiv.org/abs/2501.06680)|-|-|\n", "FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models": "|**2025-7-30**|**FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models**|Xinting Liao et.al|[paper](https://arxiv.org/abs/2506.16218)|-|<details><summary>detail</summary>Accepted by ICML25</details>|\n", "CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models": "|**2025-7-30**|**CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models**|Kedong Xiu et.al|[paper](https://arxiv.org/abs/2507.22828)|-|-|\n", "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention": "|**2025-7-30**|**MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention**|Yuqi Pang et.al|[paper](https://arxiv.org/abs/2507.22805)|-|-|\n", "CLIP-HandID: Vision-Language Model for Hand-Based Person Identification": "|**2025-7-30**|**CLIP-HandID: Vision-Language Model for Hand-Based Person Identification**|Nathanael L. Baisa et.al|[paper](https://arxiv.org/abs/2506.12447)|-|-|\n", "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions": "|**2025-7-30**|**Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions**|Akash Ghosh et.al|[paper](https://arxiv.org/abs/2404.07214)|-|<details><summary>detail</summary>One of the first survey on Visual Language Models</details>|\n", "HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models": "|**2025-7-30**|**HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models**|Zhixiang Wei et.al|[paper](https://arxiv.org/abs/2507.22431)|[code](https://zxwei.site/hqclip.)|-|\n", "Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance": "|**2025-7-30**|**Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance**|Songsheng Wang et.al|[paper](https://arxiv.org/abs/2507.22424)|-|-|\n", "On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations": "|**2025-7-30**|**On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations**|Jordan Vice et.al|[paper](https://arxiv.org/abs/2507.22398)|-|<details><summary>detail</summary>Keywords: Vision-Language Models</details>|\n", "Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding": "|**2025-7-29**|**Invisible Injections: Exploiting Vision-Language Models Through Steganographic Prompt Embedding**|Chetan Pathade et.al|[paper](https://arxiv.org/abs/2507.22304)|-|<details><summary>detail</summary>14 Pages</details>|\n", "SmartCLIP: Modular Vision-language Alignment with Identification Guarantees": "|**2025-7-29**|**SmartCLIP: Modular Vision-language Alignment with Identification Guarantees**|Shaoan Xie et.al|[paper](https://arxiv.org/abs/2507.22264)|[code](https://github.com/Mid-Push/SmartCLIP.)|<details><summary>detail</summary>CVPR2025</details>|\n", "Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training": "|**2025-7-29**|**Positive-Augmented Contrastive Learning for Vision-and-Language Evaluation and Training**|Sara Sarto et.al|[paper](https://arxiv.org/abs/2410.07336)|[code](https://github.com/aimagelab/pacscore.)|<details><summary>detail</summary>International Journal of Computer Vision (2025)</details>|\n", "The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?": "|**2025-7-29**|**The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?**|Dinh Nam Pham et.al|[paper](https://arxiv.org/abs/2507.20884)|-|<details><summary>detail</summary>9th International Workshop on Sign Language Translation and Avatar Technologies @ ACM IVA'25</details>|\n", "GLIMPSE: Holistic Cross-Modal Explainability for Large Vision-Language Models": "|**2025-7-29**|**GLIMPSE: Holistic Cross-Modal Explainability for Large Vision-Language Models**|Guanxi Shen et.al|[paper](https://arxiv.org/abs/2506.18985)|-|<details><summary>detail</summary>Keywords: Explainable Computer Vision</details>|\n", "Cardiac-CLIP: A Vision-Language Foundation Model for 3D Cardiac CT Images": "|**2025-7-29**|**Cardiac-CLIP: A Vision-Language Foundation Model for 3D Cardiac CT Images**|Yutao Hu et.al|[paper](https://arxiv.org/abs/2507.22024)|-|-|\n"}}