{"source-free": {"Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation": "|**2026-2-9**|**Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation**|Shanshan Wang et.al|[paper](https://arxiv.org/abs/2602.08730)|[code](https://github.com/soloiro/CGA)|-|\n", "USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation": "|**2026-2-9**|**USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation**|Yingxu Wang et.al|[paper](https://arxiv.org/abs/2602.08431)|-|-|\n", "Rethinking Test-Time Training: Tilting The Latent Distribution For Few-Shot Source-Free Adaptation": "|**2026-2-2**|**Rethinking Test-Time Training: Tilting The Latent Distribution For Few-Shot Source-Free Adaptation**|Tahir Qasim Syed et.al|[paper](https://arxiv.org/abs/2602.02633)|-|-|\n", "Collision-free Source Seeking and Flocking Control of Multi-agents with Connectivity Preservation": "|**2026-1-30**|**Collision-free Source Seeking and Flocking Control of Multi-agents with Connectivity Preservation**|Tinghua Li et.al|[paper](https://arxiv.org/abs/2301.04576)|-|<details><summary>detail</summary>Published in IEEE Transactions on Automatic Control</details>|\n", "Source Coding with Free Bits and the Multi-Way Number Partitioning Problem": "|**2026-1-29**|**Source Coding with Free Bits and the Multi-Way Number Partitioning Problem**|Niloufar Ahmadypour et.al|[paper](https://arxiv.org/abs/2009.02710)|-|-|\n", "Beyond Retraining: Training-Free Unknown Class Filtering for Source-Free Open Set Domain Adaptation of Vision-Language Models": "|**2026-1-28**|**Beyond Retraining: Training-Free Unknown Class Filtering for Source-Free Open Set Domain Adaptation of Vision-Language Models**|Yongguang Li et.al|[paper](https://arxiv.org/abs/2504.14224)|-|<details><summary>detail</summary>Core methods unchanged</details>|\n", "A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency": "|**2026-1-28**|**A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency**|Debopom Sutradhar et.al|[paper](https://arxiv.org/abs/2601.20284)|-|<details><summary>detail</summary>Manuscript under review in IEEE Transactions on Image Processing</details>|\n", "Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity": "|**2026-1-24**|**Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity**|Harsharaj Pathak et.al|[paper](https://arxiv.org/abs/2601.17408)|-|-|\n", "Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results": "|**2026-1-23**|**Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results**|Rohan Palanikumar et.al|[paper](https://arxiv.org/abs/2511.00752)|-|-|\n", "Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection": "|**2026-1-20**|**Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection**|Huizai Yao et.al|[paper](https://arxiv.org/abs/2511.07301)|-|<details><summary>detail</summary>AAAI 2026</details>|\n", "Towards Unbiased Source-Free Object Detection via Vision Foundation Models": "|**2026-1-19**|**Towards Unbiased Source-Free Object Detection via Vision Foundation Models**|Zhi Cai et.al|[paper](https://arxiv.org/abs/2601.12765)|-|-|\n", "Unified Source-Free Domain Adaptation": "|**2026-1-18**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|[code](https://github.com/tntek/CausalDA.)|-|\n", "GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling": "|**2026-1-16**|**GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2601.11161)|[code](https://github.com/pascalschlachter/GMM-COMET.)|-|\n", "SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling": "|**2026-1-13**|**SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling**|Xi Chen et.al|[paper](https://arxiv.org/abs/2601.08608)|[code](https://github.com/chenxi52/SfMamba.)|-|\n", "Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation": "|**2026-1-13**|**Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation**|Yuan Gao et.al|[paper](https://arxiv.org/abs/2601.08375)|-|-|\n"}, "object detection": {"RAWDet-7: A Multi-Scenario Benchmark for Object Detection and Description on Quantized RAW Images": "|**2026-2-10**|**RAWDet-7: A Multi-Scenario Benchmark for Object Detection and Description on Quantized RAW Images**|Mishal Fatima et.al|[paper](https://arxiv.org/abs/2602.03760)|-|<details><summary>detail</summary>*Equal Contribution</details>|\n", "Energy-Efficient Fast Object Detection on Edge Devices for IoT Systems": "|**2026-2-10**|**Energy-Efficient Fast Object Detection on Edge Devices for IoT Systems**|Mas Nurul Achmadiah et.al|[paper](https://arxiv.org/abs/2602.09515)|-|-|\n", "ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection": "|**2026-2-9**|**ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection**|Janghyun Baek et.al|[paper](https://arxiv.org/abs/2512.18187)|-|-|\n", "MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection": "|**2026-2-8**|**MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection**|Venkatraman Narayanan et.al|[paper](https://arxiv.org/abs/2602.08126)|-|-|\n", "Mamba-based Spatio-Frequency Motion Perception for Video Camouflaged Object Detection": "|**2026-2-7**|**Mamba-based Spatio-Frequency Motion Perception for Video Camouflaged Object Detection**|Xin Li et.al|[paper](https://arxiv.org/abs/2507.23601)|[code](https://github.com/BoydeLi/Vcamba.)|-|\n", "Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection": "|**2026-2-7**|**Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection**|Tao Wang et.al|[paper](https://arxiv.org/abs/2602.07512)|[code](https://github.com/twangnh/zoomdet_code.)|<details><summary>detail</summary>paper accepted by ISPRS Journal of Photogrammetry and Remote Sensing ( IF=12</details>|\n", "You Only Pose Once: A Minimalist's Detection Transformer for Monocular RGB Category-level 9D Multi-Object Pose Estimation": "|**2026-2-7**|**You Only Pose Once: A Minimalist's Detection Transformer for Monocular RGB Category-level 9D Multi-Object Pose Estimation**|Hakjin Lee et.al|[paper](https://arxiv.org/abs/2508.14965)|[code](https://mikigom.github.io/YOPO-project-page.)|<details><summary>detail</summary>This paper has been accepted by IEEE ICRA 2026</details>|\n", "BADet: Boundary-Aware 3D Object Detection from Point Clouds": "|**2026-2-6**|**BADet: Boundary-Aware 3D Object Detection from Point Clouds**|Rui Qian et.al|[paper](https://arxiv.org/abs/2104.10330)|[code](https://github.com/rui-qian/BADet.)|<details><summary>detail</summary>The manuscript is accepted by Pattern Recognition on 6 Jan</details>|\n", "3D Object Detection for Autonomous Driving: A Survey": "|**2026-2-6**|**3D Object Detection for Autonomous Driving: A Survey**|Rui Qian et.al|[paper](https://arxiv.org/abs/2106.10823)|-|<details><summary>detail</summary>The manuscript is accepted by Pattern Recognition on 14 May 2022</details>|\n", "M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene, Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection": "|**2026-2-6**|**M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene, Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection**|Chao Wang et.al|[paper](https://arxiv.org/abs/2505.10931)|[code](https://github.com/wchao0601/M4-SAR.)|-|\n", "Instance-Free Domain Adaptive Object Detection": "|**2026-2-6**|**Instance-Free Domain Adaptive Object Detection**|Hengfu Yu et.al|[paper](https://arxiv.org/abs/2602.06484)|-|-|\n", "LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection": "|**2026-2-6**|**LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection**|Xu Zhang et.al|[paper](https://arxiv.org/abs/2602.06474)|-|-|\n", "Revisiting Salient Object Detection from an Observer-Centric Perspective": "|**2026-2-5**|**Revisiting Salient Object Detection from an Observer-Centric Perspective**|Fuxi Zhang et.al|[paper](https://arxiv.org/abs/2602.06369)|[code](https://github.com/Dustzx/OC_SOD)|-|\n", "Depth as Prior Knowledge for Object Detection": "|**2026-2-5**|**Depth as Prior Knowledge for Object Detection**|Moussa Kassem Sbeyti et.al|[paper](https://arxiv.org/abs/2602.05730)|[code](https://github.com/mos-ks/DepthPrior.)|<details><summary>detail</summary>This work has been submitted to the IEEE for possible publication</details>|\n", "All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles": "|**2026-2-4**|**All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles**|Sayed Pedram Haeri Boroujeni et.al|[paper](https://arxiv.org/abs/2510.26641)|-|-|\n"}, "domain adaptation": {"Impact of domain adaptation in deep learning for medical image classifications": "|**2026-2-9**|**Impact of domain adaptation in deep learning for medical image classifications**|Yihang Wu et.al|[paper](https://arxiv.org/abs/2602.09355)|-|<details><summary>detail</summary>Accepted in IEEE SMC 2025</details>|\n", "Pave Your Own Path: Graph Gradual Domain Adaptation on Fused Gromov-Wasserstein Geodesics": "|**2026-2-9**|**Pave Your Own Path: Graph Gradual Domain Adaptation on Fused Gromov-Wasserstein Geodesics**|Zhichen Zeng et.al|[paper](https://arxiv.org/abs/2505.12709)|-|-|\n", "Harvest: Adaptive Photonic Switching Schedules for Collective Communication in Scale-up Domains": "|**2026-2-9**|**Harvest: Adaptive Photonic Switching Schedules for Collective Communication in Scale-up Domains**|Mahir Rahman et.al|[paper](https://arxiv.org/abs/2602.09188)|-|-|\n", "Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation": "|**2026-2-9**|**Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation**|Shanshan Wang et.al|[paper](https://arxiv.org/abs/2602.08730)|[code](https://github.com/soloiro/CGA)|-|\n", "USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation": "|**2026-2-9**|**USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation**|Yingxu Wang et.al|[paper](https://arxiv.org/abs/2602.08431)|-|-|\n", "Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution": "|**2026-2-8**|**Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution**|Shravan Chaudhari et.al|[paper](https://arxiv.org/abs/2512.01152)|-|-|\n", "Graph Domain Adaptation via Homophily-Agnostic Reconstructing Structure": "|**2026-2-7**|**Graph Domain Adaptation via Homophily-Agnostic Reconstructing Structure**|Ruiyi Fang et.al|[paper](https://arxiv.org/abs/2602.07573)|-|<details><summary>detail</summary>Accept by AAAI2026(oral)</details>|\n", "Provable Domain Adaptation for Offline Reinforcement Learning with Limited Samples": "|**2026-2-6**|**Provable Domain Adaptation for Offline Reinforcement Learning with Limited Samples**|Weiqin Chen et.al|[paper](https://arxiv.org/abs/2408.12136)|-|-|\n", "Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling": "|**2026-2-6**|**Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling**|Kate Sanders et.al|[paper](https://arxiv.org/abs/2602.06795)|-|-|\n", "An Example for Domain Adaptation Using CycleGAN": "|**2026-2-6**|**An Example for Domain Adaptation Using CycleGAN**|Yanhua Zhao et.al|[paper](https://arxiv.org/abs/2601.08776)|-|-|\n", "Instance-Free Domain Adaptive Object Detection": "|**2026-2-6**|**Instance-Free Domain Adaptive Object Detection**|Hengfu Yu et.al|[paper](https://arxiv.org/abs/2602.06484)|-|-|\n", "Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation": "|**2026-2-5**|**Training Data Selection with Gradient Orthogonality for Efficient Domain Adaptation**|Xiyang Zhang et.al|[paper](https://arxiv.org/abs/2602.06359)|-|-|\n", "Adaptive and Balanced Re-initialization for Long-timescale Continual Test-time Domain Adaptation": "|**2026-2-5**|**Adaptive and Balanced Re-initialization for Long-timescale Continual Test-time Domain Adaptation**|Yanshuo Wang et.al|[paper](https://arxiv.org/abs/2602.06328)|-|<details><summary>detail</summary>Accepted in ICASSP 2026</details>|\n", "Cross-Domain Offline Policy Adaptation via Selective Transition Correction": "|**2026-2-5**|**Cross-Domain Offline Policy Adaptation via Selective Transition Correction**|Mengbei Yan et.al|[paper](https://arxiv.org/abs/2602.05776)|-|-|\n", "Fast Private Adaptive Query Answering for Large Data Domains": "|**2026-2-5**|**Fast Private Adaptive Query Answering for Large Data Domains**|Miguel Fuentes et.al|[paper](https://arxiv.org/abs/2602.05674)|-|-|\n"}, "domain generalization": {"Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models": "|**2026-2-10**|**Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models**|Mingzi Cao et.al|[paper](https://arxiv.org/abs/2602.08658)|-|-|\n", "Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation": "|**2026-2-10**|**Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2602.09648)|-|-|\n", "Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains": "|**2026-2-9**|**Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains**|Jaesung Bae et.al|[paper](https://arxiv.org/abs/2602.02841)|-|-|\n", "TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation": "|**2026-2-9**|**TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation**|Yiyang Cao et.al|[paper](https://arxiv.org/abs/2602.08462)|[code](https://caoyiyang1105.github.io/TriC-Motion/.)|-|\n", "DRAGON: Domain-specific Robust Automatic Data Generation for RAG Optimization": "|**2026-2-8**|**DRAGON: Domain-specific Robust Automatic Data Generation for RAG Optimization**|Haiyang Shen et.al|[paper](https://arxiv.org/abs/2505.10989)|-|-|\n", "Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling": "|**2026-2-6**|**Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling**|Kate Sanders et.al|[paper](https://arxiv.org/abs/2602.06795)|-|-|\n", "Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains": "|**2026-2-6**|**Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains**|Ben Isselmann et.al|[paper](https://arxiv.org/abs/2602.05527)|-|<details><summary>detail</summary>Preprint</details>|\n", "KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation": "|**2026-2-6**|**KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation**|Anji Li et.al|[paper](https://arxiv.org/abs/2511.14224)|-|<details><summary>detail</summary>the 48th International Conference on Software Engineering(ICSE 2026)</details>|\n", "LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection": "|**2026-2-6**|**LAB-Det: Language as a Domain-Invariant Bridge for Training-Free One-Shot Domain Generalization in Object Detection**|Xu Zhang et.al|[paper](https://arxiv.org/abs/2602.06474)|-|-|\n", "Domain Generalization Under Posterior Drift": "|**2026-2-4**|**Domain Generalization Under Posterior Drift**|Yilun Zhu et.al|[paper](https://arxiv.org/abs/2510.04441)|-|-|\n", "PEPR: Privileged Event-based Predictive Regularization for Domain Generalization": "|**2026-2-4**|**PEPR: Privileged Event-based Predictive Regularization for Domain Generalization**|Gabriele Magrini et.al|[paper](https://arxiv.org/abs/2602.04583)|-|-|\n", "Self-CriTeach: LLM Self-Teaching and Self-Critiquing for Improving Robotic Planning via Automated Domain Generation": "|**2026-2-3**|**Self-CriTeach: LLM Self-Teaching and Self-Critiquing for Improving Robotic Planning via Automated Domain Generation**|Jinbang Huang et.al|[paper](https://arxiv.org/abs/2509.21543)|-|-|\n", "CiMRAG: CiM-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs": "|**2026-2-3**|**CiMRAG: CiM-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs**|Shih-Hsuan Chiu et.al|[paper](https://arxiv.org/abs/2601.20041)|-|<details><summary>detail</summary>Accepted by ICASSP 2026</details>|\n", "Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain": "|**2026-2-3**|**Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain**|Wei Zhu et.al|[paper](https://arxiv.org/abs/2602.03368)|-|-|\n", "Robust Domain Generalization under Divergent Marginal and Conditional Distributions": "|**2026-2-2**|**Robust Domain Generalization under Divergent Marginal and Conditional Distributions**|Jewon Yeom et.al|[paper](https://arxiv.org/abs/2602.02015)|-|-|\n"}, "vision language": {"ST4VLA: Spatially Guided Training for Vision-Language-Action Models": "|**2026-2-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**|Jinhui Ye et.al|[paper](https://arxiv.org/abs/2602.10109)|[code](https://internrobotics.github.io/internvla-m1.github.io/)|<details><summary>detail</summary>Spatially Training for VLA</details>|\n", "VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model": "|**2026-2-10**|**VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model**|Jingwen Sun et.al|[paper](https://arxiv.org/abs/2602.10098)|-|-|\n", "From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors": "|**2026-2-10**|**From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors**|Zhengshen Zhang et.al|[paper](https://arxiv.org/abs/2510.17439)|[code](https://falcon-vla.github.io/)|<details><summary>detail</summary>ICLR 2026</details>|\n", "Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection": "|**2026-2-10**|**Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection**|Changjiang Jiang et.al|[paper](https://arxiv.org/abs/2602.10042)|-|<details><summary>detail</summary>Accepted by ICASSP 2026</details>|\n", "Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance": "|**2026-2-10**|**Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance**|Xinrong Chen et.al|[paper](https://arxiv.org/abs/2602.01047)|-|-|\n", "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models": "|**2026-2-10**|**Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models**|Yu Zeng et.al|[paper](https://arxiv.org/abs/2510.01304)|[code](https://github.com/yuzeng0-0/AGILE)|-|\n", "BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation": "|**2026-2-10**|**BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation**|Yucheng Hu et.al|[paper](https://arxiv.org/abs/2602.09849)|-|-|\n", "SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding": "|**2026-2-10**|**SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding**|Zhaoxu Li et.al|[paper](https://arxiv.org/abs/2602.09825)|-|-|\n", "GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation": "|**2026-2-10**|**GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation**|Sandesh Hegde et.al|[paper](https://arxiv.org/abs/2602.09701)|-|-|\n", "AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild": "|**2026-2-10**|**AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild**|Xiaolou Sun et.al|[paper](https://arxiv.org/abs/2602.09657)|-|<details><summary>detail</summary>Acceped by ICLR 2026</details>|\n", "AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models": "|**2026-2-10**|**AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models**|Yue Li et.al|[paper](https://arxiv.org/abs/2602.09611)|-|<details><summary>detail</summary>preprint</details>|\n", "Delving into Spectral Clustering with Vision-Language Representations": "|**2026-2-10**|**Delving into Spectral Clustering with Vision-Language Representations**|Bo Peng et.al|[paper](https://arxiv.org/abs/2602.09586)|-|<details><summary>detail</summary>ICLR26</details>|\n", "SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning": "|**2026-2-10**|**SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning**|Furong Jia et.al|[paper](https://arxiv.org/abs/2602.09463)|-|-|\n", "Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models": "|**2026-2-10**|**Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models**|Xinwei Zhang et.al|[paper](https://arxiv.org/abs/2602.09431)|-|<details><summary>detail</summary>Under review</details>|\n", "VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference": "|**2026-2-10**|**VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference**|Ziyan Liu et.al|[paper](https://arxiv.org/abs/2511.16449)|-|-|\n"}}