{"source-free": {"Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method": "|**2025-8-14**|**Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2508.09202)|-|-|\n", "Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation": "|**2025-8-7**|**Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation**|Jianming Liu et.al|[paper](https://arxiv.org/abs/2508.05213)|[code](https://github.com/ljm198134/TVGTANet.)|-|\n", "Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning": "|**2025-8-4**|**Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning**|Sijia Wang et.al|[paper](https://arxiv.org/abs/2508.02039)|-|-|\n", "Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments": "|**2025-7-30**|**Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2501.02184)|-|-|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-7-28**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n", "GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning": "|**2025-7-26**|**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**|Sanqing Qu et.al|[paper](https://arxiv.org/abs/2403.14410)|[code](https://github.com/ispc-lab/GLC-plus.)|<details><summary>detail</summary>A substantial extension of the CVPR paper \"Upcycling Models under Domain and Category Shift\"</details>|\n", "SFUOD: Source-Free Unknown Object Detection": "|**2025-7-23**|**SFUOD: Source-Free Unknown Object Detection**|Keon-Hee Park et.al|[paper](https://arxiv.org/abs/2507.17373)|-|<details><summary>detail</summary>This paper has been accepted by ICCV 2025</details>|\n", "Text-Driven Causal Representation Learning for Source-Free Domain Generalization": "|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|\n", "Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting": "|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|\n", "Source-Free Domain Adaptation via Multi-view Contrastive Learning": "|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|\n", "Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework": "|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|\n", "Context Aware Grounded Teacher for Source Free Object Detection": "|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|\n", "SFDLA: Source-Free Document Layout Analysis": "|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|\n", "Unified Source-Free Domain Adaptation": "|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|\n", "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation": "|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|\n"}, "object detection": {"Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios": "|**2025-8-14**|**Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios**|Zhanwen Liu et.al|[paper](https://arxiv.org/abs/2508.10704)|[code](https://github.com/Charm11492/MCFNet.)|-|\n", "Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving": "|**2025-8-14**|**Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving**|Yuxin Cao et.al|[paper](https://arxiv.org/abs/2508.10600)|-|-|\n", "GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images": "|**2025-8-14**|**GCRPNet: Graph-Enhanced Contextual and Regional Perception Network For Salient Object Detection in Optical Remote Sensing Images**|Mengyu Ren et.al|[paper](https://arxiv.org/abs/2508.10542)|-|-|\n", "DOD-SA: Infrared-Visible Decoupled Object Detection with Single-Modality Annotations": "|**2025-8-14**|**DOD-SA: Infrared-Visible Decoupled Object Detection with Single-Modality Annotations**|Hang Jin et.al|[paper](https://arxiv.org/abs/2508.10445)|-|-|\n", "RoHOI: Robustness Benchmark for Human-Object Interaction Detection": "|**2025-8-13**|**RoHOI: Robustness Benchmark for Human-Object Interaction Detection**|Di Wen et.al|[paper](https://arxiv.org/abs/2507.09111)|[code](https://github.com/Kratos-Wen/RoHOI.)|<details><summary>detail</summary>Benchmarks</details>|\n", "MGDFIS: Multi-scale Global-detail Feature Integration Strategy for Small Object Detection": "|**2025-8-13**|**MGDFIS: Multi-scale Global-detail Feature Integration Strategy for Small Object Detection**|Yuxiang Wang et.al|[paper](https://arxiv.org/abs/2506.12697)|-|-|\n", "Robustness analysis of Deep Sky Objects detection models on HPC": "|**2025-8-13**|**Robustness analysis of Deep Sky Objects detection models on HPC**|Olivier Parisot et.al|[paper](https://arxiv.org/abs/2508.09831)|-|-|\n", "COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection": "|**2025-8-13**|**COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection**|Peiran Peng et.al|[paper](https://arxiv.org/abs/2508.09533)|-|-|\n", "DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection": "|**2025-8-12**|**DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection**|Kang Ni et.al|[paper](https://arxiv.org/abs/2508.09392)|[code](https://github.com/GrokCV/GrokSAR.)|-|\n", "DriveIndia: An Object Detection Dataset for Diverse Indian Traffic Scenes": "|**2025-8-12**|**DriveIndia: An Object Detection Dataset for Diverse Indian Traffic Scenes**|Rishav Kumar et.al|[paper](https://arxiv.org/abs/2507.19912)|[code](https://tihan.iith.ac.in/TiAND.html)|<details><summary>detail</summary>ITSC 2025 Conference</details>|\n", "QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection": "|**2025-8-11**|**QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection**|Yuxiao Wang et.al|[paper](https://arxiv.org/abs/2508.08590)|-|-|\n", "Designing Object Detection Models for TinyML: Foundations, Comparative Analysis, Challenges, and Emerging Solutions": "|**2025-8-11**|**Designing Object Detection Models for TinyML: Foundations, Comparative Analysis, Challenges, and Emerging Solutions**|Christophe EL Zeinaty et.al|[paper](https://arxiv.org/abs/2508.08352)|[code](https://github.com/christophezei/Optimizing-Object-Detection-Models-for-TinyML-A-Comprehensive-Survey.)|-|\n", "DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models": "|**2025-8-11**|**DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models**|Licheng Zhang et.al|[paper](https://arxiv.org/abs/2508.07714)|-|-|\n", "Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction": "|**2025-8-11**|**Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction**|Vishakha Lall et.al|[paper](https://arxiv.org/abs/2508.07624)|-|-|\n", "GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm": "|**2025-8-10**|**GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm**|Yu-Huan Wu et.al|[paper](https://arxiv.org/abs/2508.07585)|[code](https://github.com/yuhuan-wu/GAPNet.)|-|\n"}, "domain adaptation": {"Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method": "|**2025-8-14**|**Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2508.09202)|-|-|\n", "Multiple Stochastic Prompt Tuning for Few-shot Adaptation under Extreme Domain Shift": "|**2025-8-12**|**Multiple Stochastic Prompt Tuning for Few-shot Adaptation under Extreme Domain Shift**|Debarshi Brahma et.al|[paper](https://arxiv.org/abs/2506.03926)|-|-|\n", "Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation": "|**2025-8-12**|**Unified and Semantically Grounded Domain Adaptation for Medical Image Segmentation**|Xin Wang et.al|[paper](https://arxiv.org/abs/2508.08660)|-|-|\n", "DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives": "|**2025-8-11**|**DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives**|Sehwan Moon et.al|[paper](https://arxiv.org/abs/2508.08591)|-|-|\n", "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis": "|**2025-8-11**|**FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis**|Chen Shen et.al|[paper](https://arxiv.org/abs/2508.07950)|-|-|\n", "VFM-UDA++: Improving Network Architectures and Data Strategies for Unsupervised Domain Adaptive Semantic Segmentation": "|**2025-8-10**|**VFM-UDA++: Improving Network Architectures and Data Strategies for Unsupervised Domain Adaptive Semantic Segmentation**|Brun\u00f3 B. Englert et.al|[paper](https://arxiv.org/abs/2503.10685)|-|-|\n", "Statistical Inference for Autoencoder-based Anomaly Detection after Representation Learning-based Domain Adaptation": "|**2025-8-9**|**Statistical Inference for Autoencoder-based Anomaly Detection after Representation Learning-based Domain Adaptation**|Tran Tuan Kiet et.al|[paper](https://arxiv.org/abs/2508.07049)|-|-|\n", "Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification": "|**2025-8-9**|**Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification**|Taha Mustapha Nehdi et.al|[paper](https://arxiv.org/abs/2508.06831)|-|-|\n", "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation": "|**2025-8-8**|**AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic Segmentation**|Md. Al-Masrur Khan et.al|[paper](https://arxiv.org/abs/2507.17957)|[code](https://github.com/Masrur02/AFRDA)|-|\n", "TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation": "|**2025-8-8**|**TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation**|Mattia Litrico et.al|[paper](https://arxiv.org/abs/2508.06452)|-|-|\n", "HASD: Hierarchical Adaption for pathology Slide-level Domain-shift": "|**2025-8-8**|**HASD: Hierarchical Adaption for pathology Slide-level Domain-shift**|Jingsong Liu et.al|[paper](https://arxiv.org/abs/2506.23673)|-|<details><summary>detail</summary>Accepted by MICCAI 2025</details>|\n", "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding": "|**2025-8-8**|**Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding**|Jian Hu et.al|[paper](https://arxiv.org/abs/2508.06317)|-|-|\n", "Hybrid-TTA: Continual Test-time Adaptation via Dynamic Domain Shift Detection": "|**2025-8-8**|**Hybrid-TTA: Continual Test-time Adaptation via Dynamic Domain Shift Detection**|Hyewon Park et.al|[paper](https://arxiv.org/abs/2409.08566)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|\n", "SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation": "|**2025-8-7**|**SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation**|Zhiqing Xiao et.al|[paper](https://arxiv.org/abs/2508.05182)|-|<details><summary>detail</summary>The article has been accepted by Frontiers of Computer Science (FCS)</details>|\n", "Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation": "|**2025-8-7**|**Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation**|Jianming Liu et.al|[paper](https://arxiv.org/abs/2508.05213)|[code](https://github.com/ljm198134/TVGTANet.)|-|\n"}, "domain generalization": {"MIDAS: Modeling Ground-Truth Distributions with Dark Knowledge for Domain Generalized Stereo Matching": "|**2025-8-14**|**MIDAS: Modeling Ground-Truth Distributions with Dark Knowledge for Domain Generalized Stereo Matching**|Peng Xu et.al|[paper](https://arxiv.org/abs/2503.04376)|-|-|\n", "Domain-Generalization to Improve Learning in Meta-Learning Algorithms": "|**2025-8-12**|**Domain-Generalization to Improve Learning in Meta-Learning Algorithms**|Usman Anjum et.al|[paper](https://arxiv.org/abs/2508.09418)|-|-|\n", "Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering": "|**2025-8-12**|**Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering**|Elman Ghazaei et.al|[paper](https://arxiv.org/abs/2508.08974)|[code](https://github.com/Elman295/TCSSM.)|-|\n", "FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization": "|**2025-8-11**|**FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization**|Hongze Li et.al|[paper](https://arxiv.org/abs/2505.02515)|[code](https://github.com/pizzareapers/FedSDAF.)|-|\n", "Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning": "|**2025-8-10**|**Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning**|Yuki Shigeyasu et.al|[paper](https://arxiv.org/abs/2508.07539)|-|-|\n", "GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain": "|**2025-8-9**|**GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain**|Vida Adeli et.al|[paper](https://arxiv.org/abs/2503.22397)|-|-|\n", "SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation": "|**2025-8-7**|**SPA++: Generalized Graph Spectral Alignment for Versatile Domain Adaptation**|Zhiqing Xiao et.al|[paper](https://arxiv.org/abs/2508.05182)|-|<details><summary>detail</summary>The article has been accepted by Frontiers of Computer Science (FCS)</details>|\n", "Generative Multi-Target Cross-Domain Recommendation": "|**2025-8-7**|**Generative Multi-Target Cross-Domain Recommendation**|Jinqiu Jin et.al|[paper](https://arxiv.org/abs/2507.12871)|-|<details><summary>detail</summary>fix some information by request</details>|\n", "HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation": "|**2025-8-7**|**HFedATM: Hierarchical Federated Domain Generalization via Optimal Transport and Regularized Mean Aggregation**|Thinh Nguyen et.al|[paper](https://arxiv.org/abs/2508.05135)|-|-|\n", "Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation": "|**2025-8-6**|**Augmentation-based Domain Generalization and Joint Training from Multiple Source Domains for Whole Heart Segmentation**|Franz Thaler et.al|[paper](https://arxiv.org/abs/2508.04552)|-|<details><summary>detail</summary>Accepted for the MICCAI Challenge on Comprehensive Analysis and Computing of Real-World Medical Images 2024</details>|\n", "Cross-Domain Image Synthesis: Generating H&E from Multiplex Biomarker Imaging": "|**2025-8-5**|**Cross-Domain Image Synthesis: Generating H&E from Multiplex Biomarker Imaging**|Jillur Rahman Saurav et.al|[paper](https://arxiv.org/abs/2508.04734)|-|-|\n", "FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image Segmentation": "|**2025-8-5**|**FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image Segmentation**|Zhipeng Deng et.al|[paper](https://arxiv.org/abs/2501.07378)|-|-|\n", "LLM-Generated Heuristics for AI Planning: Do We Even Need Domain-Independence Anymore?": "|**2025-8-5**|**LLM-Generated Heuristics for AI Planning: Do We Even Need Domain-Independence Anymore?**|Alexander Tuisov et.al|[paper](https://arxiv.org/abs/2501.18784)|-|-|\n", "Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation": "|**2025-8-5**|**Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation**|Jun Luo et.al|[paper](https://arxiv.org/abs/2508.03300)|[code](https://github.com/ROUJINN/SDGPA)|<details><summary>detail</summary>IROS 2025</details>|\n", "Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data": "|**2025-8-4**|**Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data**|Kota Dohi et.al|[paper](https://arxiv.org/abs/2409.16647)|-|-|\n"}, "vision language": {"From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models": "|**2025-8-14**|**From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models**|Tiancheng Han et.al|[paper](https://arxiv.org/abs/2508.10770)|-|-|\n", "AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models": "|**2025-8-14**|**AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models**|Shixiong Xu et.al|[paper](https://arxiv.org/abs/2508.10667)|-|-|\n", "Evaluation of Cultural Competence of Vision-Language Models": "|**2025-8-14**|**Evaluation of Cultural Competence of Vision-Language Models**|Srishti Yadav et.al|[paper](https://arxiv.org/abs/2505.22793)|-|-|\n", "SemPT: Semantic Prompt Tuning for Vision-Language Models": "|**2025-8-14**|**SemPT: Semantic Prompt Tuning for Vision-Language Models**|Xiao Shi et.al|[paper](https://arxiv.org/abs/2508.10645)|-|-|\n", "ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation": "|**2025-8-14**|**ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation**|Hosam Elgendy et.al|[paper](https://arxiv.org/abs/2508.10635)|-|-|\n", "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model": "|**2025-8-14**|**CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model**|Zhuoyuan Yu et.al|[paper](https://arxiv.org/abs/2508.10416)|-|-|\n", "Contrast Sensitivity Function of Multimodal Vision-Language Models": "|**2025-8-14**|**Contrast Sensitivity Function of Multimodal Vision-Language Models**|Pablo Hern\u00e1ndez-C\u00e1mara et.al|[paper](https://arxiv.org/abs/2508.10367)|-|-|\n", "Hierarchical Cross-modal Prompt Learning for Vision-Language Models": "|**2025-8-14**|**Hierarchical Cross-modal Prompt Learning for Vision-Language Models**|Hao Zheng et.al|[paper](https://arxiv.org/abs/2507.14976)|[code](https://github.com/zzeoZheng/HiCroPL.)|<details><summary>detail</summary>Accepted by ICCV2025</details>|\n", "ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver": "|**2025-8-14**|**ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver**|Wenxuan Song et.al|[paper](https://arxiv.org/abs/2508.10333)|[code](https://zionchow.github.io/ReconVLA/.)|-|\n", "A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning": "|**2025-8-13**|**A Vision-Language Pre-training Model-Guided Approach for Mitigating Backdoor Attacks in Federated Learning**|Keke Gai et.al|[paper](https://arxiv.org/abs/2508.10315)|-|-|\n", "CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting": "|**2025-8-13**|**CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting**|Atin Pothiraj et.al|[paper](https://arxiv.org/abs/2504.15485)|[code](https://github.com/atinpothiraj/CAPTURe)|<details><summary>detail</summary>ICCV 2025</details>|\n", "Bridging Modality Gaps in e-Commerce Products via Vision-Language Alignment": "|**2025-8-13**|**Bridging Modality Gaps in e-Commerce Products via Vision-Language Alignment**|Yipeng Zhang et.al|[paper](https://arxiv.org/abs/2508.10116)|-|-|\n", "LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit": "|**2025-8-13**|**LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit**|Chengtao Lv et.al|[paper](https://arxiv.org/abs/2508.09981)|[code](https://github.com/ModelTC/LightCompress.)|-|\n", "GeoVLA: Empowering 3D Representations in Vision-Language-Action Models": "|**2025-8-13**|**GeoVLA: Empowering 3D Representations in Vision-Language-Action Models**|Lin Sun et.al|[paper](https://arxiv.org/abs/2508.09071)|[code](https://linsun449.github.io/GeoVLA/)|<details><summary>detail</summary>The project is visible at https://linsun449</details>|\n", "CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment": "|**2025-8-13**|**CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment**|Bo Wang et.al|[paper](https://arxiv.org/abs/2505.17619)|-|<details><summary>detail</summary>Camera ready version for ICONIP 2025</details>|\n"}}