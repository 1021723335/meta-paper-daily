{"source-free": {"Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation": "|**2025-9-22**|**Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation**|Amirhossein Dadashzadeh et.al|[paper](https://arxiv.org/abs/2504.11669)|[code](https://github.com/Plrbear/Co-Star)|-|\n", "Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation": "|**2025-9-21**|**Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation**|Bin Wang et.al|[paper](https://arxiv.org/abs/2509.16942)|-|-|\n", "Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments": "|**2025-9-18**|**Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2501.02184)|-|-|\n", "Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation": "|**2025-9-18**|**Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation**|Silvio Mazzucco et.al|[paper](https://arxiv.org/abs/2509.15225)|[code](https://thegoodailab.org/blog/vocalign)|<details><summary>detail</summary>BMVC 2025 - Project Page: https://thegoodailab</details>|\n", "Step-wise Distribution Alignment Guided Style Prompt Tuning for Source-free Cross-domain Few-shot Learning": "|**2025-9-13**|**Step-wise Distribution Alignment Guided Style Prompt Tuning for Source-free Cross-domain Few-shot Learning**|Huali Xu et.al|[paper](https://arxiv.org/abs/2411.10070)|[code](https://github.com/xuhuali-mxj/StepSPT.)|<details><summary>detail</summary>IEEE TPAMI</details>|\n", "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment": "|**2025-9-12**|**Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment**|Rini Smita Thakur et.al|[paper](https://arxiv.org/abs/2509.10134)|[code](https://visdomlab.github.io/GCL/.)|<details><summary>detail</summary>Accepted in BMVC 2025</details>|\n", "Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models": "|**2025-9-10**|**Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models**|Kosuke Kihara et.al|[paper](https://arxiv.org/abs/2509.08372)|-|<details><summary>detail</summary>Accepted by the IEEE ICIP 2025 Satellite Workshop 1: Edge Intelligence: Smart</details>|\n", "StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails": "|**2025-9-2**|**StableSleep: Source-Free Test-Time Adaptation for Sleep Staging with Lightweight Safety Rails**|Hritik Arasu et.al|[paper](https://arxiv.org/abs/2509.02982)|-|<details><summary>detail</summary>5 page paper</details>|\n", "Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration": "|**2025-8-28**|**Model-Free Hovering and Source Seeking via Extremum Seeking Control: Experimental Demonstration**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2508.20836)|-|-|\n", "VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection": "|**2025-8-26**|**VFM-Guided Semi-Supervised Detection Transformer under Source-Free Constraints for Remote Sensing Object Detection**|Jianhong Han et.al|[paper](https://arxiv.org/abs/2508.11167)|-|<details><summary>detail</summary>Manuscript submitted to IEEE TCSVT</details>|\n", "Towards Source-Free Machine Unlearning": "|**2025-8-20**|**Towards Source-Free Machine Unlearning**|Sk Miraj Ahmed et.al|[paper](https://arxiv.org/abs/2508.15127)|-|<details><summary>detail</summary>Accepted by CVPR 2025</details>|\n", "Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method": "|**2025-8-14**|**Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2508.09202)|-|-|\n", "Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation": "|**2025-8-7**|**Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain Few-Shot Segmentation**|Jianming Liu et.al|[paper](https://arxiv.org/abs/2508.05213)|[code](https://github.com/ljm198134/TVGTANet.)|-|\n", "Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning": "|**2025-8-4**|**Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning**|Sijia Wang et.al|[paper](https://arxiv.org/abs/2508.02039)|-|-|\n", "Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation": "|**2025-7-28**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|\n"}, "object detection": {"RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion": "|**2025-9-22**|**RCTDistill: Cross-Modal Knowledge Distillation Framework for Radar-Camera 3D Object Detection with Temporal Fusion**|Geonho Bang et.al|[paper](https://arxiv.org/abs/2509.17712)|-|<details><summary>detail</summary>ICCV 2025</details>|\n", "Domain Adaptive Object Detection for Space Applications with Real-Time Constraints": "|**2025-9-22**|**Domain Adaptive Object Detection for Space Applications with Real-Time Constraints**|Samet Hicsonmez et.al|[paper](https://arxiv.org/abs/2509.17593)|-|<details><summary>detail</summary>Advanced Space Technologies in Robotics and Automation (ASTRA) 2025</details>|\n", "An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection": "|**2025-9-22**|**An Empirical Study on the Robustness of YOLO Models for Underwater Object Detection**|Edwine Nabahirwa et.al|[paper](https://arxiv.org/abs/2509.17561)|-|<details><summary>detail</summary>28 Pages</details>|\n", "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity": "|**2025-9-21**|**GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity**|Seongheon Park et.al|[paper](https://arxiv.org/abs/2508.19972)|-|<details><summary>detail</summary>NeurIPS 2025</details>|\n", "Enhanced Detection of Tiny Objects in Aerial Images": "|**2025-9-21**|**Enhanced Detection of Tiny Objects in Aerial Images**|Kihyun Kim et.al|[paper](https://arxiv.org/abs/2509.17078)|[code](https://github.com/Kihyun11/MoonNet)|-|\n", "LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection": "|**2025-9-21**|**LLM-Assisted Semantic Guidance for Sparsely Annotated Remote Sensing Object Detection**|Wei Liao et.al|[paper](https://arxiv.org/abs/2509.16970)|-|-|\n", "MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image": "|**2025-9-21**|**MO R-CNN: Multispectral Oriented R-CNN for Object Detection in Remote Sensing Image**|Leiyu Wang et.al|[paper](https://arxiv.org/abs/2509.16957)|[code](https://github.com/Iwill-github/MORCNN.)|-|\n", "Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection": "|**2025-9-20**|**Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection**|Wenhuan Lu et.al|[paper](https://arxiv.org/abs/2509.16670)|-|-|\n", "Investigating Long-term Training for Remote Sensing Object Detection": "|**2025-9-20**|**Investigating Long-term Training for Remote Sensing Object Detection**|JongHyun Park et.al|[paper](https://arxiv.org/abs/2407.15143)|[code](https://github.com/unique-chan/dbf.)|-|\n", "Efficient Detection of Objects Near a Robot Manipulator via Miniature Time-of-Flight Sensors": "|**2025-9-19**|**Efficient Detection of Objects Near a Robot Manipulator via Miniature Time-of-Flight Sensors**|Carter Sifferman et.al|[paper](https://arxiv.org/abs/2509.16122)|-|-|\n", "LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation": "|**2025-9-19**|**LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated Data Generation**|Yang Zhou et.al|[paper](https://arxiv.org/abs/2503.13794)|-|-|\n", "A re-calibration method for object detection with multi-modal alignment bias in autonomous driving": "|**2025-9-19**|**A re-calibration method for object detection with multi-modal alignment bias in autonomous driving**|Zhihang Song et.al|[paper](https://arxiv.org/abs/2405.16848)|-|<details><summary>detail</summary>Accepted for publication in IST 2025</details>|\n", "The Missing Piece: A Case for Pre-Training in 3D Medical Object Detection": "|**2025-9-19**|**The Missing Piece: A Case for Pre-Training in 3D Medical Object Detection**|Katharina Eckstein et.al|[paper](https://arxiv.org/abs/2509.15947)|[code](https://github.com/MIC-DKFZ/nnDetection-finetuning.)|<details><summary>detail</summary>MICCAI 2025</details>|\n", "PAN: Pillars-Attention-Based Network for 3D Object Detection": "|**2025-9-19**|**PAN: Pillars-Attention-Based Network for 3D Object Detection**|Ruan Bispo et.al|[paper](https://arxiv.org/abs/2509.15935)|-|-|\n", "MCOD: The First Challenging Benchmark for Multispectral Camouflaged Object Detection": "|**2025-9-19**|**MCOD: The First Challenging Benchmark for Multispectral Camouflaged Object Detection**|Yang Li et.al|[paper](https://arxiv.org/abs/2509.15753)|[code](https://github.com/yl2900260-bit/MCOD.)|-|\n"}, "domain adaptation": {"Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation": "|**2025-9-22**|**Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation**|Amirhossein Dadashzadeh et.al|[paper](https://arxiv.org/abs/2504.11669)|[code](https://github.com/Plrbear/Co-Star)|-|\n", "SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI": "|**2025-9-22**|**SmaRT: Style-Modulated Robust Test-Time Adaptation for Cross-Domain Brain Tumor Segmentation in MRI**|Yuanhan Wang et.al|[paper](https://arxiv.org/abs/2509.17925)|[code](https://github.com/baiyou1234/SmaRT.)|-|\n", "Domain Adaptive Object Detection for Space Applications with Real-Time Constraints": "|**2025-9-22**|**Domain Adaptive Object Detection for Space Applications with Real-Time Constraints**|Samet Hicsonmez et.al|[paper](https://arxiv.org/abs/2509.17593)|-|<details><summary>detail</summary>Advanced Space Technologies in Robotics and Automation (ASTRA) 2025</details>|\n", "Training-Free Label Space Alignment for Universal Domain Adaptation": "|**2025-9-22**|**Training-Free Label Space Alignment for Universal Domain Adaptation**|Dujin Lee et.al|[paper](https://arxiv.org/abs/2509.17452)|-|-|\n", "Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation": "|**2025-9-21**|**Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation**|Bin Wang et.al|[paper](https://arxiv.org/abs/2509.16942)|-|-|\n", "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation": "|**2025-9-20**|**Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation**|Junzhuo Li et.al|[paper](https://arxiv.org/abs/2509.16882)|-|<details><summary>detail</summary>EMNLP 2025 Main Conference</details>|\n", "Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies": "|**2025-9-20**|**Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies**|Salha Alyami et.al|[paper](https://arxiv.org/abs/2509.16788)|-|<details><summary>detail</summary>26 excluding bibliography </details>|\n", "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis": "|**2025-9-19**|**Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis**|YongKyung Oh et.al|[paper](https://arxiv.org/abs/2506.22393)|[code](https://proceedings.mlr.press/v287/oh25a.html)|<details><summary>detail</summary>Published at the sixth Conference on Health</details>|\n", "AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent Trajectory Modeling in Sports": "|**2025-9-19**|**AdaSports-Traj: Role- and Domain-Aware Adaptation for Multi-Agent Trajectory Modeling in Sports**|Yi Xu et.al|[paper](https://arxiv.org/abs/2509.16095)|-|<details><summary>detail</summary>Accepted by ICDM 2025</details>|\n", "SONAR: Self-Distilled Continual Pre-training for Domain Adaptive Audio Representation": "|**2025-9-19**|**SONAR: Self-Distilled Continual Pre-training for Domain Adaptive Audio Representation**|Yizhou Zhang et.al|[paper](https://arxiv.org/abs/2509.15703)|-|<details><summary>detail</summary>Submitted to ICASSP 2026</details>|\n", "Improving Anomalous Sound Detection with Attribute-aware Representation from Domain-adaptive Pre-training": "|**2025-9-19**|**Improving Anomalous Sound Detection with Attribute-aware Representation from Domain-adaptive Pre-training**|Xin Fang et.al|[paper](https://arxiv.org/abs/2509.12845)|-|<details><summary>detail</summary>Copyright 2026 IEEE</details>|\n", "Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation": "|**2025-9-18**|**Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation**|Silvio Mazzucco et.al|[paper](https://arxiv.org/abs/2509.15225)|[code](https://thegoodailab.org/blog/vocalign)|<details><summary>detail</summary>BMVC 2025 - Project Page: https://thegoodailab</details>|\n", "SWAT: Sliding Window Adversarial Training for Gradual Domain Adaptation": "|**2025-9-18**|**SWAT: Sliding Window Adversarial Training for Gradual Domain Adaptation**|Zixi Wang et.al|[paper](https://arxiv.org/abs/2501.19155)|-|<details><summary>detail</summary>submitted to NIPS 2025</details>|\n", "Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation": "|**2025-9-18**|**Style Transfer with Diffusion Models for Synthetic-to-Real Domain Adaptation**|Estelle Chigot et.al|[paper](https://arxiv.org/abs/2505.16360)|[code](https://github.com/echigot/cactif.)|<details><summary>detail</summary>Published in Computer Vision and Image Understanding</details>|\n", "ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains": "|**2025-9-18**|**ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains**|Guillaume Vray et.al|[paper](https://arxiv.org/abs/2505.14511)|[code](https://github.com/LTS5/ReservoirTTA.)|-|\n"}, "domain generalization": {"Unsupervised Structural-Counterfactual Generation under Domain Shift": "|**2025-9-22**|**Unsupervised Structural-Counterfactual Generation under Domain Shift**|Krishn Vishwas Kher et.al|[paper](https://arxiv.org/abs/2502.12013)|-|<details><summary>detail</summary>Updated author ordering</details>|\n", "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning": "|**2025-9-21**|**Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning**|Simon Ouellette et.al|[paper](https://arxiv.org/abs/2507.15877)|-|<details><summary>detail</summary>this version fixes errors in AlphaEvolve total % calculation</details>|\n", "From domain-landmark graph learning to problem-landmark graph generation": "|**2025-9-21**|**From domain-landmark graph learning to problem-landmark graph generation**|Cristian P\u00e9rez-Corral et.al|[paper](https://arxiv.org/abs/2509.17062)|-|-|\n", "Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains": "|**2025-9-20**|**Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains**|Junghwan Kim et.al|[paper](https://arxiv.org/abs/2509.16531)|-|<details><summary>detail</summary>EMNLP 2025</details>|\n", "From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR": "|**2025-9-19**|**From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR**|Juan Castorena et.al|[paper](https://arxiv.org/abs/2509.16346)|-|-|\n", "Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization": "|**2025-9-19**|**Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization**|Tan Pan et.al|[paper](https://arxiv.org/abs/2509.15791)|-|<details><summary>detail</summary>Accepted by NeurIPS 2025</details>|\n", "CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization": "|**2025-9-18**|**CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization**|Min Zhang et.al|[paper](https://arxiv.org/abs/2509.15330)|-|-|\n", "Diffusion-Based Action Recognition Generalizes to Untrained Domains": "|**2025-9-18**|**Diffusion-Based Action Recognition Generalizes to Untrained Domains**|Rogerio Guimaraes et.al|[paper](https://arxiv.org/abs/2509.08908)|[code](https://www.vision.caltech.edu/actiondiff/)|-|\n", "Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications": "|**2025-9-18**|**Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications**|Tahar Chettaoui et.al|[paper](https://arxiv.org/abs/2509.14921)|-|<details><summary>detail</summary>the IEEE International Joint Conference on Biometrics 2025 (IJCB 2025)</details>|\n", "Domain Generalization for In-Orbit 6D Pose Estimation": "|**2025-9-18**|**Domain Generalization for In-Orbit 6D Pose Estimation**|Antoine Legrand et.al|[paper](https://arxiv.org/abs/2406.11743)|-|-|\n", "SNaRe: Domain-aware Data Generation for Low-Resource Event Detection": "|**2025-9-17**|**SNaRe: Domain-aware Data Generation for Low-Resource Event Detection**|Tanmay Parekh et.al|[paper](https://arxiv.org/abs/2502.17394)|-|<details><summary>detail</summary>EMNLP 2025 Main</details>|\n", "Class-invariant Test-Time Augmentation for Domain Generalization": "|**2025-9-17**|**Class-invariant Test-Time Augmentation for Domain Generalization**|Zhicheng Lin et.al|[paper](https://arxiv.org/abs/2509.14420)|-|-|\n", "CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning": "|**2025-9-17**|**CodeLSI: Leveraging Foundation Models for Automated Code Generation with Low-Rank Optimization and Domain-Specific Instruction Tuning**|Huy Le et.al|[paper](https://arxiv.org/abs/2509.14373)|-|-|\n", "SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics": "|**2025-9-17**|**SPAR: Scalable LLM-based PDDL Domain Generation for Aerial Robotics**|Songhao Huang et.al|[paper](https://arxiv.org/abs/2509.13691)|-|-|\n", "Double Helix Diffusion for Cross-Domain Anomaly Image Generation": "|**2025-9-16**|**Double Helix Diffusion for Cross-Domain Anomaly Image Generation**|Linchun Wu et.al|[paper](https://arxiv.org/abs/2509.12787)|-|-|\n"}, "vision language": {"VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model": "|**2025-9-22**|**VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model**|Yihao Wang et.al|[paper](https://arxiv.org/abs/2509.09372)|[code](https://vla-adapter.github.io/.)|-|\n", "M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer": "|**2025-9-22**|**M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer**|Yanxin Zhang et.al|[paper](https://arxiv.org/abs/2509.18005)|-|-|\n", "I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models": "|**2025-9-22**|**I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models**|Clemence Grislain et.al|[paper](https://arxiv.org/abs/2509.16072)|[code](https://clemgris.github.io/I-FailSense/).)|-|\n", "SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models": "|**2025-9-22**|**SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models**|Pingyi Chen et.al|[paper](https://arxiv.org/abs/2509.17664)|[code](https://github.com/cpystan/SD-VLM.)|<details><summary>detail</summary>Accepted by NeurIPS 2025</details>|\n", "Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models": "|**2025-9-22**|**Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models**|Jinyeong Kim et.al|[paper](https://arxiv.org/abs/2509.17588)|-|-|\n", "Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization": "|**2025-9-22**|**Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization**|Jiulong Wu et.al|[paper](https://arxiv.org/abs/2506.04039)|-|<details><summary>detail</summary>This paper is accepted by EMNLP2025</details>|\n", "ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding": "|**2025-9-22**|**ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding**|Xingqi Wang et.al|[paper](https://arxiv.org/abs/2509.17481)|[code](https://github.com/ymcui/ChartHal)|-|\n", "StockGenChaR: A Study on the Evaluation of Large Vision-Language Models on Stock Chart Captioning": "|**2025-9-22**|**StockGenChaR: A Study on the Evaluation of Large Vision-Language Models on Stock Chart Captioning**|Le Qiu et.al|[paper](https://arxiv.org/abs/2412.04041)|-|-|\n", "Vision Language Models Are Not (Yet) Spelling Correctors": "|**2025-9-22**|**Vision Language Models Are Not (Yet) Spelling Correctors**|Junhong Liang et.al|[paper](https://arxiv.org/abs/2509.17418)|-|-|\n", "ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding": "|**2025-9-22**|**ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding**|Jialiang Kang et.al|[paper](https://arxiv.org/abs/2509.15235)|[code](https://github.com/KangJialiang/ViSpec.)|<details><summary>detail</summary>NeurIPS 2025</details>|\n", "Revisiting Vision Language Foundations for No-Reference Image Quality Assessment": "|**2025-9-22**|**Revisiting Vision Language Foundations for No-Reference Image Quality Assessment**|Ankit Yadav et.al|[paper](https://arxiv.org/abs/2509.17374)|-|-|\n", "Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning": "|**2025-9-21**|**Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning**|Wenda Qin et.al|[paper](https://arxiv.org/abs/2509.15250)|[code](https://github.com/wdqin/VLN-NAP)|<details><summary>detail</summary>EMNLP 2025</details>|\n", "The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning": "|**2025-9-21**|**The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning**|Titong Jiang et.al|[paper](https://arxiv.org/abs/2509.12594)|[code](https://liauto-research.github.io/LightVLA)|<details><summary>detail</summary>Under review</details>|\n", "FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation": "|**2025-9-21**|**FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation**|Fan Yang et.al|[paper](https://arxiv.org/abs/2506.16806)|-|-|\n", "Few-Shot Image Quality Assessment via Adaptation of Vision-Language Models": "|**2025-9-21**|**Few-Shot Image Quality Assessment via Adaptation of Vision-Language Models**|Xudong Li et.al|[paper](https://arxiv.org/abs/2409.05381)|-|-|\n"}}