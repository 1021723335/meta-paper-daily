## CV Papers Daily
- [source-free](#source-free)
- [object detection](#object-detection)
- [domain adaptation](#domain-adaptation)
- [domain generalization](#domain-generalization)
- [vision language](#vision-language)


## Updated on 2026.02.05

## source-free

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2026-2-2**|**Rethinking Test-Time Training: Tilting The Latent Distribution For Few-Shot Source-Free Adaptation**|Tahir Qasim Syed et.al|[paper](https://arxiv.org/abs/2602.02633)|-|-|
|**2026-1-30**|**Collision-free Source Seeking and Flocking Control of Multi-agents with Connectivity Preservation**|Tinghua Li et.al|[paper](https://arxiv.org/abs/2301.04576)|-|<details><summary>detail</summary>Published in IEEE Transactions on Automatic Control</details>|
|**2026-1-29**|**Source Coding with Free Bits and the Multi-Way Number Partitioning Problem**|Niloufar Ahmadypour et.al|[paper](https://arxiv.org/abs/2009.02710)|-|-|
|**2026-1-28**|**Beyond Retraining: Training-Free Unknown Class Filtering for Source-Free Open Set Domain Adaptation of Vision-Language Models**|Yongguang Li et.al|[paper](https://arxiv.org/abs/2504.14224)|-|<details><summary>detail</summary>Core methods unchanged</details>|
|**2026-1-28**|**A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency**|Debopom Sutradhar et.al|[paper](https://arxiv.org/abs/2601.20284)|-|<details><summary>detail</summary>Manuscript under review in IEEE Transactions on Image Processing</details>|
|**2026-1-24**|**Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity**|Harsharaj Pathak et.al|[paper](https://arxiv.org/abs/2601.17408)|-|-|
|**2026-1-23**|**Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results**|Rohan Palanikumar et.al|[paper](https://arxiv.org/abs/2511.00752)|-|-|
|**2026-1-20**|**Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection**|Huizai Yao et.al|[paper](https://arxiv.org/abs/2511.07301)|-|<details><summary>detail</summary>AAAI 2026</details>|
|**2026-1-19**|**Towards Unbiased Source-Free Object Detection via Vision Foundation Models**|Zhi Cai et.al|[paper](https://arxiv.org/abs/2601.12765)|-|-|
|**2026-1-18**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|[code](https://github.com/tntek/CausalDA.)|-|
|**2026-1-16**|**GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2601.11161)|[code](https://github.com/pascalschlachter/GMM-COMET.)|-|
|**2026-1-13**|**SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling**|Xi Chen et.al|[paper](https://arxiv.org/abs/2601.08608)|[code](https://github.com/chenxi52/SfMamba.)|-|
|**2026-1-13**|**Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation**|Yuan Gao et.al|[paper](https://arxiv.org/abs/2601.08375)|-|-|
|**2026-1-5**|**Empowering Source-Free Domain Adaptation via MLLM-Guided Reliability-Based Curriculum Learning**|Dongjie Chen et.al|[paper](https://arxiv.org/abs/2405.18376)|[code](https://github.com/Dong-Jie-Chen/RCL.)|-|
|**2025-12-24**|**Foundation Model Priors Enhance Object Focus in Feature Space for Source-Free Object Detection**|Sairam VCR et.al|[paper](https://arxiv.org/abs/2512.17514)|-|-|

## object detection

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2026-2-4**|**A labeled dataset of simulated phlebotomy procedures for medical AI: polygon annotations for object detection and human-object interaction**|Raúl Jiménez Cruz et.al|[paper](https://arxiv.org/abs/2602.04624)|-|-|
|**2026-2-4**|**Beyond Global Scanning: Adaptive Visual State Space Modeling for Salient Object Detection in Optical Remote Sensing Images**|Mengyu Ren et.al|[paper](https://arxiv.org/abs/2508.10542)|-|-|
|**2026-2-3**|**RAWDet-7: A Multi-Scenario Benchmark for Object Detection and Description on Quantized RAW Images**|Mishal Fatima et.al|[paper](https://arxiv.org/abs/2602.03760)|-|<details><summary>detail</summary>*Equal Contribution</details>|
|**2026-2-3**|**SPWOOD: Sparse Partial Weakly-Supervised Oriented Object Detection**|Wei Zhang et.al|[paper](https://arxiv.org/abs/2602.03634)|[code](https://github.com/VisionXLab/SPWOOD.)|<details><summary>detail</summary>The Fourteenth International Conference on Learning Representations (ICLR 2026)</details>|
|**2026-2-3**|**High-Resolution Underwater Camouflaged Object Detection: GBU-UCOD Dataset and Topology-Aware and Frequency-Decoupled Networks**|Wenji Wu et.al|[paper](https://arxiv.org/abs/2602.03591)|[code](https://github.com/Wuwenji18/GBU-UCOD.)|-|
|**2026-2-3**|**Inlier-Centric Post-Training Quantization for Object Detection Models**|Minsu Kim et.al|[paper](https://arxiv.org/abs/2602.03472)|-|-|
|**2026-2-3**|**FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion**|Chen-Bin Feng et.al|[paper](https://arxiv.org/abs/2602.03137)|[code](https://intellindust-ai-lab.github.io/projects/FSOD-VFM.)|<details><summary>detail</summary>Accepted by ICLR 2026</details>|
|**2026-2-2**|**Enhanced Detection of Tiny Objects in Aerial Images**|Kihyun Kim et.al|[paper](https://arxiv.org/abs/2509.17078)|[code](https://github.com/Kihyun11/MoonNet)|-|
|**2026-2-2**|**Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding**|Soheil Behnam Roudsari et.al|[paper](https://arxiv.org/abs/2602.02167)|-|-|
|**2026-2-2**|**Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images**|Shuai Yang et.al|[paper](https://arxiv.org/abs/2602.01954)|-|-|
|**2026-2-1**|**Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework**|Wenzhuo Zhao et.al|[paper](https://arxiv.org/abs/2602.01593)|-|-|
|**2026-2-1**|**Mixed Precision PointPillars for Efficient 3D Object Detection with TensorRT**|Ninnart Fuengfusin et.al|[paper](https://arxiv.org/abs/2601.12638)|-|-|
|**2026-1-31**|**HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection**|Junwen Chen et.al|[paper](https://arxiv.org/abs/2510.05609)|[code](https://github.com/cjw2021/HOI-R1.)|-|
|**2026-1-31**|**Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment**|Tianyi Zhang et.al|[paper](https://arxiv.org/abs/2602.00531)|-|-|
|**2026-1-30**|**Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects**|Bsher Karbouj et.al|[paper](https://arxiv.org/abs/2602.00385)|-|-|

## domain adaptation

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2026-2-4**|**Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks**|Masaya Tsunokake et.al|[paper](https://arxiv.org/abs/2602.04466)|-|-|
|**2026-2-4**|**Universal Robust Speech Adaptation for Cross-Domain Speech Recognition and Enhancement**|Chien-Chun Wang et.al|[paper](https://arxiv.org/abs/2602.04307)|-|<details><summary>detail</summary>IEEE Transactions on Audio</details>|
|**2026-2-3**|**DADP: Domain Adaptive Diffusion Policy**|Pengcheng Wang et.al|[paper](https://arxiv.org/abs/2602.04037)|[code](https://outsider86.github.io/DomainAdaptiveDiffusionPolicy/.)|-|
|**2026-2-3**|**CiMRAG: CiM-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs**|Shih-Hsuan Chiu et.al|[paper](https://arxiv.org/abs/2601.20041)|-|<details><summary>detail</summary>Accepted by ICASSP 2026</details>|
|**2026-2-2**|**DSKC: Domain Style Modeling with Adaptive Knowledge Consolidation for Exemplar-free Lifelong Person Re-Identification**|Shiben Liu et.al|[paper](https://arxiv.org/abs/2508.03516)|[code](https://github.com/LiuShiBen/DKUA.)|<details><summary>detail</summary>11 papges</details>|
|**2026-2-2**|**When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs**|Junyi Zou et.al|[paper](https://arxiv.org/abs/2601.18350)|-|-|
|**2026-2-2**|**Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation**|Zihao Wang et.al|[paper](https://arxiv.org/abs/2601.18623)|-|<details><summary>detail</summary>Paper accepted as a conference paper at ICLR 2026</details>|
|**2026-2-1**|**Instance-Guided Unsupervised Domain Adaptation for Robotic Semantic Segmentation**|Michele Antonazzi et.al|[paper](https://arxiv.org/abs/2602.01389)|-|<details><summary>detail</summary>Accepted for publication at ICRA 2026</details>|
|**2026-2-1**|**Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective**|Zhichao Chen et.al|[paper](https://arxiv.org/abs/2602.01179)|-|-|
|**2026-1-31**|**Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation**|Mritunjay Pandey et.al|[paper](https://arxiv.org/abs/2602.00899)|-|-|
|**2026-1-31**|**Domain Adaptation of Attention Heads for Zero-shot Anomaly Detection**|Kiyoon Jeong et.al|[paper](https://arxiv.org/abs/2505.22259)|[code](https://github.com/kiyoonjeong0305/HeadCLIP.)|-|
|**2026-1-31**|**Riemannian Flow Matching for Disentangled Graph Domain Adaptation**|Yingxu Wang et.al|[paper](https://arxiv.org/abs/2602.00656)|-|-|
|**2026-1-30**|**InstructDiff: Domain-Adaptive Data Selection via Differential Entropy for Efficient LLM Fine-Tuning**|Junyou Su et.al|[paper](https://arxiv.org/abs/2601.23006)|-|-|
|**2026-1-29**|**OmniLens: Towards Universal Lens Aberration Correction via LensLib-to-Specific Domain Adaptation**|Qi Jiang et.al|[paper](https://arxiv.org/abs/2409.05809)|[code](https://github.com/zju-jiangqi/OmniLens.)|<details><summary>detail</summary>Optics & Laser Technology (JOLT)</details>|
|**2026-1-29**|**Influence Guided Sampling for Domain Adaptation of Text Retrievers**|Meet Doshi et.al|[paper](https://arxiv.org/abs/2601.21759)|-|-|

## domain generalization

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2026-2-4**|**Domain Generalization Under Posterior Drift**|Yilun Zhu et.al|[paper](https://arxiv.org/abs/2510.04441)|-|-|
|**2026-2-4**|**PEPR: Privileged Event-based Predictive Regularization for Domain Generalization**|Gabriele Magrini et.al|[paper](https://arxiv.org/abs/2602.04583)|-|-|
|**2026-2-3**|**Self-CriTeach: LLM Self-Teaching and Self-Critiquing for Improving Robotic Planning via Automated Domain Generation**|Jinbang Huang et.al|[paper](https://arxiv.org/abs/2509.21543)|-|-|
|**2026-2-3**|**CiMRAG: CiM-Aware Domain-Adaptive and Noise-Resilient Retrieval-Augmented Generation for Edge-Based LLMs**|Shih-Hsuan Chiu et.al|[paper](https://arxiv.org/abs/2601.20041)|-|<details><summary>detail</summary>Accepted by ICASSP 2026</details>|
|**2026-2-3**|**Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain**|Wei Zhu et.al|[paper](https://arxiv.org/abs/2602.03368)|-|-|
|**2026-2-2**|**Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains**|Jae-Sung Bae et.al|[paper](https://arxiv.org/abs/2602.02841)|-|-|
|**2026-2-2**|**Robust Domain Generalization under Divergent Marginal and Conditional Distributions**|Jewon Yeom et.al|[paper](https://arxiv.org/abs/2602.02015)|-|-|
|**2026-1-31**|**Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry**|Seunghyun Yoo et.al|[paper](https://arxiv.org/abs/2602.00547)|-|-|
|**2026-1-29**|**Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications**|Chenhua Shi et.al|[paper](https://arxiv.org/abs/2509.25736)|-|-|
|**2026-1-29**|**Decentralized Domain Generalization with Style Sharing: Formal Model and Convergence Analysis**|Shahryar Zehtabi et.al|[paper](https://arxiv.org/abs/2504.06235)|-|-|
|**2026-1-29**|**Negatives-Dominant Contrastive Learning for Generalization in Imbalanced Domains**|Meng Cao et.al|[paper](https://arxiv.org/abs/2601.21999)|[code](https://github.com/Alrash/NDCL.)|-|
|**2026-1-29**|**MMGRid: Navigating Temporal-aware and Cross-domain Generative Recommendation via Model Merging**|Tianjun Wei et.al|[paper](https://arxiv.org/abs/2601.15930)|[code](https://github.com/Joinn99/MMGRid)|<details><summary>detail</summary>https://github</details>|
|**2026-1-28**|**Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge**|Runhao Zhao et.al|[paper](https://arxiv.org/abs/2601.10485)|-|-|
|**2026-1-28**|**SFT Doesn't Always Hurt General Capabilities: Revisiting Domain-Specific Fine-Tuning in LLMs**|Jiacheng Lin et.al|[paper](https://arxiv.org/abs/2509.20758)|-|<details><summary>detail</summary>Accepted by ICLR 2026</details>|
|**2026-1-28**|**Leveraging Generative AI for Enhancing Domain-Driven Software Design**|Götz-Henrik Wiegand et.al|[paper](https://arxiv.org/abs/2601.20909)|-|<details><summary>detail</summary>Part of the Proceedings of the Upper-Rhine Artificial Intelligence Symposium 2024</details>|

## vision language

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2026-2-4**|**When LLaVA Meets Objects: Token Composition for Vision-Language-Models**|Soumya Jahagirdar et.al|[paper](https://arxiv.org/abs/2602.04864)|-|-|
|**2026-2-4**|**VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?**|Qing'an Liu et.al|[paper](https://arxiv.org/abs/2602.04802)|[code](https://github.com/QingAnLiu/VISTA-Bench.)|-|
|**2026-2-4**|**Annotation Free Spacecraft Detection and Segmentation using Vision Language Models**|Samet Hicsonmez et.al|[paper](https://arxiv.org/abs/2602.04699)|[code](https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.)|<details><summary>detail</summary>ICRA 2026</details>|
|**2026-2-4**|**Understanding Degradation with Vision Language Model**|Guanzhou Lan et.al|[paper](https://arxiv.org/abs/2602.04565)|-|-|
|**2026-2-4**|**OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models**|Yufeng Zhong et.al|[paper](https://arxiv.org/abs/2601.21639)|-|-|
|**2026-2-4**|**Vision-aligned Latent Reasoning for Multi-modal Large Language Model**|Byungwoo Jeon et.al|[paper](https://arxiv.org/abs/2602.04476)|-|-|
|**2026-2-4**|**User-Feedback-Driven Adaptation for Vision-and-Language Navigation**|Yongqiang Yu et.al|[paper](https://arxiv.org/abs/2512.10322)|-|-|
|**2026-2-4**|**PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence**|Xiaopeng Lin et.al|[paper](https://arxiv.org/abs/2512.16793)|-|-|
|**2026-2-4**|**When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models**|Jaehyun Kwak et.al|[paper](https://arxiv.org/abs/2602.04356)|[code](https://github.com/jackwaky/SAGA.)|<details><summary>detail</summary>Pre-print</details>|
|**2026-2-4**|**Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models**|Sichu Liang et.al|[paper](https://arxiv.org/abs/2602.04355)|-|-|
|**2026-2-4**|**Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner**|Qian-Wei Wang et.al|[paper](https://arxiv.org/abs/2602.04337)|-|-|
|**2026-2-4**|**Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders**|Yizhou Wang et.al|[paper](https://arxiv.org/abs/2507.03262)|-|<details><summary>detail</summary>accepted by ICLR2026</details>|
|**2026-2-4**|**GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning**|Guoqing Ma et.al|[paper](https://arxiv.org/abs/2602.04315)|[code](https://github.com/AIGeeksGroup/GeneralVLA.)|-|
|**2026-2-4**|**MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models**|Xiongtao Sun et.al|[paper](https://arxiv.org/abs/2511.16940)|-|-|
|**2026-2-4**|**A Survey on Vision-Language-Action Models for Embodied AI**|Yueen Ma et.al|[paper](https://arxiv.org/abs/2405.14093)|[code](https://github.com/yueen-ma/Awesome-VLA.)|<details><summary>detail</summary>Project page: https://github</details>|

