## CV Papers Daily
- [source-free](#source-free)
- [object detection](#object-detection)
- [domain adaptation](#domain-adaptation)
- [domain generalization](#domain-generalization)
- [vision language](#vision-language)


## Updated on 2026.02.16

## source-free

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2026-2-9**|**Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation**|Shanshan Wang et.al|[paper](https://arxiv.org/abs/2602.08730)|[code](https://github.com/soloiro/CGA)|-|
|**2026-2-9**|**USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation**|Yingxu Wang et.al|[paper](https://arxiv.org/abs/2602.08431)|-|-|
|**2026-2-2**|**Rethinking Test-Time Training: Tilting The Latent Distribution For Few-Shot Source-Free Adaptation**|Tahir Qasim Syed et.al|[paper](https://arxiv.org/abs/2602.02633)|-|-|
|**2026-1-30**|**Collision-free Source Seeking and Flocking Control of Multi-agents with Connectivity Preservation**|Tinghua Li et.al|[paper](https://arxiv.org/abs/2301.04576)|-|<details><summary>detail</summary>Published in IEEE Transactions on Automatic Control</details>|
|**2026-1-29**|**Source Coding with Free Bits and the Multi-Way Number Partitioning Problem**|Niloufar Ahmadypour et.al|[paper](https://arxiv.org/abs/2009.02710)|-|-|
|**2026-1-28**|**Beyond Retraining: Training-Free Unknown Class Filtering for Source-Free Open Set Domain Adaptation of Vision-Language Models**|Yongguang Li et.al|[paper](https://arxiv.org/abs/2504.14224)|-|<details><summary>detail</summary>Core methods unchanged</details>|
|**2026-1-28**|**A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency**|Debopom Sutradhar et.al|[paper](https://arxiv.org/abs/2601.20284)|-|<details><summary>detail</summary>Manuscript under review in IEEE Transactions on Image Processing</details>|
|**2026-1-24**|**Source-Free Domain Adaptation by Optimizing Batch-Wise Cosine Similarity**|Harsharaj Pathak et.al|[paper](https://arxiv.org/abs/2601.17408)|-|-|
|**2026-1-23**|**Model-free source seeking of exponentially convergent unicycle: theoretical and robotic experimental results**|Rohan Palanikumar et.al|[paper](https://arxiv.org/abs/2511.00752)|-|-|
|**2026-1-20**|**Beyond Boundaries: Leveraging Vision Foundation Models for Source-Free Object Detection**|Huizai Yao et.al|[paper](https://arxiv.org/abs/2511.07301)|-|<details><summary>detail</summary>AAAI 2026</details>|
|**2026-1-19**|**Towards Unbiased Source-Free Object Detection via Vision Foundation Models**|Zhi Cai et.al|[paper](https://arxiv.org/abs/2601.12765)|-|-|
|**2026-1-18**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|[code](https://github.com/tntek/CausalDA.)|-|
|**2026-1-16**|**GMM-COMET: Continual Source-Free Universal Domain Adaptation via a Mean Teacher and Gaussian Mixture Model-Based Pseudo-Labeling**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2601.11161)|[code](https://github.com/pascalschlachter/GMM-COMET.)|-|
|**2026-1-13**|**SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling**|Xi Chen et.al|[paper](https://arxiv.org/abs/2601.08608)|[code](https://github.com/chenxi52/SfMamba.)|-|
|**2026-1-13**|**Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation**|Yuan Gao et.al|[paper](https://arxiv.org/abs/2601.08375)|-|-|

## object detection

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2026-2-13**|**Detecting Object Tracking Failure via Sequential Hypothesis Testing**|Alejandro Monroy Mu√±oz et.al|[paper](https://arxiv.org/abs/2602.12983)|-|<details><summary>detail</summary>Accepted in WACV workshop "Real World Surveillance: Applications and Challenges</details>|
|**2026-2-13**|**Robustness of Object Detection of Autonomous Vehicles in Adverse Weather Conditions**|Fox Pettersen et.al|[paper](https://arxiv.org/abs/2602.12902)|-|-|
|**2026-2-11**|**Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection**|Tao Wang et.al|[paper](https://arxiv.org/abs/2602.07512)|[code](https://github.com/twangnh/zoomdet_code.)|<details><summary>detail</summary>paper accepted by ISPRS Journal of Photogrammetry and Remote Sensing ( IF=12</details>|
|**2026-2-11**|**HyperDet: 3D Object Detection with Hyper 4D Radar Point Clouds**|Yichun Xiao et.al|[paper](https://arxiv.org/abs/2602.11554)|-|-|
|**2026-2-11**|**MambaFusion: Adaptive State-Space Fusion for Multimodal 3D Object Detection**|Venkatraman Narayanan et.al|[paper](https://arxiv.org/abs/2602.08126)|-|-|
|**2026-2-11**|**Are Dense Labels Always Necessary for 3D Object Detection from Point Cloud?**|Chenqiang Gao et.al|[paper](https://arxiv.org/abs/2403.02818)|-|<details><summary>detail</summary>update</details>|
|**2026-2-11**|**FGAA-FPN: Foreground-Guided Angle-Aware Feature Pyramid Network for Oriented Object Detection**|Jialin Ma et.al|[paper](https://arxiv.org/abs/2602.10710)|-|<details><summary>detail</summary>Submitted to The Visual Computer</details>|
|**2026-2-10**|**RAWDet-7: A Multi-Scenario Benchmark for Object Detection and Description on Quantized RAW Images**|Mishal Fatima et.al|[paper](https://arxiv.org/abs/2602.03760)|-|<details><summary>detail</summary>*Equal Contribution</details>|
|**2026-2-10**|**Energy-Efficient Fast Object Detection on Edge Devices for IoT Systems**|Mas Nurul Achmadiah et.al|[paper](https://arxiv.org/abs/2602.09515)|-|-|
|**2026-2-9**|**ALIGN: Advanced Query Initialization with LiDAR-Image Guidance for Occlusion-Robust 3D Object Detection**|Janghyun Baek et.al|[paper](https://arxiv.org/abs/2512.18187)|-|-|
|**2026-2-7**|**Mamba-based Spatio-Frequency Motion Perception for Video Camouflaged Object Detection**|Xin Li et.al|[paper](https://arxiv.org/abs/2507.23601)|[code](https://github.com/BoydeLi/Vcamba.)|-|
|**2026-2-7**|**You Only Pose Once: A Minimalist's Detection Transformer for Monocular RGB Category-level 9D Multi-Object Pose Estimation**|Hakjin Lee et.al|[paper](https://arxiv.org/abs/2508.14965)|[code](https://mikigom.github.io/YOPO-project-page.)|<details><summary>detail</summary>This paper has been accepted by IEEE ICRA 2026</details>|
|**2026-2-6**|**BADet: Boundary-Aware 3D Object Detection from Point Clouds**|Rui Qian et.al|[paper](https://arxiv.org/abs/2104.10330)|[code](https://github.com/rui-qian/BADet.)|<details><summary>detail</summary>The manuscript is accepted by Pattern Recognition on 6 Jan</details>|
|**2026-2-6**|**3D Object Detection for Autonomous Driving: A Survey**|Rui Qian et.al|[paper](https://arxiv.org/abs/2106.10823)|-|<details><summary>detail</summary>The manuscript is accepted by Pattern Recognition on 14 May 2022</details>|
|**2026-2-6**|**M4-SAR: A Multi-Resolution, Multi-Polarization, Multi-Scene, Multi-Source Dataset and Benchmark for Optical-SAR Fusion Object Detection**|Chao Wang et.al|[paper](https://arxiv.org/abs/2505.10931)|[code](https://github.com/wchao0601/M4-SAR.)|-|

## domain adaptation

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2026-2-12**|**Exploring Accurate and Transparent Domain Adaptation in Predictive Healthcare via Concept-Grounded Orthogonal Inference**|Pengfei Hu et.al|[paper](https://arxiv.org/abs/2602.12542)|-|-|
|**2026-2-12**|**Discovering Hierarchy-Grounded Domains with Adaptive Granularity for Clinical Domain Generalization**|Pengfei Hu et.al|[paper](https://arxiv.org/abs/2506.06977)|-|-|
|**2026-2-12**|**UPDA: Unsupervised Progressive Domain Adaptation for No-Reference Point Cloud Quality Assessment**|Bingxu Xie et.al|[paper](https://arxiv.org/abs/2602.11969)|[code](https://github.com/yokeno1/UPDA-main.)|<details><summary>detail</summary>to be published in IEEE Transactions on Broadcasting</details>|
|**2026-2-12**|**GP2F: Cross-Domain Graph Prompting with Adaptive Fusion of Pre-trained Graph Neural Networks**|Dongxiao He et.al|[paper](https://arxiv.org/abs/2602.11629)|-|-|
|**2026-2-11**|**Move What Matters: Parameter-Efficient Domain Adaptation via Optimal Transport Flow for Collaborative Perception**|Zesheng Jia et.al|[paper](https://arxiv.org/abs/2602.11565)|-|-|
|**2026-2-11**|**Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs**|Yuming Yan et.al|[paper](https://arxiv.org/abs/2602.10740)|-|-|
|**2026-2-10**|**Learning Structure-Semantic Evolution Trajectories for Graph Domain Adaptation**|Wei Chen et.al|[paper](https://arxiv.org/abs/2602.10506)|-|<details><summary>detail</summary>accepted by ICLR 2026</details>|
|**2026-2-10**|**Learning Adaptive Distribution Alignment with Neural Characteristic Function for Graph Domain Adaptation**|Wei Chen et.al|[paper](https://arxiv.org/abs/2602.10489)|-|<details><summary>detail</summary>Accepted by ICLR 2026</details>|
|**2026-2-9**|**Impact of domain adaptation in deep learning for medical image classifications**|Yihang Wu et.al|[paper](https://arxiv.org/abs/2602.09355)|-|<details><summary>detail</summary>Accepted in IEEE SMC 2025</details>|
|**2026-2-9**|**Pave Your Own Path: Graph Gradual Domain Adaptation on Fused Gromov-Wasserstein Geodesics**|Zhichen Zeng et.al|[paper](https://arxiv.org/abs/2505.12709)|-|-|
|**2026-2-9**|**Harvest: Adaptive Photonic Switching Schedules for Collective Communication in Scale-up Domains**|Mahir Rahman et.al|[paper](https://arxiv.org/abs/2602.09188)|-|-|
|**2026-2-9**|**Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation**|Shanshan Wang et.al|[paper](https://arxiv.org/abs/2602.08730)|[code](https://github.com/soloiro/CGA)|-|
|**2026-2-9**|**USBD: Universal Structural Basis Distillation for Source-Free Graph Domain Adaptation**|Yingxu Wang et.al|[paper](https://arxiv.org/abs/2602.08431)|-|-|
|**2026-2-8**|**Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution**|Shravan Chaudhari et.al|[paper](https://arxiv.org/abs/2512.01152)|-|-|
|**2026-2-7**|**Graph Domain Adaptation via Homophily-Agnostic Reconstructing Structure**|Ruiyi Fang et.al|[paper](https://arxiv.org/abs/2602.07573)|-|<details><summary>detail</summary>Accept by AAAI2026(oral)</details>|

## domain generalization

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2026-2-13**|**Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge**|Runhao Zhao et.al|[paper](https://arxiv.org/abs/2601.10485)|-|-|
|**2026-2-13**|**M6: Multi-generator, Multi-domain, Multi-lingual and cultural, Multi-genres, Multi-instrument Machine-Generated Music Detection Databases**|Yupei Li et.al|[paper](https://arxiv.org/abs/2412.06001)|-|<details><summary>detail</summary>Scientific reports</details>|
|**2026-2-12**|**Discovering Hierarchy-Grounded Domains with Adaptive Granularity for Clinical Domain Generalization**|Pengfei Hu et.al|[paper](https://arxiv.org/abs/2506.06977)|-|-|
|**2026-2-12**|**Manifold-Aware Temporal Domain Generalization for Large Language Models**|Yiheng Yao et.al|[paper](https://arxiv.org/abs/2602.11965)|-|-|
|**2026-2-11**|**Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain**|Liz Li et.al|[paper](https://arxiv.org/abs/2602.03368)|-|-|
|**2026-2-11**|**Learning to Compose for Cross-domain Agentic Workflow Generation**|Jialiang Wang et.al|[paper](https://arxiv.org/abs/2602.11114)|-|-|
|**2026-2-10**|**A Swap-Adversarial Framework for Improving Domain Generalization in Electroencephalography-Based Parkinson's Disease Prediction**|Seongwon Jin et.al|[paper](https://arxiv.org/abs/2602.10528)|-|-|
|**2026-2-10**|**Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models**|Mingzi Cao et.al|[paper](https://arxiv.org/abs/2602.08658)|-|-|
|**2026-2-10**|**Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2602.09648)|-|-|
|**2026-2-9**|**Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains**|Jaesung Bae et.al|[paper](https://arxiv.org/abs/2602.02841)|-|-|
|**2026-2-9**|**TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation**|Yiyang Cao et.al|[paper](https://arxiv.org/abs/2602.08462)|[code](https://caoyiyang1105.github.io/TriC-Motion/.)|-|
|**2026-2-8**|**DRAGON: Domain-specific Robust Automatic Data Generation for RAG Optimization**|Haiyang Shen et.al|[paper](https://arxiv.org/abs/2505.10989)|-|-|
|**2026-2-6**|**Generating Data-Driven Reasoning Rubrics for Domain-Adaptive Reward Modeling**|Kate Sanders et.al|[paper](https://arxiv.org/abs/2602.06795)|-|-|
|**2026-2-6**|**Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains**|Ben Isselmann et.al|[paper](https://arxiv.org/abs/2602.05527)|-|<details><summary>detail</summary>Preprint</details>|
|**2026-2-6**|**KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation**|Anji Li et.al|[paper](https://arxiv.org/abs/2511.14224)|-|<details><summary>detail</summary>the 48th International Conference on Software Engineering(ICSE 2026)</details>|

## vision language

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2026-2-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|William Chen et.al|[paper](https://arxiv.org/abs/2602.13193)|-|-|
|**2026-2-13**|**Post-hoc Probabilistic Vision-Language Models**|Anton Baumann et.al|[paper](https://arxiv.org/abs/2412.06014)|[code](https://aaltoml.github.io/BayesVLM/)|<details><summary>detail</summary>Published at ICLR 2026</details>|
|**2026-2-13**|**Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding**|Wenhui Liao et.al|[paper](https://arxiv.org/abs/2602.12957)|-|<details><summary>detail</summary>Preliminary version of an ongoing project</details>|
|**2026-2-13**|**Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation**|Yichen Zhao et.al|[paper](https://arxiv.org/abs/2602.12843)|[code](https://github.com/qiuzyc/thinking_like_a_radiologist.)|-|
|**2026-2-13**|**Investigating Redundancy in Multimodal Large Language Models with Multiple Vision Encoders**|Yizhou Wang et.al|[paper](https://arxiv.org/abs/2507.03262)|[code](https://github.com/MaoSong2022/Encoder-Redundancy)|<details><summary>detail</summary>accepted by ICLR2026</details>|
|**2026-2-13**|**Assessing Vision-Language Models for Perception in Autonomous Underwater Robotic Software**|Muhammad Yousaf et.al|[paper](https://arxiv.org/abs/2602.10655)|-|-|
|**2026-2-13**|**SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios**|Tian Gao et.al|[paper](https://arxiv.org/abs/2602.08440)|[code](https://steervla.github.io/.)|-|
|**2026-2-13**|**ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training**|Rushuai Yang et.al|[paper](https://arxiv.org/abs/2602.12691)|-|-|
|**2026-2-13**|**Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution**|Rui Cai et.al|[paper](https://arxiv.org/abs/2602.12684)|[code](https://xiaomi-robotics-0.github.io)|<details><summary>detail</summary>Project page: https://xiaomi-robotics-0</details>|
|**2026-2-13**|**IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models**|Aarish Shah Mohsin et.al|[paper](https://arxiv.org/abs/2602.12659)|-|-|
|**2026-2-12**|**Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models**|Omer Faruk Deniz et.al|[paper](https://arxiv.org/abs/2602.12618)|-|<details><summary>detail</summary>2025 IEEE International Conference on Big Data (BigData)</details>|
|**2026-2-12**|**What Matters in Building Vision-Language-Action Models for Generalist Robots**|Xinghang Li et.al|[paper](https://arxiv.org/abs/2412.14058)|-|<details><summary>detail</summary>Project page: robovlms</details>|
|**2026-2-12**|**Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models**|Ali Abbasi et.al|[paper](https://arxiv.org/abs/2602.12498)|[code](https://github.com/healthylaife/NAST.)|-|
|**2026-2-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|Carl Qi et.al|[paper](https://arxiv.org/abs/2602.12405)|[code](https://sites.google.com/utexas.edu/armor)|-|
|**2026-2-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|Jacky Kwok et.al|[paper](https://arxiv.org/abs/2602.12281)|-|-|

