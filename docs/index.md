## CV Papers Daily
- [source-free](#source-free)
- [object detection](#object-detection)
- [domain adaptation](#domain-adaptation)
- [domain generalization](#domain-generalization)
- [vision language](#vision-language)


## Updated on 2025.08.06

## source-free

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-8-4**|**Model Recycling Framework for Multi-Source Data-Free Supervised Transfer Learning**|Sijia Wang et.al|[paper](https://arxiv.org/abs/2508.02039)|-|-|
|**2025-7-30**|**Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2501.02184)|-|-|
|**2025-7-28**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|
|**2025-7-26**|**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**|Sanqing Qu et.al|[paper](https://arxiv.org/abs/2403.14410)|[code](https://github.com/ispc-lab/GLC-plus.)|<details><summary>detail</summary>A substantial extension of the CVPR paper "Upcycling Models under Domain and Category Shift"</details>|
|**2025-7-23**|**SFUOD: Source-Free Unknown Object Detection**|Keon-Hee Park et.al|[paper](https://arxiv.org/abs/2507.17373)|-|<details><summary>detail</summary>This paper has been accepted by ICCV 2025</details>|
|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|
|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|
|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|
|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|
|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|
|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|
|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|
|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|
|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|
|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|

## object detection

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-8-5**|**Architectural Insights into Knowledge Distillation for Object Detection: A Comprehensive Review**|Mahdi Golizadeh et.al|[paper](https://arxiv.org/abs/2508.03317)|-|-|
|**2025-8-5**|**Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion**|Wentao Qu et.al|[paper](https://arxiv.org/abs/2508.03252)|-|-|
|**2025-8-4**|**Adversarial Attention Perturbations for Large Object Detection Transformers**|Zachary Yahn et.al|[paper](https://arxiv.org/abs/2508.02987)|[code](https://github.com/zacharyyahn/AFOG.)|<details><summary>detail</summary>ICCV 2025</details>|
|**2025-8-4**|**UniDet-D: A Unified Dynamic Spectral Attention Model for Object Detection under Adverse Weathers**|Wei Zhang et.al|[paper](https://arxiv.org/abs/2506.12324)|-|-|
|**2025-8-4**|**Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?**|Srikanth Muralidharan et.al|[paper](https://arxiv.org/abs/2508.02927)|-|-|
|**2025-8-4**|**Prototype Embedding Optimization for Human-Object Interaction Detection in Livestreaming**|Menghui Zhang et.al|[paper](https://arxiv.org/abs/2505.22011)|-|<details><summary>detail</summary>Accepted by IEEE MMSP 2025</details>|
|**2025-8-4**|**Multi-Class Human/Object Detection on Robot Manipulators using Proprioceptive Sensing**|Justin Hehli et.al|[paper](https://arxiv.org/abs/2508.02425)|-|-|
|**2025-8-4**|**Enhancing Object Discovery for Unsupervised Instance Segmentation and Object Detection**|Xingyu Feng et.al|[paper](https://arxiv.org/abs/2508.02386)|-|-|
|**2025-8-4**|**Unleashing the Temporal Potential of Stereo Event Cameras for Continuous-Time 3D Object Detection**|Jae-Young Kang et.al|[paper](https://arxiv.org/abs/2508.02288)|[code](https://github.com/mickeykang16/Ev-Stereo3D.)|<details><summary>detail</summary>ICCV 2025</details>|
|**2025-8-4**|**Unified Category-Level Object Detection and Pose Estimation from RGB Images using 3D Prototypes**|Tom Fischer et.al|[paper](https://arxiv.org/abs/2508.02157)|[code](https://github.com/Fischer-Tom/unified-detection-and-pose-estimation.)|<details><summary>detail</summary>Published at ICCV 2025</details>|
|**2025-8-4**|**Beyond RGB and Events: Enhancing Object Detection under Adverse Lighting with Monocular Normal Maps**|Mingjie Liu et.al|[paper](https://arxiv.org/abs/2508.02127)|-|-|
|**2025-8-4**|**YOLOv1 to YOLOv11: A Comprehensive Survey of Real-Time Object Detection Innovations and Challenges**|Manikanta Kotthapalli et.al|[paper](https://arxiv.org/abs/2508.02067)|-|-|
|**2025-8-3**|**Self-Supervised YOLO: Leveraging Contrastive Learning for Label-Efficient Object Detection**|Manikanta Kotthapalli et.al|[paper](https://arxiv.org/abs/2508.01966)|-|-|
|**2025-8-3**|**Collaborative Perceiver: Elevating Vision-based 3D Object Detection via Local Density-Aware Spatial Occupancy**|Jicheng Yuan et.al|[paper](https://arxiv.org/abs/2507.21358)|[code](https://github.com/jichengyuan/Collaborative-Perceiver.)|<details><summary>detail</summary>The manuscript has been accepted by ICONIP2025</details>|
|**2025-8-3**|**Collaborative Novel Object Discovery and Box-Guided Cross-Modal Alignment for Open-Vocabulary 3D Object Detection**|Yang Cao et.al|[paper](https://arxiv.org/abs/2406.00830)|[code](https://github.com/yangcaoai/CoDA_NeurIPS2023)|<details><summary>detail</summary>Code Page: https://github</details>|

## domain adaptation

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-8-5**|**Intrusion Detection in Heterogeneous Networks with Domain-Adaptive Multi-Modal Learning**|Mabin Umman Varghese et.al|[paper](https://arxiv.org/abs/2508.03517)|-|-|
|**2025-8-5**|**MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation**|Yazhou Zhu et.al|[paper](https://arxiv.org/abs/2508.03511)|[code](https://github.com/YazhouZhu19/MAUP.)|<details><summary>detail</summary>Accepted by MICCAI 2025</details>|
|**2025-8-5**|**From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation**|Rongyao Cai et.al|[paper](https://arxiv.org/abs/2507.20968)|-|-|
|**2025-8-5**|**Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation**|Jun Luo et.al|[paper](https://arxiv.org/abs/2508.03300)|[code](https://github.com/ROUJINN/SDGPA)|<details><summary>detail</summary>IROS 2025</details>|
|**2025-8-5**|**CORE-ReID: Comprehensive Optimization and Refinement through Ensemble fusion in Domain Adaptation for person re-identification**|Trinh Quoc Nguyen et.al|[paper](https://arxiv.org/abs/2508.03064)|[code](https://github.com/TrinhQuocNguyen/CORE-ReID.)|<details><summary>detail</summary>Report number:2674-113X/3/2/12</details>|
|**2025-8-4**|**MoExDA: Domain Adaptation for Edge-based Action Recognition**|Takuya Sugimoto et.al|[paper](https://arxiv.org/abs/2508.02981)|-|-|
|**2025-8-4**|**Wi-CBR: Salient-aware Adaptive WiFi Sensing for Cross-domain Behavior Recognition**|Ruobei Zhang et.al|[paper](https://arxiv.org/abs/2506.11616)|-|-|
|**2025-8-3**|**Probabilistic Domain Adaptation for Biomedical Image Segmentation**|Anwai Archit et.al|[paper](https://arxiv.org/abs/2303.11790)|[code](https://github.com/computational-cell-analytics/Probabilistic-Domain-Adaptation.)|<details><summary>detail</summary>Published in ICCVW (BioImage Computing) 2025</details>|
|**2025-8-3**|**OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets**|Maziyar Panahi et.al|[paper](https://arxiv.org/abs/2508.01630)|-|-|
|**2025-8-2**|**A Comprohensive Review of Domain Adaptation Techniques for Agricultural Image Analysis in Precision Agriculture**|Xing Hu et.al|[paper](https://arxiv.org/abs/2506.05972)|-|-|
|**2025-8-1**|**Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning**|Yingxu Wang et.al|[paper](https://arxiv.org/abs/2508.00716)|-|-|
|**2025-8-1**|**MPCA-based Domain Adaptation for Transfer Learning in Ultrasonic Guided Waves**|Lucio Pinello et.al|[paper](https://arxiv.org/abs/2508.02726)|-|-|
|**2025-8-1**|**Deep Learning-Based Rate-Adaptive CSI Feedback for Wideband XL-MIMO Systems in the Near-Field Domain**|Zhenyu Liu et.al|[paper](https://arxiv.org/abs/2508.00626)|-|-|
|**2025-8-1**|**AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation**|Itay Nakash et.al|[paper](https://arxiv.org/abs/2503.19693)|-|-|
|**2025-7-31**|**Multi-Prompt Progressive Alignment for Multi-Source Unsupervised Domain Adaptation**|Haoran Chen et.al|[paper](https://arxiv.org/abs/2507.23373)|-|-|

## domain generalization

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-8-5**|**FedSemiDG: Domain Generalized Federated Semi-supervised Medical Image Segmentation**|Zhipeng Deng et.al|[paper](https://arxiv.org/abs/2501.07378)|-|-|
|**2025-8-5**|**LLM-Generated Heuristics for AI Planning: Do We Even Need Domain-Independence Anymore?**|Alexander Tuisov et.al|[paper](https://arxiv.org/abs/2501.18784)|-|-|
|**2025-8-5**|**Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation**|Jun Luo et.al|[paper](https://arxiv.org/abs/2508.03300)|[code](https://github.com/ROUJINN/SDGPA)|<details><summary>detail</summary>IROS 2025</details>|
|**2025-8-4**|**Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data**|Kota Dohi et.al|[paper](https://arxiv.org/abs/2409.16647)|-|-|
|**2025-8-4**|**Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation**|Xinhui Li et.al|[paper](https://arxiv.org/abs/2508.03007)|-|-|
|**2025-8-3**|**SpectralX: Parameter-efficient Domain Generalization for Spectral Remote Sensing Foundation Models**|Yuxiang Zhang et.al|[paper](https://arxiv.org/abs/2508.01731)|[code](https://github.com/YuxiangZhang-BIT.)|-|
|**2025-8-2**|**Are Domain Generalization Benchmarks with Accuracy on the Line Misspecified?**|Olawale Salaudeen et.al|[paper](https://arxiv.org/abs/2504.00186)|-|<details><summary>detail</summary>Published in TMLR 08/25</details>|
|**2025-8-2**|**Domain Generalized Stereo Matching with Uncertainty-guided Data Augmentation**|Shuangli Du et.al|[paper](https://arxiv.org/abs/2508.01303)|-|-|
|**2025-7-31**|**Learning Semantic Directions for Feature Augmentation in Domain-Generalized Medical Segmentation**|Yingkai Wang et.al|[paper](https://arxiv.org/abs/2507.23326)|-|-|
|**2025-7-30**|**Rethink Domain Generalization in Heterogeneous Sequence MRI Segmentation**|Zheyuan Zhang et.al|[paper](https://arxiv.org/abs/2507.23110)|[code](https://pancreasdg.netlify.app.)|-|
|**2025-7-30**|**A Unified Analysis of Generalization and Sample Complexity for Semi-Supervised Domain Adaptation**|Elif Vural et.al|[paper](https://arxiv.org/abs/2507.22632)|-|-|
|**2025-7-29**|**Domain Generalization and Adaptation in Intensive Care with Anchor Regression**|Malte Londschien et.al|[paper](https://arxiv.org/abs/2507.21783)|-|-|
|**2025-7-28**|**Exploring Probabilistic Modeling Beyond Domain Generalization for Semantic Segmentation**|I-Hsiang Chen et.al|[paper](https://arxiv.org/abs/2507.21367)|-|<details><summary>detail</summary>Accepted by ICCV2025</details>|
|**2025-7-26**|**FedS2R: One-Shot Federated Domain Generalization for Synthetic-to-Real Semantic Segmentation in Autonomous Driving**|Tao Lian et.al|[paper](https://arxiv.org/abs/2507.19881)|-|-|
|**2025-7-25**|**PennyCoder: Efficient Domain-Specific LLMs for PennyLane-Based Quantum Code Generation**|Abdul Basit et.al|[paper](https://arxiv.org/abs/2507.19562)|-|-|

## vision language

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-8-5**|**Can Large Vision-Language Models Understand Multimodal Sarcasm?**|Xinyu Wang et.al|[paper](https://arxiv.org/abs/2508.03654)|[code](https://github.com/cp-cp/LVLM-MSA.)|<details><summary>detail</summary>Accepted by CIKM 2025</details>|
|**2025-8-5**|**Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI**|Benjamin Raphael Ernhofer et.al|[paper](https://arxiv.org/abs/2505.05895)|-|-|
|**2025-8-5**|**DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping**|Yifan Zhong et.al|[paper](https://arxiv.org/abs/2502.20900)|[code](https://dexgraspvla.github.io.)|-|
|**2025-8-5**|**CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation**|Kun Song et.al|[paper](https://arxiv.org/abs/2508.03526)|-|-|
|**2025-8-5**|**IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves**|Ruofan Wang et.al|[paper](https://arxiv.org/abs/2411.00827)|-|-|
|**2025-8-5**|**IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models**|Jiabing Yang et.al|[paper](https://arxiv.org/abs/2508.03469)|-|-|
|**2025-8-5**|**Talking to DINO: Bridging Self-Supervised Vision Backbones with Language for Open-Vocabulary Segmentation**|Luca Barsellotti et.al|[paper](https://arxiv.org/abs/2411.19331)|[code](https://lorebianchi98.github.io/Talk2DINO/.)|-|
|**2025-8-5**|**MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention**|Yuqi Pang et.al|[paper](https://arxiv.org/abs/2507.22805)|-|-|
|**2025-8-5**|**VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation**|Yufei Xue et.al|[paper](https://arxiv.org/abs/2508.03351)|-|-|
|**2025-8-5**|**GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations**|Xinwei Liu et.al|[paper](https://arxiv.org/abs/2508.03209)|-|-|
|**2025-8-5**|**SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision**|Zhaoxu Li et.al|[paper](https://arxiv.org/abs/2508.03177)|-|-|
|**2025-8-5**|**Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery**|Sai Ma et.al|[paper](https://arxiv.org/abs/2508.03127)|[code](https://github.com/papersubmit1/landsat30-au.)|-|
|**2025-8-5**|**Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models**|Zaiying Zhao et.al|[paper](https://arxiv.org/abs/2508.03079)|-|<details><summary>detail</summary>the Responsible Generative AI (ReGenAI) Workshop</details>|
|**2025-8-5**|**CHIRP: A Fine-Grained Benchmark for Open-Ended Response Evaluation in Vision-Language Models**|Alexis Roger et.al|[paper](https://arxiv.org/abs/2501.09672)|-|-|
|**2025-8-4**|**Negation-Aware Test-Time Adaptation for Vision-Language Models**|Haochen Han et.al|[paper](https://arxiv.org/abs/2507.19064)|[code](https://github.com/hhc1997/NEAT.)|<details><summary>detail</summary>This paper will be submitted to the IEEE for possible publication</details>|

