## CV Papers Daily
- [source-free](#source-free)
- [object detection](#object-detection)
- [domain adaptation](#domain-adaptation)
- [domain generalization](#domain-generalization)
- [vision language](#vision-language)


## Updated on 2023.10.08

## source-free

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2023-10-2**|**Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation**|Shivang Chopraet.al|[paper](https://arxiv.org/abs/2310.01701)|-|-|
|**2023-9-30**|**In Search for a Generalizable Method for Source Free Domain Adaptation**|M Boudiaf et.al|[paper](https://arxiv.org/abs/2302.06658)|[code](https://paperswithcode.com/paper/in-search-for-a-generalizable-method-for)|-|
|**2023-9-27**|**MAPS: A Noise-Robust Progressive Learning Approach for Source-Free Domain Adaptive Keypoint Detection**|Y Ding et.al|[paper](https://arxiv.org/abs/2302.04589)|[code](https://github.com/yuhed/maps)|-|
|**2023-9-25**|**Source-free Depth for Object Pop-out**|Zongwei Wuet.al|[paper](https://arxiv.org/abs/2212.05370)|-|<details><summary>detail</summary>ICCV 2023</details>|
|**2023-9-23**|**Dual-Reference Source-Free Active Domain Adaptation for Nasopharyngeal Carcinoma Tumor Segmentation across Multiple Hospitals**|Hongqiu Wanget.al|[paper](https://arxiv.org/abs/2309.13401)|-|-|
|**2023-9-20**|**Universal source-free domain adaptation method for cross-domain fault diagnosis of machines**|Y Zhang et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0888327023000663)|-|<details><summary>detail</summary>Mechanical Systems and…, 2023 Elsevier</details>|
|**2023-9-19**|**Source-free Active Domain Adaptation for Diabetic Retinopathy Grading Based on Ultra-wide-field Fundus Image**|Jinye Ranet.al|[paper](https://arxiv.org/abs/2309.10619)|-|-|
|**2023-9-18**|**UPL-SFDA: Uncertainty-aware Pseudo Label Guided Source-Free Domain Adaptation for Medical Image Segmentation**|Jianghao Wuet.al|[paper](https://arxiv.org/abs/2309.10244)|-|-|
|**2023-9-18**|**Contrast and Clustering: Learning Neighborhood Pair Representation for Source-free Domain Adaptation**|Y Chen et.al|[paper](https://arxiv.org/abs/2301.13428)|[code](https://github.com/yukilulu/cac)|-|
|**2023-9-14**|**TIDo: Source-free Task Incremental Learning in Non-stationary Environments**|AK Ambastha et.al|[paper](https://arxiv.org/abs/2301.12055)|[code](https://paperswithcode.com/paper/tido-source-free-task-incremental-learning-in)|-|
|**2023-9-14**|**Adversarial Learning Networks: Source-free Unsupervised Domain Incremental Learning**|AK Ambastha et.al|[paper](https://arxiv.org/abs/2301.12054)|[code](https://paperswithcode.com/paper/adversarial-learning-networks-source-free)|-|
|**2023-9-9**|**Cross-platform privacy-preserving CT image COVID-19 diagnosis based on source-free domain adaptation**|Y Feng et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0950705123000746)|-|<details><summary>detail</summary>Knowledge Based Systems, 2023 Elsevier</details>|
|**2023-9-6**|**Source-free Subject Adaptation for EEG-based Visual Recognition**|P Lee et.al|[paper](https://arxiv.org/abs/2301.08448)|[code](https://github.com/DeepBCI/Deep-BCI)|-|
|**2023-9-6**|**When Source-Free Domain Adaptation Meets Label Propagation**|C Wu et.al|[paper](https://arxiv.org/abs/2301.08413)|-|-|
|**2023-9-4**|**Source-Free Domain Adaptive Detection of Concealed Objects in Passive Millimeter-Wave Images**|H Yang et.al|[paper](https://ieeexplore.ieee.org/abstract/document/10019315/)|-|<details><summary>detail</summary>IEEE Transactions on…, 2023 ieeexplore.ieee.org</details>|
|**2023-9-1**|**Trust your Good Friends: Source-free Domain Adaptation by Reciprocal Neighborhood Clustering**|Shiqi Yanget.al|[paper](https://arxiv.org/abs/2309.00528)|-|<details><summary>detail</summary>Accepted by IEEE TPAMI</details>|
|**2023-8-30**|**Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with Wavelet Augmentation Transformer**|Yuang Aiet.al|[paper](https://arxiv.org/abs/2303.17783)|-|-|
|**2023-8-28**|**Local-Global Pseudo-label Correction for Source-free Domain Adaptive Medical Image Segmentation**|Yanyu Yeet.al|[paper](https://arxiv.org/abs/2308.14312)|-|-|
|**2023-8-27**|**Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation**|Sunandini Sanyalet.al|[paper](https://arxiv.org/abs/2308.14023)|[code](http://val.cds.iisc.ac.in/DSiT-SFDA)|<details><summary>detail</summary>ICCV 2023</details>|
|**2023-8-26**|**Prior-guided Source-free Domain Adaptation for Human Pose Estimation**|Dripta S. Raychaudhuriet.al|[paper](https://arxiv.org/abs/2308.13954)|-|<details><summary>detail</summary>ICCV 2023</details>|
|**2023-8-25**|**Rethinking the Role of Pre-Trained Networks in Source-Free Domain Adaptation**|Wenyu Zhanget.al|[paper](https://arxiv.org/abs/2212.07585)|-|<details><summary>detail</summary>ICCV 2023</details>|
|**2023-8-23**|**Source-Free Collaborative Domain Adaptation via Multi-Perspective Feature Enrichment for Functional MRI Analysis**|Yuqi Fanget.al|[paper](https://arxiv.org/abs/2308.12495)|-|-|
|**2023-8-23**|**Black-box Source-free Domain Adaptation via Two-stage Knowledge Distillation**|Shuai Wanget.al|[paper](https://arxiv.org/abs/2305.07881)|-|<details><summary>detail</summary>The short version is accepted by IJCAI 1st International Workshop on Generalizing from Limited Resources in the Open World</details>|
|**2023-8-22**|**SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets**|Cody Simonset.al|[paper](https://arxiv.org/abs/2308.11880)|[code](https://github.com/csimo005/SUMMIT.)|-|
|**2023-8-22**|**The Unreasonable Effectiveness of Large Language-Vision Models for Source-free Video Domain Adaptation**|Giacomo Zaraet.al|[paper](https://arxiv.org/abs/2308.09139)|[code](https://github.com/giaczara/dallv)|<details><summary>detail</summary>ICCV2023</details>|

## object detection

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2023-10-5**|**Building Flyweight FLIM-based CNNs with Adaptive Decoding for Object Detection**|Leonardo de Melo Joaoet.al|[paper](https://arxiv.org/abs/2306.14840)|-|-|
|**2023-10-5**|**SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with 4D Imaging Radar**|Jianan Liuet.al|[paper](https://arxiv.org/abs/2307.10784)|-|<details><summary>detail</summary>Accepted by IEEE Transactions on Intelligent Vehicles</details>|
|**2023-10-5**|**Towards Robust 3D Object Detection In Rainy Conditions**|Aldi Piroliet.al|[paper](https://arxiv.org/abs/2310.00944)|-|<details><summary>detail</summary>Published at IEEE International Conference on Intelligent Transportation Systems ITSC 2023</details>|
|**2023-10-5**|**Real-time Multi-modal Object Detection and Tracking on Edge for Regulatory Compliance Monitoring**|Jia Syuen Limet.al|[paper](https://arxiv.org/abs/2310.03333)|-|-|
|**2023-10-5**|**E-detector: Asynchronous Spatio-temporal for Event-based Object Detection in Intelligent Transportation System**|S Zhang et.al|[paper](https://dl.acm.org/doi/abs/10.1145/3584361)|-|<details><summary>detail</summary>ACM Transactions on Multimedia…, 2023 dl.acm.org</details>|
|**2023-10-4**|**CoDA: Collaborative Novel Box Discovery and Cross-modal Alignment for Open-vocabulary 3D Object Detection**|Yang Caoet.al|[paper](https://arxiv.org/abs/2310.02960)|[code](https://yangcaoai.github.io/publications/CoDA.html)|<details><summary>detail</summary>Accepted by NeurIPS 2023</details>|
|**2023-10-4**|**CoBEV: Elevating Roadside 3D Object Detection with Depth and Height Complementarity**|Hao Shiet.al|[paper](https://arxiv.org/abs/2310.02815)|[code](https://github.com/MasterHow/CoBEV.)|<details><summary>detail</summary>The source code will be made publicly available at https://github</details>|
|**2023-10-4**|**Land-cover change detection using paired OpenStreetMap data and optical high-resolution imagery via object-guided Transformer**|Hongruixuan Chenet.al|[paper](https://arxiv.org/abs/2310.02674)|-|-|
|**2023-10-4**|**…Simultaneous Defects Visualizing Algorithm for Both Macro and Micro Defects Based on Nonlinear Lamb Wave with an Application of Faster R-Cnn Object Detection**|Y Lee et.al|[paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4362451)|-|<details><summary>detail</summary>Available at SSRN 4362451 papers.ssrn.com</details>|
|**2023-10-4**|**YOGA: Deep Object Detection in the Wild with Lightweight Feature Learning and Multiscale Attention**|R Sunkara et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0031320323001516)|[code](https://paperswithcode.com/paper/yoga-deep-object-detection-in-the-wild-with)|<details><summary>detail</summary>Pattern Recognition, 2023 Elsevier</details>|
|**2023-10-4**|**Spectral-Spatial Feature Enhancement Algorithm for Nighttime Object Detection and Tracking**|Y Lv et.al|[paper](https://www.mdpi.com/2073-8994/15/2/546)|-|<details><summary>detail</summary>Symmetry, 2023 mdpi.com</details>|
|**2023-10-4**|**CRRNet: Channel Relation Reasoning Network for Salient Object Detection**|S Gao et.al|[paper](https://link.springer.com/chapter/10.1007/978-981-99-0301-6_2)|-|<details><summary>detail</summary>…Conference, CCF CIRAC 2022, Xi'an…, 2023 Springer</details>|
|**2023-10-4**|**Iterative Fusion and Dual Enhancement for Accurate and Efficient Object Detection**|Z Duan et.al|[paper](https://www.worldscientific.com/doi/abs/10.1142/S0218126623502328)|-|<details><summary>detail</summary>Journal of Circuits…, 2023 World Scientific</details>|
|**2023-10-4**|**CTA-FPN: Channel-Target Attention Feature Pyramid Network for Prohibited Object Detection in X-ray Images**|Y Zhang et.al|[paper](https://www.researchsquare.com/article/rs-2584406/latest.pdf)|-|<details><summary>detail</summary>2023 researchsquare.com</details>|
|**2023-10-4**|**Hierarchical Cross-modal Transformer for RGB-D Salient Object Detection**|H Chen et.al|[paper](https://arxiv.org/abs/2302.08052)|[code](https://github.com/liuzywen/swinnet)|-|
|**2023-10-4**|**3M3D: Multi-view, Multi-path, Multi-representation for 3D Object Detection**|J Park et.al|[paper](https://arxiv.org/abs/2302.08231)|[code](https://paperswithcode.com/paper/3m3d-multi-view-multi-path-multi)|-|
|**2023-10-4**|**Research on road object detection algorithm based on improved YOLOX**|T Yang et.al|[paper](https://arxiv.org/abs/2302.08156)|[code](https://paperswithcode.com/paper/research-on-road-object-detection-algorithm)|-|
|**2023-10-3**|**RCS-YOLO: A Fast and High-Accuracy Object Detector for Brain Tumor Detection**|Ming Kanget.al|[paper](https://arxiv.org/abs/2307.16412)|[code](https://github.com/mkang315/RCS-YOLO.)|<details><summary>detail</summary>MSC Class:68U10 (Primary) 68T10</details>|
|**2023-10-3**|**MarineDet: Towards Open-Marine Object Detection**|Liang Haixinet.al|[paper](https://arxiv.org/abs/2310.01931)|-|-|
|**2023-10-3**|**LXL: LiDAR Excluded Lean 3D Object Detection with 4D Imaging Radar and Camera Fusion**|Weiyi Xionget.al|[paper](https://arxiv.org/abs/2307.00724)|-|<details><summary>detail</summary>Accepted by IEEE Transactions on Intelligent Vehicles</details>|
|**2023-10-2**|**Pixel-Aligned Recurrent Queries for Multi-View 3D Object Detection**|Yiming Xieet.al|[paper](https://arxiv.org/abs/2310.01401)|[code](https://ymingxie.github.io/parq)|<details><summary>detail</summary>ICCV 2023</details>|
|**2023-10-2**|**DST-Det: Simple Dynamic Self-Training for Open-Vocabulary Object Detection**|Shilin Xuet.al|[paper](https://arxiv.org/abs/2310.01393)|[code](https://github.com/xushilin1/dst-det.)|-|
|**2023-10-2**|**LS-VOS: Identifying Outliers in 3D Object Detections Using Latent Space Virtual Outlier Synthesis**|Aldi Piroliet.al|[paper](https://arxiv.org/abs/2310.00952)|-|<details><summary>detail</summary>Published at IEEE International Conference on Intelligent Transportation Systems ITSC 2023</details>|
|**2023-10-2**|**Every Dataset Counts: Scaling up Monocular 3D Object Detection with Joint Datasets Training**|Fulong Maet.al|[paper](https://arxiv.org/abs/2310.00920)|-|-|
|**2023-10-1**|**You Do Not Need Additional Priors in Camouflage Object Detection**|Yuchen Donget.al|[paper](https://arxiv.org/abs/2310.00702)|-|-|

## domain adaptation

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2023-10-5**|**Mitigating the Influence of Domain Shift in Skin Lesion Classification: A Benchmark Study of Unsupervised Domain Adaptation Methods on Dermoscopic Images**|Sireesha Chamarthiet.al|[paper](https://arxiv.org/abs/2310.03432)|-|-|
|**2023-10-5**|**Multi-label Image Classification using Adaptive Graph Convolutional Networks: from a Single Domain to Multiple Domains**|Indel Pal Singhet.al|[paper](https://arxiv.org/abs/2301.04494)|-|-|
|**2023-10-5**|**Continual Test-time Domain Adaptation via Dynamic Sample Selection**|Yanshuo Wanget.al|[paper](https://arxiv.org/abs/2310.03335)|-|-|
|**2023-10-5**|**Reformulating Domain Adaptation of Large Language Models as Adapt-Retrieve-Revise**|Zhen wanet.al|[paper](https://arxiv.org/abs/2310.03328)|-|<details><summary>detail</summary>Under submission to ICLR 2024</details>|
|**2023-10-4**|**Gradual Domain Adaptation via Normalizing Flows**|Shogo Sagawaet.al|[paper](https://arxiv.org/abs/2206.11492)|-|-|
|**2023-10-4**|**Kurcuma: a kitchen utensil recognition collection for unsupervised domain adaptation**|A Rosello et.al|[paper](https://link.springer.com/article/10.1007/s10044-023-01147-x)|-|<details><summary>detail</summary>Mas, AJ Gallego… Pattern Analysis and…, 2023 Springer</details>|
|**2023-10-4**|**An improved multi-source domain adaptation network for inter-subject mental fatigue detection based on DANN**|K Chen et.al|[paper](https://www.degruyter.com/document/doi/10.1515/bmt-2022-0354/html)|-|<details><summary>detail</summary>Biomedical Engineering…, 2023 degruyter.com</details>|
|**2023-10-4**|**A multi-source transfer learning model based on LSTM and domain adaptation for building energy prediction**|H Lu et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0142061523000819)|-|<details><summary>detail</summary>International Journal of…, 2023 Elsevier</details>|
|**2023-10-4**|**Open Set Domain Adaptation with Latent Structure Discovery and Kernelized Classifier Learning**|Y Tang et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0925231223001509)|-|<details><summary>detail</summary>Neurocomputing, 2023 Elsevier</details>|
|**2023-10-4**|**Towards Real-Time Online Unsupervised Domain Adaptation for Real-World Person Re-identification**|C Neff et.al|[paper](https://www.researchsquare.com/article/rs-2588554/latest.pdf)|-|<details><summary>detail</summary>2023 researchsquare.com</details>|
|**2023-10-4**|**Unsupervised Domain Adaptation for MRI Volume Segmentation and Classification Using Image-to-Image Translation**|S Kondo et.al|[paper](https://arxiv.org/abs/2302.08016)|[code](https://paperswithcode.com/paper/unsupervised-domain-adaptation-for-mri-volume)|-|
|**2023-10-3**|**Learnable Data Augmentation for One-Shot Unsupervised Domain Adaptation**|Julio Ivan Davila Carrazcoet.al|[paper](https://arxiv.org/abs/2310.02201)|[code](https://github.com/IIT-PAVIS/LearnAug-UDA)|<details><summary>detail</summary>The 34th British Machine Vision Conference (BMVC 2023)</details>|
|**2023-10-3**|**Cross-City Matters: A Multimodal Remote Sensing Benchmark Dataset for Cross-City Semantic Segmentation using High-Resolution Domain Adaptation Networks**|Danfeng Honget.al|[paper](https://arxiv.org/abs/2309.16499)|[code](https://github.com/danfenghong.)|-|
|**2023-10-3**|**High-Intensified Resemblance and Statistic-Restructured Alignment in Few-Shot Domain Adaptation for Industrial-Specialized Employment**|J Petchhan et.al|[paper](https://ieeexplore.ieee.org/abstract/document/10045719/)|-|<details><summary>detail</summary>IEEE Transactions on Consumer…, 2023 ieeexplore.ieee.org</details>|
|**2023-10-3**|**KRADA: Known-region-aware Domain Alignment for Open-set Domain Adaptation in Semantic Segmentation**|C Zhou et.al|[paper](https://europepmc.org/article/ppr/ppr617459)|[code](https://github.com/chenhong-zhou/krada)|<details><summary>detail</summary>2023 europepmc.org</details>|
|**2023-10-3**|**Blade crack detection based on domain adaptation and autoencoder of multidimensional vibro-acoustic feature fusion**|J Shen et.al|[paper](https://journals.sagepub.com/doi/abs/10.1177/14759217221139134)|-|<details><summary>detail</summary>Structural Health Monitoring, 2023 journals.sagepub.com</details>|
|**2023-10-2**|**Transcending Domains through Text-to-Image Diffusion: A Source-Free Approach to Domain Adaptation**|Shivang Chopraet.al|[paper](https://arxiv.org/abs/2310.01701)|-|-|
|**2023-10-2**|**The CHiME-7 UDASE task: Unsupervised domain adaptation for conversational speech enhancement**|Simon Leglaiveet.al|[paper](https://arxiv.org/abs/2307.03533)|-|<details><summary>detail</summary>Journal ref:The 7th International Workshop on Speech Processing in Everyday Environments (CHiME)</details>|
|**2023-10-2**|**Infrared ship target segmentation based on Adversarial Domain Adaptation**|T Zhang et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0950705123000941)|-|<details><summary>detail</summary>Knowledge Based…, 2023 Elsevier</details>|
|**2023-10-1**|**Exploring Sparse Visual Prompt for Domain Adaptive Dense Prediction**|Senqiao Yanget.al|[paper](https://arxiv.org/abs/2303.09792)|-|-|
|**2023-9-30**|**ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation**|Jiaming Liuet.al|[paper](https://arxiv.org/abs/2306.04344)|-|<details><summary>detail</summary>Neurips2023 final Rating: Weak Accept</details>|
|**2023-9-29**|**Confidence-based Visual Dispersal for Few-shot Unsupervised Domain Adaptation**|Yizhe Xionget.al|[paper](https://arxiv.org/abs/2309.15575)|[code](https://github.com/Bostoncake/C-VisDiT.)|<details><summary>detail</summary>Accepted as ICCV 2023 poster (https://openaccess</details>|
|**2023-9-29**|**Few-Shot Domain Adaptation for Charge Prediction on Unprofessional Descriptions**|Jie Zhaoet.al|[paper](https://arxiv.org/abs/2309.17313)|-|-|
|**2023-9-29**|**Domain-Adaptive Learning: Unsupervised Adaptation for Histology Images with Improved Loss Function Combination**|Ravi Kant Guptaet.al|[paper](https://arxiv.org/abs/2309.17172)|-|-|
|**2023-9-28**|**PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label**|Joonhyung Parket.al|[paper](https://arxiv.org/abs/2309.16936)|-|-|

## domain generalization

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2023-10-5**|**Assessment of the Reliablity of a Model's Decision by Generalizing Attribution to the Wavelet Domain**|Gabriel Kasmiet.al|[paper](https://arxiv.org/abs/2305.14979)|-|-|
|**2023-10-5**|**Domain Generalization with Global Sample Mixup**|Y Lu et.al|[paper](https://link.springer.com/chapter/10.1007/978-3-031-25075-0_35)|-|<details><summary>detail</summary>European Conference on Computer…, 2023 Springer</details>|
|**2023-10-5**|**Cross-Domain Augmentation Diagnosis: An Adversarial Domain-Augmented Generalization Method for Fault Diagnosis under Unseen Working Conditions**|Q Li et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0951832023000868)|-|<details><summary>detail</summary>Reliability Engineering &…, 2023 Elsevier</details>|
|**2023-10-4**|**Towards Domain-Specific Features Disentanglement for Domain Generalization**|Hao Chenet.al|[paper](https://arxiv.org/abs/2310.03007)|-|-|
|**2023-10-3**|**Prompting-based Efficient Temporal Domain Generalization**|Sepidehsadat Hosseiniet.al|[paper](https://arxiv.org/abs/2310.02473)|-|-|
|**2023-10-3**|**On the Hyperparameters influencing a PINN's generalization beyond the training domain**|A Bonfanti et.al|[paper](https://arxiv.org/abs/2302.07557)|-|-|
|**2023-10-2**|**Adversarial Bayesian Augmentation for Single-Source Domain Generalization**|Sheng Chenget.al|[paper](https://arxiv.org/abs/2307.09520)|-|<details><summary>detail</summary>ICCV 2023</details>|
|**2023-10-2**|**CODA: Temporal Domain Generalization via Concept Drift Simulator**|Chia-Yuan Changet.al|[paper](https://arxiv.org/abs/2310.01508)|-|-|
|**2023-10-2**|**Domain-Agnostic Molecular Generation with Self-feedback**|Yin Fanget.al|[paper](https://arxiv.org/abs/2301.11259)|[code](https://github.com/zjunlp/MolGen.)|<details><summary>detail</summary>Work in progress</details>|
|**2023-10-2**|**Incorporating Supervised Domain Generalization into Data Augmentation**|Shohei Enomotoet.al|[paper](https://arxiv.org/abs/2310.01029)|-|-|
|**2023-10-2**|**Robust Representation Learning with Self-Distillation for Domain Generalization**|A Singh et.al|[paper](https://arxiv.org/abs/2302.06874)|[code](https://github.com/tongkunguan/ccd)|-|
|**2023-10-1**|**Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic AI Models**|Soroosh Tayebi Arastehet.al|[paper](https://arxiv.org/abs/2310.00757)|-|-|
|**2023-9-30**|**AfriSpeech-200: Pan-African Accented Speech Dataset for Clinical and General Domain ASR**|Tobi Olatunjiet.al|[paper](https://arxiv.org/abs/2310.00274)|-|<details><summary>detail</summary>TACL 2023</details>|
|**2023-9-30**|**Cross-corpora spoken language identification with domain diversification and generalization**|S Dey et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0885230823000086)|[code](https://paperswithcode.com/paper/cross-corpora-spoken-language-identification)|<details><summary>detail</summary>Computer Speech & Language, 2023 Elsevier</details>|
|**2023-9-28**|**Unlabeled Out-Of-Domain Data Improves Generalization**|Amir Hossein Saberiet.al|[paper](https://arxiv.org/abs/2310.00027)|-|-|
|**2023-9-28**|**Rethinking Domain Generalization: Discriminability and Generalizability**|Shaocong Longet.al|[paper](https://arxiv.org/abs/2309.16483)|-|-|
|**2023-9-28**|**Diverse Target and Contribution Scheduling for Domain Generalization**|Shaocong Longet.al|[paper](https://arxiv.org/abs/2309.16460)|-|-|
|**2023-9-28**|**Domain-Conditioned Normalization for Test-Time Domain Generalization**|Y Jiang et.al|[paper](https://link.springer.com/chapter/10.1007/978-3-031-25085-9_17)|-|<details><summary>detail</summary>Computer Vision–ECCV…, 2023 Springer</details>|
|**2023-9-27**|**Domain generalization across tumor types, laboratories, and species -- insights from the 2022 edition of the Mitosis Domain Generalization Challenge**|Marc Aubrevilleet.al|[paper](https://arxiv.org/abs/2309.15589)|-|-|
|**2023-9-27**|**Robust Internal Representations for Domain Generalization**|Mohammad Rostamiet.al|[paper](https://arxiv.org/abs/2309.15522)|-|<details><summary>detail</summary>to appear in AI Magazine Winter 2023 Issue</details>|
|**2023-9-27**|**CauDR: A Causality-inspired Domain Generalization Framework for Fundus-based Diabetic Retinopathy Grading**|Hao Weiet.al|[paper](https://arxiv.org/abs/2309.15493)|-|-|
|**2023-9-27**|**Domain Generalization by Functional Regression**|M Holzleitner et.al|[paper](https://arxiv.org/abs/2302.04724)|[code](https://github.com/mlr-org/mlr)|-|
|**2023-9-24**|**Leveraging Domain Relations for Domain Generalization**|H Yao et.al|[paper](https://arxiv.org/abs/2302.02609)|[code](https://github.com/rusty1s/pytorch_geometric)|-|
|**2023-9-22**|**Aggregation of Disentanglement: Reconsidering Domain Variations in Domain Generalization**|D Zhang et.al|[paper](https://arxiv.org/abs/2302.02350)|[code](https://paperswithcode.com/paper/aggregation-of-disentanglement-reconsidering)|-|
|**2023-9-20**|**Domain Generalization Emerges from Dreaming**|H Heo et.al|[paper](https://arxiv.org/abs/2302.00980)|[code](https://paperswithcode.com/paper/domain-generalization-emerges-from-dreaming)|-|

## vision language

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2023-10-5**|**Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency**|Tianhong Liet.al|[paper](https://arxiv.org/abs/2310.03734)|-|-|
|**2023-10-5**|**Revisiting the Role of Language Priors in Vision-Language Models**|Zhiqiu Linet.al|[paper](https://arxiv.org/abs/2306.01879)|[code](https://linzhiqiu.github.io/papers/visual_gpt_score/)|<details><summary>detail</summary>Website: https://linzhiqiu</details>|
|**2023-10-4**|**CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers**|Dachuan Shiet.al|[paper](https://arxiv.org/abs/2305.17455)|[code](https://github.com/sdc17/CrossGET)|<details><summary>detail</summary>Technical Report</details>|
|**2023-10-4**|**ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models**|Yi-Lin Sunget.al|[paper](https://arxiv.org/abs/2310.02998)|[code](https://ecoflap.github.io/)|<details><summary>detail</summary>Project page: https://ecoflap</details>|
|**2023-10-4**|**Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples**|Phillip Howardet.al|[paper](https://arxiv.org/abs/2310.02988)|-|-|
|**2023-10-4**|**Improving Vision Anomaly Detection with the Guidance of Language Modality**|Dong Chenet.al|[paper](https://arxiv.org/abs/2310.02821)|-|-|
|**2023-10-4**|**The Role of Linguistic Priors in Measuring Compositional Generalization of Vision-Language Models**|Chenwei Wuet.al|[paper](https://arxiv.org/abs/2310.02777)|-|-|
|**2023-10-4**|**ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks**|Zejun Liet.al|[paper](https://arxiv.org/abs/2310.02569)|-|-|
|**2023-10-3**|**MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens**|Kaizhi Zhenget.al|[paper](https://arxiv.org/abs/2310.02239)|-|-|
|**2023-10-3**|**Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond**|Liang Chenet.al|[paper](https://arxiv.org/abs/2310.02071)|-|-|
|**2023-10-3**|**HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption**|Bohan Zhaiet.al|[paper](https://arxiv.org/abs/2310.01779)|-|-|
|**2023-10-3**|**Weakly-Supervised HOI Detection from Interaction Labels Only and Language/Vision-Language Priors**|M Erhan Unal et.al|[paper](https://ui.adsabs.harvard.edu/abs/2023arXiv230305546E/abstract)|[code](https://paperswithcode.com/paper/weakly-supervised-hoi-detection-from)|-|
|**2023-10-3**|**Scaling Vision-Language Models with Sparse Mixture of Experts**|S Shen et.al|[paper](https://arxiv.org/abs/2303.07226)|[code](https://github.com/google-research/vmoe)|-|
|**2023-10-3**|**Vision-Language Models as Success Detectors**|Y Du et.al|[paper](https://arxiv.org/abs/2303.07280)|[code](https://github.com/dyabel/detpro)|-|
|**2023-10-3**|**Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images**|N Bitton-Guetta et.al|[paper](https://arxiv.org/abs/2303.07274)|[code](https://paperswithcode.com/paper/breaking-common-sense-whoops-a-vision-and)|-|
|**2023-10-2**|**Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations**|Yongshuo Zonget.al|[paper](https://arxiv.org/abs/2310.01651)|[code](https://github.com/ys-zong/FoolyourVLLMs)|-|
|**2023-10-2**|**Vision-Language Dataset Distillation**|Xindi Wuet.al|[paper](https://arxiv.org/abs/2308.07545)|-|-|
|**2023-10-2**|**MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models**|Deyao Zhuet.al|[paper](https://arxiv.org/abs/2304.10592)|[code](https://minigpt-4.github.io/.)|<details><summary>detail</summary>Project Website: https://minigpt-4</details>|
|**2023-10-2**|**GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot Attention for Vision-and-Language Navigation**|Jingyang Huoet.al|[paper](https://arxiv.org/abs/2305.17102)|-|<details><summary>detail</summary>Accepted by CVPR 2023</details>|
|**2023-10-1**|**Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models**|Z Zheng et.al|[paper](https://arxiv.org/abs/2303.06628)|[code](https://github.com/thunderbeee/zscl)|-|
|**2023-10-1**|**Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models**|J Li et.al|[paper](https://arxiv.org/abs/2303.06571)|[code](https://paperswithcode.com/paper/gradient-regulated-meta-prompt-learning-for)|-|
|**2023-10-1**|**Towards Universal Vision-language Omni-supervised Segmentation**|B Dong et.al|[paper](https://arxiv.org/abs/2303.06547)|[code](https://paperswithcode.com/paper/towards-universal-vision-language-omni)|-|
|**2023-9-30**|**Learning Grounded Vision-Language Representation for Versatile Understanding in Untrimmed Videos**|T Wang et.al|[paper](https://arxiv.org/abs/2303.06378)|[code](https://github.com/zjr2000/gvl)|-|
|**2023-9-29**|**Tag2Text: Guiding Vision-Language Model via Image Tagging**|X Huang et.al|[paper](https://arxiv.org/abs/2303.05657)|[code](https://github.com/xinyu1205/recognize-anything)|-|
|**2023-9-29**|**Robotic Applications of Pre-Trained Vision-Language Models to Various Recognition Behaviors**|K Kawaharazuka et.al|[paper](https://arxiv.org/abs/2303.05674)|-|-|

