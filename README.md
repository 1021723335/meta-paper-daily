## CV Papers Daily
- [source-free](#source-free)
- [object detection](#object-detection)
- [domain adaptation](#domain-adaptation)
- [domain generalization](#domain-generalization)
- [vision language](#vision-language)


## Updated on 2024.03.25

## source-free

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2024-3-22**|**Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation**|Xu Zheng et.al|[paper](https://arxiv.org/abs/2403.12505)|-|<details><summary>detail</summary>CVPR 2024</details>|
|**2024-3-21**|**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**|Sanqing Qu et.al|[paper](https://arxiv.org/abs/2403.14410)|-|<details><summary>detail</summary>This is a substantial extension of the CVPR 2023 paper "Upcycling Models under Domain and Category Shift"</details>|
|**2024-3-20**|**EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition**|Xu Zheng et.al|[paper](https://arxiv.org/abs/2403.14082)|-|<details><summary>detail</summary>CVPR2024</details>|
|**2024-3-20**|**When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather**|Giulia Rizzoli et.al|[paper](https://arxiv.org/abs/2403.13762)|-|-|
|**2024-3-20**|**Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with Wavelet Augmentation Transformer**|Yuang Ai et.al|[paper](https://arxiv.org/abs/2303.17783)|-|-|
|**2024-3-17**|**Source-Free Domain Adaptation for Question Answering with Masked Self-training**|M. Yin et.al|[paper](https://arxiv.org/abs/2212.09563)|-|-|
|**2024-3-17**|**Uncertainty-Aware Pseudo-Label Filtering for Source-Free Unsupervised Domain Adaptation**|Xi Chen et.al|[paper](https://arxiv.org/abs/2403.11256)|[code](https://github.com/chenxi52/UPA.)|<details><summary>detail</summary>Neurocomputing 2024</details>|
|**2024-3-16**|**SF(DA)$^2$: Source-free Domain Adaptation Through the Lens of Data Augmentation**|Uiwon Hwang et.al|[paper](https://arxiv.org/abs/2403.10834)|[code](https://github.com/shinyflight/SFDA2)|<details><summary>detail</summary>ICLR 2024</details>|
|**2024-3-13**|**De-Confusing Pseudo-Labels in Source-Free Domain Adaptation**|Idit Diamant et.al|[paper](https://arxiv.org/abs/2401.01650)|-|-|
|**2024-3-13**|**Source-Free Domain Adaptation with Frozen Multimodal Foundation Model**|Song Tang et.al|[paper](https://arxiv.org/abs/2311.16510)|-|<details><summary>detail</summary>CVPR 2024</details>|
|**2024-3-12**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|[code](https://github.com/tntek/source-free-domain-adaptation.)|-|
|**2024-3-11**|**COCA: Classifier-Oriented Calibration via Textual Prototype for Source-Free Universal Domain Adaptation**|Xinghong Liu et.al|[paper](https://arxiv.org/abs/2308.10450)|-|-|
|**2024-3-8**|**Model-Free Source Seeking by a Novel Single-Integrator with Attenuating Oscillations and Better Convergence Rate: Robotic Experiments**|Shivam Bajpai et.al|[paper](https://arxiv.org/abs/2311.04330)|-|-|
|**2024-3-8**|**Agile Multi-Source-Free Domain Adaptation**|Xinyao Li et.al|[paper](https://arxiv.org/abs/2403.05062)|[code](https://github.com/TL-UESTC/Bi-ATEN.)|<details><summary>detail</summary>AAAI2024</details>|
|**2024-3-6**|**MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection**|Boyang Peng et.al|[paper](https://arxiv.org/abs/2403.04149)|-|<details><summary>detail</summary>CVPR 2024</details>|

## object detection

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2024-3-22**|**Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection**|Hongzhi Gao et.al|[paper](https://arxiv.org/abs/2403.15317)|-|<details><summary>detail</summary>Accepted by AAAI2024</details>|
|**2024-3-22**|**IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection**|Junbo Yin et.al|[paper](https://arxiv.org/abs/2403.15241)|[code](https://github.com/yinjunbo/IS-Fusion.)|<details><summary>detail</summary>CVPR 2024</details>|
|**2024-3-22**|**CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object Detection under Unknown Degradations**|Yuwei Zhang et.al|[paper](https://arxiv.org/abs/2403.11220)|-|-|
|**2024-3-22**|**Gradient-based Sampling for Class Imbalanced Semi-supervised Object Detection**|Jiaming Li et.al|[paper](https://arxiv.org/abs/2403.15127)|[code](https://github.com/nightkeepers/CI-SSOD.)|<details><summary>detail</summary>Accepted by ICCV2023</details>|
|**2024-3-21**|**Rethinking Boundary Discontinuity Problem for Oriented Object Detection**|Hang Xu et.al|[paper](https://arxiv.org/abs/2305.10061)|[code](https://github.com/hangxu-cv/cvpr24acm.)|<details><summary>detail</summary>cvpr 2024</details>|
|**2024-3-21**|**Online Open-set Semi-supervised Object Detection with Dual Competing Head**|Zerun Wang et.al|[paper](https://arxiv.org/abs/2305.13802)|-|-|
|**2024-3-21**|**T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy**|Qing Jiang et.al|[paper](https://arxiv.org/abs/2403.14610)|[code](https://github.com/IDEA-Research/T-Rex)|<details><summary>detail</summary>Technical Report</details>|
|**2024-3-21**|**Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision**|Yi Yu et.al|[paper](https://arxiv.org/abs/2311.14758)|[code](https://github.com/yuyi1005/point2rbox-mmrotate)|-|
|**2024-3-21**|**R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement**|Michele Antonazzi et.al|[paper](https://arxiv.org/abs/2403.11567)|-|-|
|**2024-3-21**|**3D Object Detection from Point Cloud via Voting Step Diffusion**|Haoran Hou et.al|[paper](https://arxiv.org/abs/2403.14133)|[code](https://github.com/HHrEtvP/DiffVote.)|-|
|**2024-3-20**|**Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments**|Djamahl Etchegaray et.al|[paper](https://arxiv.org/abs/2403.13556)|-|-|
|**2024-3-20**|**Few-shot Oriented Object Detection with Memorable Contrastive Learning in Remote Sensing Images**|Jiawei Zhou et.al|[paper](https://arxiv.org/abs/2403.13375)|-|-|
|**2024-3-19**|**D-YOLO a robust framework for object detection in adverse weather conditions**|Zihan Chu et.al|[paper](https://arxiv.org/abs/2403.09233)|-|<details><summary>detail</summary>Object detection in adverse weather conditions</details>|
|**2024-3-19**|**BugNIST - a Large Volumetric Dataset for Object Detection under Domain Shift**|Patrick Møller Jensen et.al|[paper](https://arxiv.org/abs/2304.01838)|-|-|
|**2024-3-19**|**Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object Detector**|Yuqian Fu et.al|[paper](https://arxiv.org/abs/2402.03094)|-|-|

## domain adaptation

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2024-3-22**|**Improve Cross-domain Mixed Sampling with Guidance Training for Adaptive Segmentation**|Wenlve Zhou et.al|[paper](https://arxiv.org/abs/2403.14995)|[code](https://github.com/Wenlve-Zhou/Guidance-Training.)|-|
|**2024-3-21**|**CODA: A COst-efficient Test-time Domain Adaptation Mechanism for HAR**|Minghui Qiu et.al|[paper](https://arxiv.org/abs/2403.14922)|-|-|
|**2024-3-21**|**DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning**|Jonathan Lebensold et.al|[paper](https://arxiv.org/abs/2403.14421)|-|-|
|**2024-3-21**|**GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning**|Sanqing Qu et.al|[paper](https://arxiv.org/abs/2403.14410)|-|<details><summary>detail</summary>This is a substantial extension of the CVPR 2023 paper "Upcycling Models under Domain and Category Shift"</details>|
|**2024-3-21**|**Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training**|Arun Reddy et.al|[paper](https://arxiv.org/abs/2312.02914)|-|<details><summary>detail</summary>CVPR 2024</details>|
|**2024-3-21**|**R2SNet: Scalable Domain Adaptation for Object Detection in Cloud-Based Robots Ecosystems via Proposal Refinement**|Michele Antonazzi et.al|[paper](https://arxiv.org/abs/2403.11567)|-|-|
|**2024-3-21**|**A Fourier Transform Framework for Domain Adaptation**|Le Luo et.al|[paper](https://arxiv.org/abs/2403.07798)|-|<details><summary>detail</summary>The paper contains significant errors and the experimental methodology is not rigorous</details>|
|**2024-3-20**|**Improving $Λ$ Signal Extraction with Domain Adaptation via Normalizing Flows**|Rowan Kelleher et.al|[paper](https://arxiv.org/abs/2403.14076)|-|<details><summary>detail</summary>Proceedings for the 25th International Spin Physics Symposium (SPIN 2023)</details>|
|**2024-3-20**|**When Cars meet Drones: Hyperbolic Federated Learning for Source-Free Domain Adaptation in Adverse Weather**|Giulia Rizzoli et.al|[paper](https://arxiv.org/abs/2403.13762)|-|-|
|**2024-3-20**|**ZoDi: Zero-Shot Domain Adaptation with Diffusion-Based Image Transfer**|Hiroki Azuma et.al|[paper](https://arxiv.org/abs/2403.13652)|-|-|
|**2024-3-20**|**High-confidence pseudo-labels for domain adaptation in COVID-19 detection**|Robert Turnbull et.al|[paper](https://arxiv.org/abs/2403.13509)|-|-|
|**2024-3-19**|**Confusing Pair Correction Based on Category Prototype for Domain Adaptation under Noisy Environments**|Churan Zhi et.al|[paper](https://arxiv.org/abs/2403.12883)|[code](https://github.com/Hehxcf/CPC/.)|<details><summary>detail</summary>AAAI 2024</details>|
|**2024-3-19**|**Addressing Source Scale Bias via Image Warping for Domain Adaptation**|Shen Zheng et.al|[paper](https://arxiv.org/abs/2403.12712)|-|-|
|**2024-3-18**|**Align and Distill: Unifying and Improving Domain Adaptive Object Detection**|Justin Kay et.al|[paper](https://arxiv.org/abs/2403.12029)|[code](https://github.com/justinkay/aldi)|-|
|**2024-3-18**|**Optimal Transport for Domain Adaptation through Gaussian Mixture Models**|Eduardo Fernandes Montesuma et.al|[paper](https://arxiv.org/abs/2403.13847)|-|-|

## domain generalization

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2024-3-21**|**Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen Domains by Intrinsic Learning from Redundant LLM Semantics**|Jiaqi Yue et.al|[paper](https://arxiv.org/abs/2403.14362)|-|<details><summary>detail</summary>This work is submitted to IEEE TNNLS and is subject to IEEE copyright</details>|
|**2024-3-21**|**DomainLab: A modular Python package for domain generalization in deep learning**|Xudong Sun et.al|[paper](https://arxiv.org/abs/2403.14356)|[code](https://github.com/marrlab/DomainLab.)|-|
|**2024-3-20**|**V2X-DGW: Domain Generalization for Multi-agent Perception under Adverse Weather Conditions**|Baolu Li et.al|[paper](https://arxiv.org/abs/2403.11371)|-|-|
|**2024-3-19**|**A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation**|Qucheng Peng et.al|[paper](https://arxiv.org/abs/2403.11310)|[code](https://github.com/davidpengucf/DAF-DG)|<details><summary>detail</summary>Accepted by CVPR 2024</details>|
|**2024-3-18**|**Towards Generalizing to Unseen Domains with Few Labels**|Chamuditha Jayanga Galappaththige et.al|[paper](https://arxiv.org/abs/2403.11674)|-|<details><summary>detail</summary>CVPR 2024</details>|
|**2024-3-18**|**SETA: Semantic-Aware Token Augmentation for Domain Generalization**|Jintao Guo et.al|[paper](https://arxiv.org/abs/2403.11792)|[code](https://github.com/lingeringlight/SETA.)|-|
|**2024-3-18**|**Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$**|Simon Ståhlberg et.al|[paper](https://arxiv.org/abs/2403.11734)|-|<details><summary>detail</summary>Submitted to IJCAI 2024</details>|
|**2024-3-18**|**Depth- and Semantics-aware Multi-modal Domain Translation: Generating 3D Panoramic Color Images from LiDAR Point Clouds**|Tiago Cortinhal et.al|[paper](https://arxiv.org/abs/2302.07661)|-|-|
|**2024-3-17**|**Understanding Domain Generalization: A Noise Robustness Perspective**|Rui Qiao et.al|[paper](https://arxiv.org/abs/2401.14846)|[code](https://github.com/qiaoruiyt/NoiseRobustDG)|<details><summary>detail</summary>the 12th International Conference on Learning Representations (ICLR 2024)</details>|
|**2024-3-17**|**AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation**|Zihao Tang et.al|[paper](https://arxiv.org/abs/2403.07030)|[code](https://github.com/IshiKura-a/AuG-KD)|<details><summary>detail</summary>ICLR 2024</details>|
|**2024-3-17**|**Artifact Feature Purification for Cross-domain Detection of AI-generated Images**|Zheling Meng et.al|[paper](https://arxiv.org/abs/2403.11172)|-|<details><summary>detail</summary>This work is under consideration at Computer Vision and Image Understanding</details>|
|**2024-3-16**|**Improving Domain Generalization with Domain Relations**|Huaxiu Yao et.al|[paper](https://arxiv.org/abs/2302.02609)|-|<details><summary>detail</summary>Accepted by ICLR 2024 (Spotlight)</details>|
|**2024-3-16**|**TFS-ViT: Token-Level Feature Stylization for Domain Generalization**|Mehrdad Noori et.al|[paper](https://arxiv.org/abs/2303.15698)|[code](https://github.com/Mehrdad-Noori/TFS-ViT_Token-level_Feature_Stylization.)|-|
|**2024-3-15**|**Bidirectional Multi-Step Domain Generalization for Visible-Infrared Person Re-Identification**|Mahdi Alehdaghi et.al|[paper](https://arxiv.org/abs/2403.10782)|-|-|
|**2024-3-15**|**Seeking Flat Minima with Mean Teacher on Semi- and Weakly-Supervised Domain Generalization for Object Detection**|Ryosuke Furuta et.al|[paper](https://arxiv.org/abs/2310.19351)|-|-|

## vision language

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2024-3-22**|**Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images**|Kuofeng Gao et.al|[paper](https://arxiv.org/abs/2401.11170)|[code](https://github.com/KuofengGao/Verbose_Images.)|<details><summary>detail</summary>Accepted by ICLR 2024</details>|
|**2024-3-22**|**Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning**|Yunhao Gou et.al|[paper](https://arxiv.org/abs/2312.12379)|[code](https://gyhdog99.github.io/projects/mocle/)|<details><summary>detail</summary>Project website: https://gyhdog99</details>|
|**2024-3-22**|**Continual Vision-and-Language Navigation**|Seongjun Jeong et.al|[paper](https://arxiv.org/abs/2403.15049)|-|-|
|**2024-3-22**|**Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization**|Yang Jin et.al|[paper](https://arxiv.org/abs/2309.04669)|[code](https://github.com/jy0205/LaVIT.)|<details><summary>detail</summary>ICLR 2024</details>|
|**2024-3-21**|**Few-Shot Adversarial Prompt Learning on Vision-Language Models**|Yiwei Zhou et.al|[paper](https://arxiv.org/abs/2403.14774)|-|-|
|**2024-3-21**|**Can 3D Vision-Language Models Truly Understand Natural Language?**|Weipeng Deng et.al|[paper](https://arxiv.org/abs/2403.14760)|[code](https://github.com/VincentDENGP/3D-LR)|<details><summary>detail</summary>https://github</details>|
|**2024-3-21**|**Unveiling Typographic Deceptions: Insights of the Typographic Vulnerability in Large Vision-Language Model**|Hao Cheng et.al|[paper](https://arxiv.org/abs/2402.19150)|-|-|
|**2024-3-21**|**Chain-of-Spot: Interactive Reasoning Improves Large Vision-Language Models**|Zuyan Liu et.al|[paper](https://arxiv.org/abs/2403.12966)|[code](https://github.com/dongyh20/Chain-of-Spot)|<details><summary>detail</summary>Project Page: https://sites</details>|
|**2024-3-21**|**Active Prompt Learning in Vision Language Models**|Jihwan Bang et.al|[paper](https://arxiv.org/abs/2311.11178)|[code](https://github.com/kaist-dmlab/pcb)|<details><summary>detail</summary>accepted at CVPR 2024</details>|
|**2024-3-21**|**Volumetric Environment Representation for Vision-Language Navigation**|Rui Liu et.al|[paper](https://arxiv.org/abs/2403.14158)|-|<details><summary>detail</summary>CVPR 2024</details>|
|**2024-3-21**|**Vision-Language Models can Identify Distracted Driver Behavior from Naturalistic Videos**|Md Zahid Hasan et.al|[paper](https://arxiv.org/abs/2306.10159)|-|-|
|**2024-3-21**|**C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion**|Hee Suk Yoon et.al|[paper](https://arxiv.org/abs/2403.14119)|-|<details><summary>detail</summary>ICLR 2024</details>|
|**2024-3-20**|**Bridge the Modality and Capacity Gaps in Vision-Language Model Selection**|Chao Yi et.al|[paper](https://arxiv.org/abs/2403.13797)|-|-|
|**2024-3-20**|**Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models**|Nicholas Bai et.al|[paper](https://arxiv.org/abs/2403.13771)|-|-|
|**2024-3-20**|**Enhancing Gait Video Analysis in Neurodegenerative Diseases by Knowledge Augmentation in Vision Language Model**|Diwei Wang et.al|[paper](https://arxiv.org/abs/2403.13756)|-|-|

