## CV Papers Daily
- [source-free](#source-free)
- [object detection](#object-detection)
- [domain adaptation](#domain-adaptation)
- [domain generalization](#domain-generalization)
- [vision language](#vision-language)


## Updated on 2025.10.07

## source-free

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-10-6**|**A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation**|Jiaping Yu et.al|[paper](https://arxiv.org/abs/2509.22229)|-|-|
|**2025-10-6**|**Leveraging Confident Image Regions for Source-Free Domain-Adaptive Object Detection**|Mohamed Lamine Mekhalfi et.al|[paper](https://arxiv.org/abs/2501.10081)|-|-|
|**2025-10-3**|**Temporal Source Recovery for Time-Series Source-Free Unsupervised Domain Adaptation**|Yucheng Wang et.al|[paper](https://arxiv.org/abs/2409.19635)|-|-|
|**2025-10-2**|**OT Score: An OT based Confidence Score for Source Free Unsupervised Domain Adaptation**|Yiming Zhang et.al|[paper](https://arxiv.org/abs/2505.11669)|-|-|
|**2025-10-2**|**Source-Free Cross-Domain Continual Learning**|Muhammad Tanzil Furqon et.al|[paper](https://arxiv.org/abs/2510.01649)|-|-|
|**2025-10-1**|**Consistent Assistant Domains Transformer for Source-free Domain Adaptation**|Renrong Shao et.al|[paper](https://arxiv.org/abs/2510.01559)|[code](https://github.com/RoryShao/CADTrans.git.)|-|
|**2025-9-30**|**Source-Free Domain Adaptive Object Detection with Semantics Compensation**|Song Tang et.al|[paper](https://arxiv.org/abs/2410.05557)|-|-|
|**2025-9-29**|**DAM: Dual Active Learning with Multimodal Foundation Model for Source-Free Domain Adaptation**|Xi Chen et.al|[paper](https://arxiv.org/abs/2509.24896)|-|-|
|**2025-9-29**|**Lost in Translation? Vocabulary Alignment for Source-Free Adaptation in Open-Vocabulary Semantic Segmentation**|Silvio Mazzucco et.al|[paper](https://arxiv.org/abs/2509.15225)|[code](https://thegoodailab.org/blog/vocalign)|<details><summary>detail</summary>BMVC 2025 - Project Page: https://thegoodailab</details>|
|**2025-9-22**|**Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment**|Wenjie Liu et.al|[paper](https://arxiv.org/abs/2509.18502)|-|-|
|**2025-9-22**|**Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation**|Amirhossein Dadashzadeh et.al|[paper](https://arxiv.org/abs/2504.11669)|[code](https://github.com/Plrbear/Co-Star)|-|
|**2025-9-21**|**Prototype-Based Pseudo-Label Denoising for Source-Free Domain Adaptation in Remote Sensing Semantic Segmentation**|Bin Wang et.al|[paper](https://arxiv.org/abs/2509.16942)|-|-|
|**2025-9-18**|**Model-Free and Real-Time Unicycle-Based Source Seeking with Differential Wheeled Robotic Experiments**|Ahmed A. Elgohary et.al|[paper](https://arxiv.org/abs/2501.02184)|-|-|
|**2025-9-13**|**Step-wise Distribution Alignment Guided Style Prompt Tuning for Source-free Cross-domain Few-shot Learning**|Huali Xu et.al|[paper](https://arxiv.org/abs/2411.10070)|[code](https://github.com/xuhuali-mxj/StepSPT.)|<details><summary>detail</summary>IEEE TPAMI</details>|
|**2025-9-12**|**Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment**|Rini Smita Thakur et.al|[paper](https://arxiv.org/abs/2509.10134)|[code](https://visdomlab.github.io/GCL/.)|<details><summary>detail</summary>Accepted in BMVC 2025</details>|

## object detection

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-10-6**|**Leveraging Confident Image Regions for Source-Free Domain-Adaptive Object Detection**|Mohamed Lamine Mekhalfi et.al|[paper](https://arxiv.org/abs/2501.10081)|-|-|
|**2025-10-6**|**SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection**|Baber Jan et.al|[paper](https://arxiv.org/abs/2510.04472)|[code](https://github.com/Baber-Jan/SPEGNet)|-|
|**2025-10-4**|**From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance**|Ardalan Aryashad et.al|[paper](https://arxiv.org/abs/2510.03906)|-|-|
|**2025-10-4**|**Cross-View Open-Vocabulary Object Detection in Aerial Imagery**|Jyoti Kini et.al|[paper](https://arxiv.org/abs/2510.03858)|-|-|
|**2025-10-4**|**SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection**|Zhengyi Liu et.al|[paper](https://arxiv.org/abs/2510.03689)|-|<details><summary>detail</summary>Accepted by TMM</details>|
|**2025-10-3**|**Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models**|Wei-Lung Mao et.al|[paper](https://arxiv.org/abs/2510.01914)|-|-|
|**2025-10-3**|**Neural Posterior Estimation with Autoregressive Tiling for Detecting Objects in Astronomical Images**|Jeffrey Regier et.al|[paper](https://arxiv.org/abs/2510.03074)|-|<details><summary>detail</summary>MSC Class:62P35ACM Class:G</details>|
|**2025-10-3**|**Visual Language Model as a Judge for Object Detection in Industrial Diagrams**|Sanjukta Ghosh et.al|[paper](https://arxiv.org/abs/2510.03376)|-|<details><summary>detail</summary>Pre-review version submitted to IEEE ICASSP 2026</details>|
|**2025-10-3**|**Align Your Query: Representation Alignment for Multimodality Medical Object Detection**|Ara Seo et.al|[paper](https://arxiv.org/abs/2510.02789)|[code](https://araseo.github.io/alignyourquery/.)|<details><summary>detail</summary>Project page: https://araseo</details>|
|**2025-10-3**|**Contextualized Representation Learning for Effective Human-Object Interaction Detection**|Zhehao Li et.al|[paper](https://arxiv.org/abs/2509.12784)|[code](https://github.com/lzzhhh1019/CRL.)|-|
|**2025-10-3**|**Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2510.02750)|-|<details><summary>detail</summary>Under Review</details>|
|**2025-10-2**|**Towards Size-invariant Salient Object Detection: A Generic Evaluation and Optimization Approach**|Shilong Bao et.al|[paper](https://arxiv.org/abs/2509.15573)|[code](https://github.com/Ferry-Li/SI-SOD.)|-|
|**2025-10-2**|**Automated Model Evaluation for Object Detection via Prediction Consistency and Reliability**|Seungju Yoo et.al|[paper](https://arxiv.org/abs/2508.12082)|[code](https://github.com/YonseiML/autoeval-det.)|<details><summary>detail</summary>ICCV 2025 Oral</details>|
|**2025-10-1**|**Adaptive Event Stream Slicing for Open-Vocabulary Event-Based Object Detection via Vision-Language Knowledge Distillation**|Jinchang Zhang et.al|[paper](https://arxiv.org/abs/2510.00681)|-|-|
|**2025-10-1**|**PAN: Pillars-Attention-Based Network for 3D Object Detection**|Ruan Bispo et.al|[paper](https://arxiv.org/abs/2509.15935)|-|-|

## domain adaptation

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-10-6**|**A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation**|Jiaping Yu et.al|[paper](https://arxiv.org/abs/2509.22229)|-|-|
|**2025-10-6**|**Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry**|Anastasia Zhukova et.al|[paper](https://arxiv.org/abs/2510.04631)|-|<details><summary>detail</summary>accepted to EMNLP 2025 (industry track)</details>|
|**2025-10-6**|**Leveraging Confident Image Regions for Source-Free Domain-Adaptive Object Detection**|Mohamed Lamine Mekhalfi et.al|[paper](https://arxiv.org/abs/2501.10081)|-|-|
|**2025-10-5**|**VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming**|Duy Nguyen et.al|[paper](https://arxiv.org/abs/2510.01660)|-|-|
|**2025-10-5**|**Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains**|Akshay Mittal et.al|[paper](https://arxiv.org/abs/2510.04375)|-|-|
|**2025-10-5**|**The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation**|Jincan Lou et.al|[paper](https://arxiv.org/abs/2510.04243)|-|-|
|**2025-10-3**|**Temporal Source Recovery for Time-Series Source-Free Unsupervised Domain Adaptation**|Yucheng Wang et.al|[paper](https://arxiv.org/abs/2409.19635)|-|-|
|**2025-10-3**|**Deep Domain Adaptation for Turbofan Engine Remaining Useful Life Prediction: Methodologies, Evaluation and Future Trends**|Yucheng Wang et.al|[paper](https://arxiv.org/abs/2510.03604)|-|-|
|**2025-10-3**|**Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Governance**|Zixuan Wang et.al|[paper](https://arxiv.org/abs/2509.21486)|-|-|
|**2025-10-2**|**Unified Domain Adaptive Semantic Segmentation**|Zhe Zhang et.al|[paper](https://arxiv.org/abs/2311.13254)|[code](https://github.com/ZHE-SAPI/UDASS.)|-|
|**2025-10-2**|**OT Score: An OT based Confidence Score for Source Free Unsupervised Domain Adaptation**|Yiming Zhang et.al|[paper](https://arxiv.org/abs/2505.11669)|-|-|
|**2025-10-2**|**Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive**|Tyler Farghly et.al|[paper](https://arxiv.org/abs/2510.02305)|-|-|
|**2025-10-1**|**Consistent Assistant Domains Transformer for Source-free Domain Adaptation**|Renrong Shao et.al|[paper](https://arxiv.org/abs/2510.01559)|[code](https://github.com/RoryShao/CADTrans.git.)|-|
|**2025-10-1**|**Signal Classification Recovery Across Domains Using Unsupervised Domain Adaptation**|Mohammad Ali et.al|[paper](https://arxiv.org/abs/2510.00589)|-|-|
|**2025-10-1**|**Black-Box Time-Series Domain Adaptation via Cross-Prompt Foundation Models**|M. T. Furqon et.al|[paper](https://arxiv.org/abs/2510.00487)|-|-|

## domain generalization

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-10-5**|**Domain Generalization: A Tale of Two ERMs**|Yilun Zhu et.al|[paper](https://arxiv.org/abs/2510.04441)|-|-|
|**2025-10-5**|**The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation**|Jincan Lou et.al|[paper](https://arxiv.org/abs/2510.04243)|-|-|
|**2025-10-4**|**GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Mechanism**|Hailong Yang et.al|[paper](https://arxiv.org/abs/2509.10018)|-|-|
|**2025-10-4**|**Complex Domain Approach for Reversible Data Hiding and Homomorphic Encryption: General Framework and Application to Dispersed Data**|David Megias et.al|[paper](https://arxiv.org/abs/2510.03770)|-|-|
|**2025-10-3**|**Domain Generalization for Semantic Segmentation: A Survey**|Manuel Schwonberg et.al|[paper](https://arxiv.org/abs/2510.03540)|-|<details><summary>detail</summary>CVPR2025W</details>|
|**2025-10-2**|**Automated Constraint Specification for Job Scheduling by Regulating Generative Model with Domain-Specific Representation**|Yu-Zhe Shi et.al|[paper](https://arxiv.org/abs/2510.02679)|-|<details><summary>detail</summary>Accepted for publication in IEEE Transactions on Automation Science and Engineering</details>|
|**2025-10-2**|**DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion across General and Biomedical Domains**|Yongkang Xiao et.al|[paper](https://arxiv.org/abs/2506.00708)|-|<details><summary>detail</summary>EMNLP 2025 Findings</details>|
|**2025-9-30**|**Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted Annealing**|Yang Tang et.al|[paper](https://arxiv.org/abs/2509.26242)|-|-|
|**2025-9-30**|**Scaling Up Temporal Domain Generalization via Temporal Experts Averaging**|Aoming Liu et.al|[paper](https://arxiv.org/abs/2509.26045)|-|<details><summary>detail</summary>Accepted by EMNLP 2025 main</details>|
|**2025-9-29**|**Think Less, Label Better: Multi-Stage Domain-Grounded Synthetic Data Generation for Fine-Tuning Large Language Models in Telecommunications**|Chenhua Shi et.al|[paper](https://arxiv.org/abs/2509.25736)|-|-|
|**2025-9-29**|**SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL Translation**|Hasan Alp Caferoğlu et.al|[paper](https://arxiv.org/abs/2509.25672)|-|-|
|**2025-9-27**|**Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation**|Chaojun Nie et.al|[paper](https://arxiv.org/abs/2509.20162)|[code](https://github.com/ChaojunNie/RLAG.)|<details><summary>detail</summary>Corrected author name spelling</details>|
|**2025-9-27**|**SynDoc: A Hybrid Discriminative-Generative Framework for Enhancing Synthetic Domain-Adaptive Document Key Information Extraction**|Yihao Ding et.al|[paper](https://arxiv.org/abs/2509.23273)|-|<details><summary>detail</summary>Work in progress</details>|
|**2025-9-26**|**Bézier Meets Diffusion: Robust Generation Across Domains for Medical Image Segmentation**|Chen Li et.al|[paper](https://arxiv.org/abs/2509.22476)|-|-|
|**2025-9-26**|**A Survey on LLM-based Code Generation for Low-Resource and Domain-Specific Programming Languages**|Sathvik Joel et.al|[paper](https://arxiv.org/abs/2410.03981)|-|-|

## vision language

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-10-6**|**Efficient Navigation in Unknown Indoor Environments with Vision-Language Models**|D. Schwartz et.al|[paper](https://arxiv.org/abs/2510.04991)|-|-|
|**2025-10-6**|**MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly**|Zhaowei Wang et.al|[paper](https://arxiv.org/abs/2505.10610)|-|<details><summary>detail</summary>Accepted as a spotlight at NeurIPS 2025</details>|
|**2025-10-6**|**HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks**|Zheng Xiong et.al|[paper](https://arxiv.org/abs/2510.04898)|[code](https://github.com/MasterXiong/HyperVLA)|-|
|**2025-10-6**|**CBVLM: Training-free Explainable Concept-based Large Vision Language Models for Medical Image Classification**|Cristiano Patrício et.al|[paper](https://arxiv.org/abs/2501.12266)|[code](https://cristianopatricio.github.io/CBVLM/.)|<details><summary>detail</summary>Accepted for publication in Computers in Biology and Medicine</details>|
|**2025-10-6**|**SIA: Enhancing Safety via Intent Awareness for Vision-Language Models**|Youngjin Na et.al|[paper](https://arxiv.org/abs/2507.16856)|-|<details><summary>detail</summary>Safe and Trustworthy Multimodal AI Systems(SafeMM-AI) Workshop at ICCV2025</details>|
|**2025-10-6**|**More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models**|Xurui Song et.al|[paper](https://arxiv.org/abs/2510.04532)|-|<details><summary>detail</summary>The dataset will be released publicly once the paper is accepted for publication</details>|
|**2025-10-6**|**Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models**|Young Kyun Jang et.al|[paper](https://arxiv.org/abs/2405.14715)|-|-|
|**2025-10-6**|**MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models**|Soo Yong Kim et.al|[paper](https://arxiv.org/abs/2510.04477)|-|-|
|**2025-10-5**|**Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting**|Xuyang Guo et.al|[paper](https://arxiv.org/abs/2510.04401)|-|-|
|**2025-10-5**|**Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark**|Rashid Mushkani et.al|[paper](https://arxiv.org/abs/2509.14574)|-|-|
|**2025-10-5**|**ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context**|Huiwon Jang et.al|[paper](https://arxiv.org/abs/2510.04246)|[code](https://huiwon-jang.github.io/contextvla)|<details><summary>detail</summary>Project page: https://huiwon-jang</details>|
|**2025-10-5**|**From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation**|Jingkun Chen et.al|[paper](https://arxiv.org/abs/2504.11368)|[code](https://github.com/jingkunchen/FGI.)|-|
|**2025-10-5**|**From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models**|Chenyue Zhou et.al|[paper](https://arxiv.org/abs/2509.25373)|-|-|
|**2025-10-5**|**Automating construction safety inspections using a multi-modal vision-language RAG framework**|Chenxin Wang et.al|[paper](https://arxiv.org/abs/2510.04145)|-|-|
|**2025-10-4**|**AgriGPT-VL: Agricultural Vision-Language Understanding Suite**|Bo Yang et.al|[paper](https://arxiv.org/abs/2510.04002)|-|-|

