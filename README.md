## CV Papers Daily
- [source-free](#source-free)
- [object detection](#object-detection)
- [domain adaptation](#domain-adaptation)
- [domain generalization](#domain-generalization)
- [vision language](#vision-language)


## Updated on 2025.07.16

## source-free

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|
|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|
|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|
|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|
|**2025-6-26**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|
|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|
|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|
|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|
|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|
|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|
|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|
|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|
|**2025-5-30**|**Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation**|Prasanna Reddy Pulakurthi et.al|[paper](https://arxiv.org/abs/2505.24216)|[code](https://github.com/PrasannaPulakurthi/SPM)|-|
|**2025-5-27**|**Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation**|Peihua Deng et.al|[paper](https://arxiv.org/abs/2411.16064)|[code](https://github.com/dengpeihua/GROTO.)|<details><summary>detail</summary>Accepted by CVPR 2025</details>|
|**2025-5-26**|**Training-Free Multi-Step Audio Source Separation**|Yongyi Zang et.al|[paper](https://arxiv.org/abs/2505.19534)|-|-|

## object detection

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-7-15**|**Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery**|Nicolas Drapier et.al|[paper](https://arxiv.org/abs/2507.11040)|-|-|
|**2025-7-15**|**Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection**|Quan Bi Pay et.al|[paper](https://arxiv.org/abs/2507.10977)|[code](https://github.com/henry-pay/RayEncoder].)|<details><summary>detail</summary>International Joint Conference on Neural Networks (IJCNN 2025)</details>|
|**2025-7-14**|**LLM-Guided Agentic Object Detection for Open-World Understanding**|Furkan Mumcu et.al|[paper](https://arxiv.org/abs/2507.10844)|-|-|
|**2025-7-14**|**Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection**|Huiyi Wang et.al|[paper](https://arxiv.org/abs/2507.10814)|-|-|
|**2025-7-14**|**Fine-Grained Zero-Shot Object Detection**|Hongxu Ma et.al|[paper](https://arxiv.org/abs/2507.10358)|-|<details><summary>detail</summary>Accepted by ACM MM'25</details>|
|**2025-7-14**|**Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?**|Miao Zhang et.al|[paper](https://arxiv.org/abs/2503.02687)|-|-|
|**2025-7-14**|**Measuring the Impact of Rotation Equivariance on Aerial Object Detection**|Xiuyu Wu et.al|[paper](https://arxiv.org/abs/2507.09896)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|
|**2025-7-12**|**Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline**|Shiyi Mu et.al|[paper](https://arxiv.org/abs/2507.09214)|[code](https://github.com/xxxx/xxx).)|<details><summary>detail</summary>under review</details>|
|**2025-7-12**|**HA-RDet: Hybrid Anchor Rotation Detector for Oriented Object Detection**|Phuc D. A. Nguyen et.al|[paper](https://arxiv.org/abs/2412.14379)|-|<details><summary>detail</summary>Bachelor thesis</details>|
|**2025-7-11**|**RoHOI: Robustness Benchmark for Human-Object Interaction Detection**|Di Wen et.al|[paper](https://arxiv.org/abs/2507.09111)|[code](https://github.com/Kratos-Wen/RoHOI.)|<details><summary>detail</summary>Benchmarks</details>|
|**2025-7-11**|**Quantifying Context Bias in Domain Adaptation for Object Detection**|Hojun Son et.al|[paper](https://arxiv.org/abs/2409.14679)|-|<details><summary>detail</summary>Under review</details>|
|**2025-7-11**|**Smelly, dense, and spreaded: The Object Detection for Olfactory References (ODOR) dataset**|Mathias Zinnen et.al|[paper](https://arxiv.org/abs/2507.08384)|-|<details><summary>detail</summary>MSC Class:68T45 68T45ACM Class:I</details>|
|**2025-7-11**|**EVT: Efficient View Transformation for Multi-Modal 3D Object Detection**|Yongjin Lee et.al|[paper](https://arxiv.org/abs/2411.10715)|-|<details><summary>detail</summary>ICCV 2025</details>|
|**2025-7-10**|**Uncertainty-Aware Gradient Stabilization for Small Object Detection**|Huixin Sun et.al|[paper](https://arxiv.org/abs/2303.01803)|-|-|
|**2025-7-10**|**RT-OVAD: Real-Time Open-Vocabulary Aerial Object Detection via Image-Text Collaboration**|Guoting Wei et.al|[paper](https://arxiv.org/abs/2408.12246)|[code](https://github.com/GT-Wei/RT-OVAD.)|-|

## domain adaptation

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-7-15**|**Few-Shot Radar Signal Recognition through Self-Supervised Learning and Radio Frequency Domain Adaptation**|Zi Huang et.al|[paper](https://arxiv.org/abs/2501.03461)|-|-|
|**2025-7-15**|**Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving**|Chinmay Vilas Samak et.al|[paper](https://arxiv.org/abs/2507.00236)|-|-|
|**2025-7-14**|**SA-GDA: Spectral Augmentation for Graph Domain Adaptation**|Jinhui Pang et.al|[paper](https://arxiv.org/abs/2408.09189)|-|-|
|**2025-7-14**|**Domain-Adaptive Small Language Models for Structured Tax Code Prediction**|Souvik Nath et.al|[paper](https://arxiv.org/abs/2507.10880)|-|-|
|**2025-7-14**|**DQLoRA: A Lightweight Domain-Aware Denoising ASR via Adapter-guided Distillation**|Yiru Yang et.al|[paper](https://arxiv.org/abs/2507.10313)|-|-|
|**2025-7-14**|**Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation**|Manuel RÃ¶der et.al|[paper](https://arxiv.org/abs/2507.10160)|[code](http://dx.doi.org/10.5220/0012351900003654)|<details><summary>detail</summary>Extension of http://dx</details>|
|**2025-7-13**|**NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance**|Hanwool Lee et.al|[paper](https://arxiv.org/abs/2507.09601)|-|<details><summary>detail</summary>Under Review</details>|
|**2025-7-12**|**Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data**|Timothy Chase Jr et.al|[paper](https://arxiv.org/abs/2507.09420)|[code](https://tjchase34.github.io/assets/pdfs/rss_poster.pdf)|<details><summary>detail</summary>Presented at the RSS Space Robotics Workshop 2025</details>|
|**2025-7-12**|**Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting**|Zheang Huai et.al|[paper](https://arxiv.org/abs/2505.08527)|[code](https://github.com/xmed-lab/DFG.)|<details><summary>detail</summary>Accepted in TMI 2025</details>|
|**2025-7-12**|**CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification**|Nan Yin et.al|[paper](https://arxiv.org/abs/2306.04979)|-|-|
|**2025-7-11**|**Quantifying Context Bias in Domain Adaptation for Object Detection**|Hojun Son et.al|[paper](https://arxiv.org/abs/2409.14679)|-|<details><summary>detail</summary>Under review</details>|
|**2025-7-10**|**Adaptive Graph Integration for Cross-Domain Recommendation via Heterogeneous Graph Coordinators**|Hengyu Zhang et.al|[paper](https://arxiv.org/abs/2410.11719)|[code](https://github.com/zhy99426/HAGO.)|<details><summary>detail</summary>Accept by SIGIR 2025</details>|
|**2025-7-10**|**Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation**|Junyu Luo et.al|[paper](https://arxiv.org/abs/2507.07621)|-|<details><summary>detail</summary>ICML 2025</details>|
|**2025-7-10**|**Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction**|Rui An et.al|[paper](https://arxiv.org/abs/2506.18939)|-|-|
|**2025-7-10**|**ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining**|Seonwu Kim et.al|[paper](https://arxiv.org/abs/2507.06795)|-|<details><summary>detail</summary>under review</details>|

## domain generalization

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-7-15**|**Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2504.12753)|[code](https://github.com/anonymouse-xzrptkvyqc/DepthForge.)|<details><summary>detail</summary>Accepted by ICCV 2025</details>|
|**2025-7-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Lihua Zhou et.al|[paper](https://arxiv.org/abs/2507.09961)|-|<details><summary>detail</summary>Under Review</details>|
|**2025-7-13**|**Fair Domain Generalization: An Information-Theoretic View**|Tangzheng Lian et.al|[paper](https://arxiv.org/abs/2507.05823)|-|-|
|**2025-7-11**|**RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze**|Yunsoo Kim et.al|[paper](https://arxiv.org/abs/2507.09097)|-|-|
|**2025-7-11**|**Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement**|Jia-Xuan Jiang et.al|[paper](https://arxiv.org/abs/2507.08340)|[code](https://github.com/HopkinsKwong/MCCSDG)|<details><summary>detail</summary>Accepted by ACMMM 25</details>|
|**2025-7-10**|**From Domain Documents to Requirements: Retrieval-Augmented Generation in the Space Industry**|Chetan Arora et.al|[paper](https://arxiv.org/abs/2507.07689)|-|-|
|**2025-7-10**|**Sparse Causal Discovery with Generative Intervention for Unsupervised Graph Domain Adaptation**|Junyu Luo et.al|[paper](https://arxiv.org/abs/2507.07621)|-|<details><summary>detail</summary>ICML 2025</details>|
|**2025-7-9**|**SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains**|Krithika Ramesh et.al|[paper](https://arxiv.org/abs/2507.07229)|-|-|
|**2025-7-9**|**Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis**|Srihari K B et.al|[paper](https://arxiv.org/abs/2507.06571)|-|-|
|**2025-7-7**|**AdaptaGen: Domain-Specific Image Generation through Hierarchical Semantic Optimization Framework**|Suoxiang Zhang et.al|[paper](https://arxiv.org/abs/2507.05621)|-|-|
|**2025-7-7**|**LAID: Lightweight AI-Generated Image Detection in Spatial and Spectral Domains**|Nicholas Chivaran et.al|[paper](https://arxiv.org/abs/2507.05162)|[code](https://github.com/nchivar/LAID.)|<details><summary>detail</summary>To appear in the proceedings of PST2025</details>|
|**2025-7-6**|**Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization**|Zuyu Zhang et.al|[paper](https://arxiv.org/abs/2507.04302)|-|<details><summary>detail</summary>ICCV 2025</details>|
|**2025-7-5**|**CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning**|Jiacheng Shi et.al|[paper](https://arxiv.org/abs/2507.04048)|-|<details><summary>detail</summary>Interspeech2025</details>|
|**2025-7-4**|**Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations**|Hai Huang et.al|[paper](https://arxiv.org/abs/2507.03304)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|
|**2025-7-3**|**Set Valued Predictions For Robust Domain Generalization**|Ron Tsibulsky et.al|[paper](https://arxiv.org/abs/2507.03146)|-|<details><summary>detail</summary>ICML 2025</details>|

## vision language

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-7-15**|**Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models**|Lucy Xiaoyang Shi et.al|[paper](https://arxiv.org/abs/2502.19417)|[code](https://www.pi.website/research/hirobot)|<details><summary>detail</summary>ICML 2025</details>|
|**2025-7-15**|**How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study**|Che Liu et.al|[paper](https://arxiv.org/abs/2507.11200)|-|<details><summary>detail</summary>Accepted by the International Conference on AI in Healthcare 2025</details>|
|**2025-7-15**|**Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**|Yiting Qu et.al|[paper](https://arxiv.org/abs/2507.11155)|-|<details><summary>detail</summary>To Appear in the 34th USENIX Security Symposium</details>|
|**2025-7-15**|**Assessing Color Vision Test in Large Vision-language Models**|Hongfei Ye et.al|[paper](https://arxiv.org/abs/2507.11153)|[code](https://anonymous.4open.science/r/color-vision-test-dataset-3BCD)|-|
|**2025-7-15**|**MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models**|Seif Ahmed et.al|[paper](https://arxiv.org/abs/2507.11114)|-|-|
|**2025-7-15**|**Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander**|Li Wang et.al|[paper](https://arxiv.org/abs/2507.11079)|-|-|
|**2025-7-14**|**View Invariant Learning for Vision-Language Navigation in Continuous Environments**|Josh Qixuan Sun et.al|[paper](https://arxiv.org/abs/2507.08831)|-|<details><summary>detail</summary>Under review</details>|
|**2025-7-14**|**TAB: Transformer Attention Bottlenecks enable User Intervention and Debugging in Vision-Language Models**|Pooyan Rahmanzadehgervi et.al|[paper](https://arxiv.org/abs/2412.18675)|-|-|
|**2025-7-14**|**Vision Language Action Models in Robotic Manipulation: A Systematic Review**|Muhayy Ud Din et.al|[paper](https://arxiv.org/abs/2507.10672)|-|<details><summary>detail</summary>submitted to annual review in control</details>|
|**2025-7-14**|**UniQA: Unified Vision-Language Pre-training for Image Quality and Aesthetic Assessment**|Hantao Zhou et.al|[paper](https://arxiv.org/abs/2406.01069)|[code](https://github.com/zht8506/UniQA.)|-|
|**2025-7-14**|**Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R**|Pablo Robin Guerrero et.al|[paper](https://arxiv.org/abs/2507.08505)|-|-|
|**2025-7-14**|**Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect**|Tom Kouwenhoven et.al|[paper](https://arxiv.org/abs/2507.10013)|-|-|
|**2025-7-13**|**VisOnlyQA: Large Vision Language Models Still Struggle with Visual Perception of Geometric Information**|Ryo Kamoi et.al|[paper](https://arxiv.org/abs/2412.00947)|[code](https://github.com/psunlpgroup/VisOnlyQA.)|<details><summary>detail</summary>COLM 2025</details>|
|**2025-7-13**|**VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding**|Younggun Kim et.al|[paper](https://arxiv.org/abs/2507.09815)|-|-|
|**2025-7-13**|**DriveMRP: Enhancing Vision-Language Models with Synthetic Motion Data for Motion Risk Prediction**|Zhiyi Hou et.al|[paper](https://arxiv.org/abs/2507.02948)|[code](https://github.com/hzy138/DriveMRP)|-|

