## CV Papers Daily
- [source-free](#source-free)
- [object detection](#object-detection)
- [domain adaptation](#domain-adaptation)
- [domain generalization](#domain-generalization)
- [vision language](#vision-language)


## Updated on 2025.07.08

## source-free

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-7-5**|**Source-Free Domain Adaptation for Speaker Verification in Data-Scarce Languages and Noisy Channels**|S Salo Elia et.al|[paper](https://ui.adsabs.harvard.edu/abs/2024arXiv240605863S/abstract)|[code](https://paperswithcode.com/paper/source-free-domain-adaptation-for-speaker)|-|
|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|
|**2025-7-4**|**Global self-sustaining and local inheritance for source-free unsupervised domain adaptation**|L Peng et.al|[paper](https://www.sciencedirect.com/science/article/pii/S0031320324004308)|-|<details><summary>detail</summary>Pattern Recognition, 2024 Elsevier</details>|
|**2025-7-3**|**Unveiling the Unknown: Unleashing the Power of Unknown to Known in Open-Set Source-Free Domain Adaptation**|F Wan et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Wan_Unveiling_the_Unknown_Unleashing_the_Power_of_Unknown_to_Known_CVPR_2024_paper.html)|[code](https://github.com/xdwfl/upuk)|<details><summary>detail</summary>Proceedings of the IEEE…, 2024 openaccess.thecvf.com</details>|
|**2025-7-3**|**Discriminative Pattern Calibration Mechanism for Source-Free Domain Adaptation**|H Xia et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Xia_Discriminative_Pattern_Calibration_Mechanism_for_Source-Free_Domain_Adaptation_CVPR_2024_paper.html)|[code](https://paperswithcode.com/paper/discriminative-pattern-calibration-mechanism)|<details><summary>detail</summary>…of the IEEE/CVF Conference on…, 2024 openaccess.thecvf.com</details>|
|**2025-7-3**|**Understanding and Improving Source-free Domain Adaptation from a Theoretical Perspective**|Y Mitsuzumi et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/html/Mitsuzumi_Understanding_and_Improving_Source-free_Domain_Adaptation_from_a_Theoretical_Perspective_CVPR_2024_paper.html)|[code](https://paperswithcode.com/paper/understanding-and-improving-source-free)|<details><summary>detail</summary>Proceedings of the IEEE…, 2024 openaccess.thecvf.com</details>|
|**2025-7-3**|**Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation–Supplementary Material–**|X Zheng et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zheng_Semantics_Distortion_and_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>openaccess.thecvf.com</details>|
|**2025-7-3**|**LEAD: Learning Decomposition for Source-free Universal Domain Adaptation—Supplementary Material**|S Qu et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Qu_LEAD_Learning_Decomposition_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>Integration openaccess.thecvf.com</details>|
|**2025-7-3**|**Stable Neighbor Denoising for Source-free Domain Adaptive Segmentation (Supplementary Material)**|D Zhao et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zhao_Stable_Neighbor_Denoising_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>openaccess.thecvf.com</details>|
|**2025-7-3**|**EventDance: Unsupervised Source-free Cross-modal Adaptation for Event-based Object Recognition–Supplementray Material–**|X Zheng et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Zheng_EventDance_Unsupervised_Source-free_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>openaccess.thecvf.com</details>|
|**2025-7-3**|**MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection—Supplementary Material**|B Peng et.al|[paper](https://openaccess.thecvf.com/content/CVPR2024/supplemental/Peng_MAP_MAsk-Pruning_for_CVPR_2024_supplemental.pdf)|-|<details><summary>detail</summary>openaccess.thecvf.com</details>|
|**2025-6-26**|**Recall and Refine: A Simple but Effective Source-free Open-set Domain Adaptation Framework**|Ismail Nejjar et.al|[paper](https://arxiv.org/abs/2411.12558)|-|<details><summary>detail</summary>TMLR 2025</details>|
|**2025-6-26**|**Unlocking Constraints: Source-Free Occlusion-Aware Seamless Segmentation**|Yihong Cao et.al|[paper](https://arxiv.org/abs/2506.21198)|[code](https://github.com/yihong-97/UNLOCK.)|<details><summary>detail</summary>ICCV 2025</details>|
|**2025-6-25**|**Context Aware Grounded Teacher for Source Free Object Detection**|Tajamul Ashraf et.al|[paper](https://arxiv.org/abs/2504.15404)|[code](https://github.com/Tajamul21/Grounded_Teacher.)|-|
|**2025-6-18**|**SFDLA: Source-Free Document Layout Analysis**|Sebastian Tewes et.al|[paper](https://arxiv.org/abs/2503.18742)|[code](https://github.com/s3setewe/sfdla-DLAdapter.)|<details><summary>detail</summary>Accepted by ICDAR 2025</details>|
|**2025-6-17**|**Unified Source-Free Domain Adaptation**|Song Tang et.al|[paper](https://arxiv.org/abs/2403.07601)|-|-|
|**2025-6-11**|**SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation**|Xinya Liu et.al|[paper](https://arxiv.org/abs/2506.09403)|[code](https://github.com/HiLab-git/SRPL-SFDA.)|-|
|**2025-6-5**|**DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization**|Geonyoung Lee et.al|[paper](https://arxiv.org/abs/2506.02858)|[code](https://wltschmrz.github.io/DGMO/)|<details><summary>detail</summary>Interspeech 2025</details>|
|**2025-5-30**|**Disentangled Source-Free Personalization for Facial Expression Recognition with Neutral Target Data**|Masoumeh Sharafi et.al|[paper](https://arxiv.org/abs/2503.20771)|-|-|
|**2025-5-30**|**Analysis of Pseudo-Labeling for Online Source-Free Universal Domain Adaptation**|Pascal Schlachter et.al|[paper](https://arxiv.org/abs/2504.11992)|[code](https://github.com/pascalschlachter/PLAnalysis.)|<details><summary>detail</summary>the 33rd European Signal Processing Conference (EUSIPCO 2025)</details>|
|**2025-5-30**|**Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation**|Prasanna Reddy Pulakurthi et.al|[paper](https://arxiv.org/abs/2505.24216)|[code](https://github.com/PrasannaPulakurthi/SPM)|-|
|**2025-5-27**|**Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation**|Peihua Deng et.al|[paper](https://arxiv.org/abs/2411.16064)|[code](https://github.com/dengpeihua/GROTO.)|<details><summary>detail</summary>Accepted by CVPR 2025</details>|
|**2025-5-26**|**Training-Free Multi-Step Audio Source Separation**|Yongyi Zang et.al|[paper](https://arxiv.org/abs/2505.19534)|-|-|
|**2025-5-23**|**Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation**|Peiliang Gong et.al|[paper](https://arxiv.org/abs/2505.21525)|-|-|
|**2025-5-20**|**Listen, Analyze, and Adapt to Learn New Attacks: An Exemplar-Free Class Incremental Learning Method for Audio Deepfake Source Tracing**|Yang Xiao et.al|[paper](https://arxiv.org/abs/2505.14601)|-|<details><summary>detail</summary>Accepted by Interspeech 2025</details>|

## object detection

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-7-7**|**Rethinking Detecting Salient and Camouflaged Objects in Unconstrained Scenes**|Zhangjun Zhou et.al|[paper](https://arxiv.org/abs/2412.10943)|[code](https://github.com/ssecv/USCNet.)|-|
|**2025-7-6**|**CVFusion: Cross-View Fusion of 4D Radar and Camera for 3D Object Detection**|Hanzhi Zhong et.al|[paper](https://arxiv.org/abs/2507.04587)|-|-|
|**2025-7-6**|**MambaFusion: Height-Fidelity Dense Global Fusion for Multi-modal 3D Object Detection**|Hanshi Wang et.al|[paper](https://arxiv.org/abs/2507.04369)|-|-|
|**2025-7-6**|**DMAT: An End-to-End Framework for Joint Atmospheric Turbulence Mitigation and Object Detection**|Paul Hill et.al|[paper](https://arxiv.org/abs/2507.04323)|-|-|
|**2025-7-5**|**Towards Accurate and Efficient 3D Object Detection for Autonomous Driving: A Mixture of Experts Computing System on Edge**|Linshen Liu et.al|[paper](https://arxiv.org/abs/2507.04123)|-|<details><summary>detail</summary>ICCV 2025</details>|
|**2025-7-4**|**Pillar-Voxel Fusion Network for 3D Object Detection in Airborne Hyperspectral Point Clouds**|Yanze Jiang et.al|[paper](https://arxiv.org/abs/2504.09506)|-|-|
|**2025-7-4**|**Hallucinatory Image Tokens: A Training-free EAZY Approach on Detecting and Mitigating Object Hallucinations in LVLMs**|Liwei Che et.al|[paper](https://arxiv.org/abs/2503.07772)|-|<details><summary>detail</summary>ICCV2025</details>|
|**2025-7-4**|**Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection**|Jicheng Yuan et.al|[paper](https://arxiv.org/abs/2404.01988)|[code](https://github.com/jichengyuan/Cooperitive_Students.)|<details><summary>detail</summary>Code is available at https://github</details>|
|**2025-7-4**|**2.5D Object Detection for Intelligent Roadside Infrastructure**|Nikolai Polley et.al|[paper](https://arxiv.org/abs/2507.03564)|[code](https://gitlab.kit.edu/kit/aifb/ATKS/public/digit4taf/2.5d-object-detection)|<details><summary>detail</summary>2025 IEEE 28th International Conference on Intelligent Transportation Systems (ITSC)</details>|
|**2025-7-4**|**UnitModule: A Lightweight Joint Image Enhancement Module for Underwater Object Detection**|Zhuoyan Liu et.al|[paper](https://arxiv.org/abs/2309.04708)|[code](https://github.com/LEFTeyex/UnitModule.)|-|
|**2025-7-3**|**Partial Weakly-Supervised Oriented Object Detection**|Mingxin Liu et.al|[paper](https://arxiv.org/abs/2507.02751)|[code](https://github.com/VisionXLab/PWOOD)|-|
|**2025-7-3**|**PLOT: Pseudo-Labeling via Video Object Tracking for Scalable Monocular 3D Object Detection**|Seokyeong Lee et.al|[paper](https://arxiv.org/abs/2507.02393)|-|-|
|**2025-7-3**|**MV2DFusion: Leveraging Modality-Specific Object Semantics for Multi-Modal 3D Detection**|Zitian Wang et.al|[paper](https://arxiv.org/abs/2408.05945)|-|-|
|**2025-7-2**|**Anyview: Generalizable Indoor 3D Object Detection with Variable Frames**|Zhenyu Wu et.al|[paper](https://arxiv.org/abs/2310.05346)|-|-|
|**2025-7-1**|**Rapid Salient Object Detection with Difference Convolutional Neural Networks**|Zhuo Su et.al|[paper](https://arxiv.org/abs/2507.01182)|[code](https://github.com/hellozhuo/stdnet.git.)|-|

## domain adaptation

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-7-7**|**Domain Adaptation of VLM for Soccer Video Understanding**|Tiancheng Jiang et.al|[paper](https://arxiv.org/abs/2505.13860)|-|-|
|**2025-7-6**|**Tackling Dimensional Collapse toward Comprehensive Universal Domain Adaptation**|Hung-Chieh Fang et.al|[paper](https://arxiv.org/abs/2410.11271)|[code](https://dc-unida.github.io/)|-|
|**2025-7-6**|**Domain Adaptation of Drag Reduction Policy to Partial Measurements**|Anton Plaksin et.al|[paper](https://arxiv.org/abs/2507.04309)|-|<details><summary>detail</summary>Journal ref:Machine Learning and the Physical Sciences Workshop</details>|
|**2025-7-5**|**Beyond Independent Passages: Adaptive Passage Combination Retrieval for Retrieval Augmented Open-Domain Question Answering**|Ting-Wen Ko et.al|[paper](https://arxiv.org/abs/2507.04069)|-|-|
|**2025-7-4**|**Cooperative Students: Navigating Unsupervised Domain Adaptation in Nighttime Object Detection**|Jicheng Yuan et.al|[paper](https://arxiv.org/abs/2404.01988)|[code](https://github.com/jichengyuan/Cooperitive_Students.)|<details><summary>detail</summary>Code is available at https://github</details>|
|**2025-7-4**|**Source-Free Domain Adaptation via Multi-view Contrastive Learning**|Amirfarhad Farhadi et.al|[paper](https://arxiv.org/abs/2507.03321)|-|-|
|**2025-7-4**|**Global Variational Inference Enhanced Robust Domain Adaptation**|Lingkun Luo et.al|[paper](https://arxiv.org/abs/2507.03291)|-|-|
|**2025-7-4**|**MORDA: A Synthetic Dataset to Facilitate Adaptation of Object Detectors to Unseen Real-target Domain While Preserving Performance on Real-source Domain**|Hojun Lim et.al|[paper](https://arxiv.org/abs/2501.04950)|-|-|
|**2025-7-3**|**F^2TTA: Free-Form Test-Time Adaptation on Cross-Domain Medical Image Classification via Image-Level Disentangled Prompt Tuning**|Wei Li et.al|[paper](https://arxiv.org/abs/2507.02437)|[code](https://github.com/mar-cry/F2TTA.)|<details><summary>detail</summary>This paper has been submitted to relevant journals</details>|
|**2025-7-3**|**Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability**|Mark Atta Mensah et.al|[paper](https://arxiv.org/abs/2507.02407)|-|<details><summary>detail</summary>This version has been reviewed and accepted for presentation at the Future Technologies Conference (FTC) 2025</details>|
|**2025-7-2**|**Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation**|Yuxiang Zhang et.al|[paper](https://arxiv.org/abs/2507.02268)|[code](https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.)|-|
|**2025-7-2**|**Generalized Adaptive Transfer Network: Enhancing Transfer Learning in Reinforcement Learning Across Domains**|Abhishek Verma et.al|[paper](https://arxiv.org/abs/2507.03026)|-|-|
|**2025-7-1**|**Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation**|Xihang Yu et.al|[paper](https://arxiv.org/abs/2507.00984)|-|-|
|**2025-7-1**|**UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement**|Xiao Zhang et.al|[paper](https://arxiv.org/abs/2507.00721)|[code](https://github.com/AMAP-ML/UPRE.)|<details><summary>detail</summary>ICCV2025</details>|
|**2025-7-1**|**UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions**|Siyuan Yao et.al|[paper](https://arxiv.org/abs/2507.00648)|[code](https://github.com/Z-Z188/UMDATrack.)|<details><summary>detail</summary>ICCV 2025</details>|

## domain generalization

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-7-6**|**Adversarial Data Augmentation for Single Domain Generalization via Lyapunov Exponent-Guided Optimization**|Zuyu Zhang et.al|[paper](https://arxiv.org/abs/2507.04302)|-|<details><summary>detail</summary>ICCV 2025</details>|
|**2025-7-5**|**CLEP-DG: Contrastive Learning for Speech Emotion Domain Generalization via Soft Prompt Tuning**|Jiacheng Shi et.al|[paper](https://arxiv.org/abs/2507.04048)|-|<details><summary>detail</summary>Interspeech2025</details>|
|**2025-7-4**|**Bridging Domain Generalization to Multimodal Domain Generalization via Unified Representations**|Hai Huang et.al|[paper](https://arxiv.org/abs/2507.03304)|-|<details><summary>detail</summary>Accepted by ICCV 2025</details>|
|**2025-7-3**|**Set Valued Predictions For Robust Domain Generalization**|Ron Tsibulsky et.al|[paper](https://arxiv.org/abs/2507.03146)|-|<details><summary>detail</summary>ICML 2025</details>|
|**2025-7-3**|**Stronger, Steadier & Superior: Geometric Consistency in Depth VFM Forges Domain Generalized Semantic Segmentation**|Siyu Chen et.al|[paper](https://arxiv.org/abs/2504.12753)|[code](https://github.com/anonymouse-xzrptkvyqc/DepthForge.)|<details><summary>detail</summary>Accepted by ICCV 2025</details>|
|**2025-7-2**|**Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization**|De Cheng et.al|[paper](https://arxiv.org/abs/2507.02288)|-|-|
|**2025-7-2**|**Generalized Adaptive Transfer Network: Enhancing Transfer Learning in Reinforcement Learning Across Domains**|Abhishek Verma et.al|[paper](https://arxiv.org/abs/2507.03026)|-|-|
|**2025-7-2**|**NuSegDG: Integration of Heterogeneous Space and Gaussian Kernel for Domain-Generalized Nuclei Segmentation**|Zhenye Lou et.al|[paper](https://arxiv.org/abs/2408.11787)|[code](https://github.com/xq141839/NuSegDG.)|-|
|**2025-6-30**|**DGSAM: Domain Generalization via Individual Sharpness-Aware Minimization**|Youngjun Song et.al|[paper](https://arxiv.org/abs/2503.23430)|-|-|
|**2025-6-30**|**Calculation of Photocarrier Generation from Optical Absorption for Time-domain Simulation of Optoelectronic Devices**|Liang Chen et.al|[paper](https://arxiv.org/abs/2102.06702)|-|-|
|**2025-6-30**|**Generalizing vision-language models to novel domains: A comprehensive survey**|Xinyao Li et.al|[paper](https://arxiv.org/abs/2506.18504)|-|-|
|**2025-6-28**|**Learning Time-Aware Causal Representation for Model Generalization in Evolving Domains**|Zhuo He et.al|[paper](https://arxiv.org/abs/2506.17718)|-|<details><summary>detail</summary>ICML 2025</details>|
|**2025-6-27**|**Boosting Domain Generalized and Adaptive Detection with Diffusion Models: Fitness, Generalization, and Transferability**|Boyong He et.al|[paper](https://arxiv.org/abs/2506.21042)|[code](https://github.com/heboyong/Fitness-Generalization-Transferability)|<details><summary>detail</summary>Accepted by ICCV2025</details>|
|**2025-6-27**|**Video-Guided Text-to-Music Generation Using Public Domain Movie Collections**|Haven Kim et.al|[paper](https://arxiv.org/abs/2506.12573)|[code](https://havenpersona.github.io/ossl-v1)|<details><summary>detail</summary>ISMIR 2025 regular paper</details>|
|**2025-6-27**|**Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning**|Fangling Jiang et.al|[paper](https://arxiv.org/abs/2506.21895)|-|-|

## vision language

|Date|Title|Authors|PDF|Code|Comments|
|:------|:---------------------|:---|:-|:-|:---|
|**2025-7-7**|**SVLA: A Unified Speech-Vision-Language Assistant with Multimodal Reasoning and Speech Generation**|Ngoc Dung Huynh et.al|[paper](https://arxiv.org/abs/2503.24164)|-|-|
|**2025-7-7**|**INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling**|Xin Dong et.al|[paper](https://arxiv.org/abs/2507.05056)|-|-|
|**2025-7-7**|**PEVLM: Parallel Encoding for Vision-Language Models**|Letian Kang et.al|[paper](https://arxiv.org/abs/2506.19651)|-|-|
|**2025-7-7**|**From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach**|Mihai Masala et.al|[paper](https://arxiv.org/abs/2507.04815)|-|<details><summary>detail</summary>arXiv admin note: text overlap with arXiv:2501</details>|
|**2025-7-7**|**Vision-Language Models Can't See the Obvious**|Yasser Dahou et.al|[paper](https://arxiv.org/abs/2507.04741)|-|-|
|**2025-7-7**|**An analysis of vision-language models for fabric retrieval**|Francesco Giuliari et.al|[paper](https://arxiv.org/abs/2507.04735)|-|<details><summary>detail</summary>Ital-IA 2025</details>|
|**2025-7-6**|**RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models**|Jacky Kwok et.al|[paper](https://arxiv.org/abs/2506.17811)|-|-|
|**2025-7-6**|**HiLa: Hierarchical Vision-Language Collaboration for Cancer Survival Prediction**|Jiaqi Cui et.al|[paper](https://arxiv.org/abs/2507.04613)|-|<details><summary>detail</summary>Accepted by MICCAI2025</details>|
|**2025-7-6**|**PAVLM: Advancing Point Cloud based Affordance Understanding Via Vision-Language Model**|Shang-Ching Liu et.al|[paper](https://arxiv.org/abs/2410.11564)|-|-|
|**2025-7-6**|**FA: Forced Prompt Learning of Vision-Language Models for Out-of-Distribution Detection**|Xinhua Lu et.al|[paper](https://arxiv.org/abs/2507.04511)|[code](https://github.com/0xFAFA/FA.)|-|
|**2025-7-6**|**MVL-Loc: Leveraging Vision-Language Model for Generalizable Multi-Scene Camera Relocalization**|Zhendong Xiao et.al|[paper](https://arxiv.org/abs/2507.04509)|-|<details><summary>detail</summary>PRCV</details>|
|**2025-7-6**|**DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge**|Wenyao Zhang et.al|[paper](https://arxiv.org/abs/2507.04447)|-|-|
|**2025-7-6**|**Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition**|Dasol Choi et.al|[paper](https://arxiv.org/abs/2505.15367)|-|-|
|**2025-7-6**|**CLIP-RL: Surgical Scene Segmentation Using Contrastive Language-Vision Pretraining & Reinforcement Learning**|Fatmaelzahraa Ali Ahmed et.al|[paper](https://arxiv.org/abs/2507.04317)|-|-|
|**2025-7-5**|**Pedestrian Intention Prediction via Vision-Language Foundation Models**|Mohsen Azarmi et.al|[paper](https://arxiv.org/abs/2507.04141)|-|-|

